{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cfccd65",
   "metadata": {},
   "source": [
    "# 基于DDPG算法的电磁式阻尼器控制\n",
    "\n",
    "本笔记本实现了基于DDPG算法的二自由度电磁阻尼器控制系统。参数来源于MATLAB仿真文件，方法迁移自《数据驱动的动力学系统建模及控制策略研究》。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from env import ElectromagneticDamperEnv # 自定义环境\n",
    "from ddpg_agent import DDPGAgent, ReplayBuffer # DDPG智能体和经验回放缓冲区\n",
    "from train import train_ddpg # 训练函数\n",
    "from af import plot_rewards, plot_state_comparison, find_checkpoint_files # 绘图和工具函数\n",
    "\n",
    "# 设置随机种子，保证结果可重现\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.family'] = ['SimHei', 'Arial']\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 创建保存模型的基础目录\n",
    "save_base_dir = \"./saved_models\"\n",
    "save_plot_dir = \"./saved_plots\"\n",
    "os.makedirs(save_base_dir, exist_ok=True)\n",
    "os.makedirs(save_plot_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a81cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置日志系统和训练会话名称\n",
    "def setup_training_session(base_dir=\"./saved_models\"):\n",
    "    \"\"\"设置训练会话，创建目录和配置日志\"\"\"\n",
    "    # 获取默认训练名称（时间戳）\n",
    "    default_name = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "    \n",
    "    # 让用户输入自定义训练名称\n",
    "    custom_name = input(f\"请输入本次训练名称 (默认: {default_name}): \").strip()\n",
    "    session_name = custom_name if custom_name else default_name\n",
    "    \n",
    "    # 创建会话目录\n",
    "    session_dir = os.path.join(base_dir, session_name)\n",
    "    os.makedirs(session_dir, exist_ok=True)\n",
    "    \n",
    "    # 配置日志\n",
    "    log_file = os.path.join(session_dir, \"training.log\")\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"创建训练会话: {session_name}\")\n",
    "    logging.info(f\"日志保存路径: {log_file}\")\n",
    "    \n",
    "    return session_name, session_dir\n",
    "\n",
    "# 设置训练会话\n",
    "session_name, save_model_dir = setup_training_session(save_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da5c0d0",
   "metadata": {},
   "source": [
    "## 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb62b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 控制参数\n",
    "T = 3  # 系统运行时间\n",
    "Ts = 0.001  # 采样时间步长\n",
    "\n",
    "# 保存训练配置参数\n",
    "training_config = {\n",
    "    \"session_name\": session_name,\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"device\": str(device),\n",
    "    \"training\": {\n",
    "        \"n_episodes\": 200,\n",
    "        \"batch_size\": 64,\n",
    "        \"min_buffer_size\": 1000,\n",
    "        \"print_interval\": 2,\n",
    "        \"save_interval\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "# 记录系统参数\n",
    "logging.info(f\"训练会话: {session_name}\")\n",
    "logging.info(f\"设备: {device}\")\n",
    "logging.info(f\"仿真时间: T={T}, Ts={Ts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecddede",
   "metadata": {},
   "source": [
    "## 系统参数设置\n",
    "\n",
    "从MATLAB仿真文件中提取二自由度电磁阻尼器系统的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f485b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系统参数（来自MATLAB文件）\n",
    "m = 1.6    # 电磁吸振器质量\n",
    "M = 100.0  # 待减振对象质量\n",
    "k_m = 3000.0  # 电磁吸振器刚度\n",
    "k_M = 200000.0  # 平台刚度\n",
    "k_f = 45.0  # 电—力常数 N/A\n",
    "k_E = 0.0  # 作动器反电动势系数\n",
    "L = 0.0045  # 线圈的电感\n",
    "R_m = 5.0  # 线圈的电阻\n",
    "c_m = 0.2  # 电磁吸振器阻尼\n",
    "c_M = 1.0  # 平台阻尼\n",
    "\n",
    "# 状态空间矩阵（来自MATLAB文件）\n",
    "A = np.array([\n",
    "    [0.0,    1.0,       0.0,         0.0],\n",
    "    [-k_m/m, -c_m/m,    k_m/m,       c_m/m],\n",
    "    [0.0,    0.0,       0.0,         1.0],\n",
    "    [k_m/M,  c_m/M,     -(k_m+k_M)/M, -(c_m+c_M)/M]\n",
    "])\n",
    "\n",
    "B = np.array([[0.0], [k_f/m], [0.0], [-k_f/M]])\n",
    "\n",
    "C = np.array([\n",
    "    [-k_m/m, -c_m/m,    k_m/m,       c_m/m],\n",
    "    [k_m/M,  c_m/M,     -(k_m+k_M)/M, -(c_m+c_M)/M]\n",
    "])\n",
    "\n",
    "D = np.array([[k_f/m], [-k_f/M]])\n",
    "\n",
    "E = np.array([\n",
    "    [0.0, 0.0, 0.0, c_M/M],\n",
    "    [0.0, 0.0, 0.0, k_M/M]\n",
    "]).T\n",
    "\n",
    "# 更新训练配置参数中的系统参数\n",
    "training_config[\"parameters\"] = {\n",
    "    \"system\": {\n",
    "        \"m\": m,\n",
    "        \"M\": M,\n",
    "        \"k_m\": k_m,\n",
    "        \"k_M\": k_M,\n",
    "        \"k_f\": k_f,\n",
    "        \"k_E\": k_E,\n",
    "        \"L\": L,\n",
    "        \"R_m\": R_m,\n",
    "        \"c_m\": c_m,\n",
    "        \"c_M\": c_M\n",
    "    },\n",
    "    \"simulation\": {\n",
    "        \"T\": T,\n",
    "        \"Ts\": Ts\n",
    "    }\n",
    "}\n",
    "\n",
    "# 记录系统参数\n",
    "logging.info(\"系统参数:\")\n",
    "logging.info(f\"  吸振器质量 m = {m}\")\n",
    "logging.info(f\"  减振对象质量 M = {M}\")\n",
    "logging.info(f\"  吸振器刚度 k_m = {k_m}\")\n",
    "logging.info(f\"  平台刚度 k_M = {k_M}\")\n",
    "logging.info(f\"  电-力常数 k_f = {k_f}\")\n",
    "\n",
    "# 将配置保存为JSON文件\n",
    "config_file = os.path.join(save_model_dir, \"config.json\")\n",
    "with open(config_file, \"w\") as f:\n",
    "    json.dump(training_config, f, indent=4)\n",
    "\n",
    "logging.info(f\"训练配置已保存至: {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a993de7",
   "metadata": {},
   "source": [
    "### 无控制输出环境验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_env = ElectromagneticDamperEnv(A, B, C, D, E, Ts=0.0001, T=1)\n",
    "\n",
    "# 定义正弦扰动函数用于测试\n",
    "def sine_disturbance(amp, freq):\n",
    "    \"\"\"正弦扰动函数\"\"\"\n",
    "    def func(t):\n",
    "        return amp * np.sin(2 * np.pi * freq * t)\n",
    "    return func\n",
    "\n",
    "# 运行无控制的仿真\n",
    "results_no_control = rl_env.run_simulation(z_func=sine_disturbance(0.01, 30))\n",
    "plt.plot(results_no_control['times'], results_no_control['states'][:, 2], label='No Control')\n",
    "plt.xlabel('时间 (s)')\n",
    "plt.ylabel('位移')\n",
    "plt.title('无控制系统响应')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "logging.info(\"无控制系统验证完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d992f",
   "metadata": {},
   "source": [
    "## 初始化环境和DDPG代理\n",
    "\n",
    "使用从MATLAB文件提取的参数创建电磁阻尼器系统环境和DDPG代理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d145f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建仿真环境\n",
    "rl_env = ElectromagneticDamperEnv(A, B, C, D, E, Ts=Ts, T=T)\n",
    "\n",
    "# 设置外部扰动函数 - 这里设置为简单的正弦波扰动\n",
    "rl_env.set_disturbance(sine_disturbance(0.001, 30)) # 设置扰动函数\n",
    "\n",
    "agent = DDPGAgent(\n",
    "    state_dim=1, # 状态维度\n",
    "    action_dim=1, # 动作维度\n",
    "    hidden_dim=64, # 隐藏层维度\n",
    "    action_bound=5.0, # 动作范围\n",
    "    actor_lr=1e-4, # 调整学习率可能需要\n",
    "    critic_lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    sigma=0.2 # 初始噪声标准差\n",
    ")\n",
    "\n",
    "# 创建经验回放池\n",
    "replay_buffer = ReplayBuffer(capacity=100000, batch_size=64)\n",
    "\n",
    "# 记录DDPG代理参数\n",
    "logging.info(\"DDPG代理参数:\")\n",
    "logging.info(f\"  状态维度: 1\")\n",
    "logging.info(f\"  动作维度: 1\")\n",
    "logging.info(f\"  隐藏层维度: 64\")\n",
    "logging.info(f\"  动作范围: 5.0\")\n",
    "logging.info(f\"  Actor学习率: 1e-4\")\n",
    "logging.info(f\"  Critic学习率: 1e-3\")\n",
    "logging.info(f\"  折扣因子gamma: 0.99\")\n",
    "logging.info(f\"  软更新参数tau: 0.005\")\n",
    "logging.info(f\"  探索噪声标准差: 0.2\")\n",
    "logging.info(f\"  经验回放池容量: 100000\")\n",
    "logging.info(f\"  批次大小: 64\")\n",
    "\n",
    "# 更新训练配置中的DDPG参数\n",
    "training_config[\"agent_params\"] = {\n",
    "    \"state_dim\": 1,\n",
    "    \"action_dim\": 1,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"action_bound\": 5.0,\n",
    "    \"actor_lr\": 1e-4,\n",
    "    \"critic_lr\": 1e-3,\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.005,\n",
    "    \"sigma\": 0.2\n",
    "}\n",
    "\n",
    "# 重新保存更新后的配置\n",
    "with open(config_file, \"w\") as f:\n",
    "    json.dump(training_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e07fe3",
   "metadata": {},
   "source": [
    "## 训练DDPG代理\n",
    "\n",
    "训练DDPG代理来控制电磁阻尼器系统。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aaffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_func(obs:np.ndarray, action:np.ndarray, next_obs:np.ndarray)-> float:\n",
    "    \"\"\"自定义奖励函数\"\"\"\n",
    "    sum_next_obs = np.sum(next_obs**2) # 计算下一个状态的平方和\n",
    "    sum_action = np.sum(action**2) # 计算动作的平方和\n",
    "    # 奖励函数：负的状态平方和和动作平方和之和\n",
    "    reward:float = - (10 * sum_next_obs + 0.001 * sum_action) # 奖励越大越好\n",
    "    return reward\n",
    "\n",
    "# 记录奖励函数\n",
    "logging.info(\"奖励函数: reward = -(10 * sum_next_obs + 0.001 * sum_action)\")\n",
    "training_config[\"reward_function\"] = \"-(10 * sum_next_obs + 0.001 * sum_action)\"\n",
    "\n",
    "# 重新保存更新后的配置\n",
    "with open(config_file, \"w\") as f:\n",
    "    json.dump(training_config, f, indent=4)\n",
    "\n",
    "# 是否加载先前的训练模型\n",
    "load_previous_model = input(\"是否加载先前的训练模型? (y/n): \").strip().lower() == 'y'\n",
    "\n",
    "start_episode = 0\n",
    "initial_episode_rewards = None\n",
    "previous_model_path = None\n",
    "\n",
    "if load_previous_model:\n",
    "    checkpoint_files = find_checkpoint_files(save_base_dir)\n",
    "    \n",
    "    if checkpoint_files:\n",
    "        print(\"\\n找到以下检查点文件:\")\n",
    "        for i, file in enumerate(checkpoint_files):\n",
    "            file_name = os.path.basename(file)\n",
    "            file_time = os.path.getmtime(file)\n",
    "            print(f\"{i+1}. {file_name} (修改时间: {datetime.fromtimestamp(file_time).strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "        \n",
    "        choice = input(\"请选择要加载的检查点文件编号 (输入数字，直接回车取最新): \")\n",
    "        \n",
    "        if choice.strip():\n",
    "            selected_index = int(choice) - 1\n",
    "            if 0 <= selected_index < len(checkpoint_files):\n",
    "                previous_model_path = checkpoint_files[selected_index]\n",
    "        else:\n",
    "            # 默认选择最新的检查点\n",
    "            previous_model_path = checkpoint_files[0]\n",
    "        \n",
    "        print(f\"加载模型: {os.path.basename(previous_model_path)}\")\n",
    "        logging.info(f\"加载检查点: {os.path.basename(previous_model_path)}\")\n",
    "        \n",
    "        # 加载模型并获取训练状态\n",
    "        episode_rewards, current_episode = agent.load_checkpoint(previous_model_path)\n",
    "        start_episode = current_episode\n",
    "        initial_episode_rewards = episode_rewards\n",
    "        logging.info(f\"继续从第 {start_episode} 轮训练，已完成 {len(episode_rewards)} 轮\")\n",
    "        print(f\"继续从第 {start_episode} 轮训练，已完成 {len(episode_rewards)} 轮\")\n",
    "    else:\n",
    "        print(\"未找到可加载的检查点文件，将从零开始训练\")\n",
    "        logging.info(\"未找到可加载的检查点，从零开始训练\")\n",
    "\n",
    "# 记录训练参数\n",
    "training_params = {\n",
    "    \"n_episodes\": 200,\n",
    "    \"batch_size\": 64,\n",
    "    \"min_buffer_size\": 1000,\n",
    "    \"print_interval\": 2,\n",
    "    \"save_interval\": 5,\n",
    "    \"start_episode\": start_episode\n",
    "}\n",
    "\n",
    "logging.info(f\"训练参数: {training_params}\")\n",
    "logging.info(f\"开始DDPG训练...\")\n",
    "\n",
    "# 训练DDPG代理\n",
    "training_results = train_ddpg(\n",
    "    env=rl_env,\n",
    "    agent=agent,\n",
    "    replay_buffer=replay_buffer,\n",
    "    n_episodes=training_params[\"n_episodes\"],  # 训练轮数\n",
    "    batch_size=training_params[\"batch_size\"], # 批次大小\n",
    "    min_buffer_size=training_params[\"min_buffer_size\"], # 最小回放池大小\n",
    "    print_interval=training_params[\"print_interval\"], # 打印信息的间隔\n",
    "    save_interval=training_params[\"save_interval\"], # 保存模型的间隔\n",
    "    save_path=save_model_dir, # 保存模型的路径\n",
    "    log_path=save_model_dir, # 日志保存路径\n",
    "    r_func=r_func, # 奖励函数\n",
    "    start_episode=start_episode,\n",
    "    initial_episode_rewards=initial_episode_rewards,\n",
    "    load_previous=load_previous_model,\n",
    "    previous_model=previous_model_path\n",
    ")\n",
    "\n",
    "logging.info(f\"训练完成，最终平均奖励: {training_results['avg_rewards'][-1]:.2f}\")\n",
    "\n",
    "# 绘制训练奖励曲线\n",
    "rewards_plot_path = os.path.join(save_model_dir, \"training_rewards.png\")\n",
    "plot_rewards(training_results['episode_rewards'], training_results['avg_rewards'], save_path=rewards_plot_path)\n",
    "logging.info(f\"奖励图表已保存: {rewards_plot_path}\")\n",
    "\n",
    "# 记录训练结果摘要\n",
    "logging.info(\"训练结果摘要:\")\n",
    "logging.info(f\"  总轮次: {len(training_results['episode_rewards'])}\")\n",
    "logging.info(f\"  最终平均奖励: {training_results['avg_rewards'][-1]:.4f}\")\n",
    "logging.info(f\"  最高单轮奖励: {max(training_results['episode_rewards']):.4f}\")\n",
    "logging.info(f\"  最终模型路径: {training_results['final_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0dfb48",
   "metadata": {},
   "source": [
    "## 测试与结果对比\n",
    "\n",
    "测试不同控制策略（无控制和DDPG控制）的效果，并进行对比分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e623de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建测试环境\n",
    "test_env = ElectromagneticDamperEnv(A, B, C, D, E, Ts=0.001, T=1)\n",
    "\n",
    "# --- 运行仿真 ---\n",
    "logging.info(\"开始测试仿真\")\n",
    "\n",
    "# 运行无控制的仿真\n",
    "logging.info(\"运行无控制仿真\")\n",
    "results_no_control = test_env.run_simulation(z_func=sine_disturbance(0.001, 30))\n",
    "\n",
    "# 选择加载的模型\n",
    "present_model_options = [\n",
    "    (\"最新训练模型\", f\"{save_model_dir}/ddpg_agent_{session_name}_ep{start_episode+training_params['n_episodes']}_final.pth\"),\n",
    "    (\"手动选择模型\", None)\n",
    "]\n",
    "\n",
    "choice = input(f\"请选择测试模型: 1 = 最新训练模型, 2 = 手动选择模型: \").strip()\n",
    "\n",
    "if choice == \"2\":\n",
    "    # 允许用户手动选择模型文件\n",
    "    model_files = []\n",
    "    for root, dirs, files in os.walk(save_base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.pth') and not file.endswith('_checkpoint.pth'):\n",
    "                model_files.append(os.path.join(root, file))\n",
    "                \n",
    "    if not model_files:\n",
    "        print(\"未找到模型文件，使用最新训练的模型\")\n",
    "        model_path = present_model_options[0][1]\n",
    "    else:\n",
    "        print(\"\\n找到以下模型文件:\")\n",
    "        for i, file in enumerate(model_files):\n",
    "            print(f\"{i+1}. {os.path.basename(file)}\")\n",
    "                \n",
    "        idx = int(input(\"请选择模型文件编号: \")) - 1\n",
    "        if 0 <= idx < len(model_files):\n",
    "            model_path = model_files[idx]\n",
    "        else:\n",
    "            model_path = present_model_options[0][1]\n",
    "else:\n",
    "    # 使用当前训练的最新模型\n",
    "    model_path = present_model_options[0][1]\n",
    "\n",
    "# 加载选择的DDPG模型\n",
    "logging.info(f\"加载DDPG模型: {os.path.basename(model_path)}\")\n",
    "print(f\"加载模型: {os.path.basename(model_path)}\")\n",
    "agent.load(model_path)\n",
    "\n",
    "# 运行DDPG控制的仿真\n",
    "logging.info(\"运行DDPG控制仿真\")\n",
    "results_ddpg = test_env.run_simulation(z_func=sine_disturbance(0.001, 30), controller=agent.actor)\n",
    "\n",
    "# 计算DDPG控制效果改善百分比\n",
    "max_displacement_no_control = np.max(np.abs(results_no_control['states'][:, 2]))\n",
    "max_displacement_ddpg = np.max(np.abs(results_ddpg['states'][:, 2]))\n",
    "improvement = (max_displacement_no_control - max_displacement_ddpg) / max_displacement_no_control * 100\n",
    "\n",
    "logging.info(f\"位移控制效果: 无控制最大位移={max_displacement_no_control:.6f}, DDPG最大位移={max_displacement_ddpg:.6f}\")\n",
    "logging.info(f\"位移改善: {improvement:.2f}%\")\n",
    "\n",
    "# --- 绘制对比结果 ---\n",
    "comparison_plot_path = os.path.join(save_model_dir, \"control_comparison.png\")\n",
    "fig = plot_state_comparison(results_no_control, results_ddpg, save_path=comparison_plot_path)\n",
    "logging.info(f\"对比结果图表已保存: {comparison_plot_path}\")\n",
    "\n",
    "# --- 保存测试结果 ---\n",
    "test_results = {\n",
    "    \"no_control\": {\n",
    "        \"states_mean\": results_no_control['states'].mean(axis=0).tolist(),\n",
    "        \"states_max\": results_no_control['states'].max(axis=0).tolist(),\n",
    "        \"states_min\": results_no_control['states'].min(axis=0).tolist(),\n",
    "    },\n",
    "    \"ddpg_control\": {\n",
    "        \"states_mean\": results_ddpg['states'].mean(axis=0).tolist(),\n",
    "        \"states_max\": results_ddpg['states'].max(axis=0).tolist(),\n",
    "        \"states_min\": results_ddpg['states'].min(axis=0).tolist(),\n",
    "        \"actions_mean\": float(results_ddpg['actions'].mean()),\n",
    "        \"actions_max\": float(results_ddpg['actions'].max()),\n",
    "        \"actions_min\": float(results_ddpg['actions'].min()),\n",
    "    }\n",
    "}\n",
    "\n",
    "results_file = os.path.join(save_model_dir, \"test_results.json\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(test_results, f, indent=4)\n",
    "\n",
    "logging.info(f\"测试结果已保存至: {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
