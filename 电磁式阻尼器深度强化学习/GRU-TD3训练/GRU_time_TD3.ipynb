{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb8b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间感知GRU-TD3深度强化学习训练\n",
    "# Time-Aware GRU-TD3 Deep Reinforcement Learning Training\n",
    "# \n",
    "# 本notebook实现基于时间感知GRU的TD3算法用于电磁式阻尼器控制\n",
    "# This notebook implements time-aware GRU-based TD3 algorithm for electromagnetic damper control\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# 导入自定义模块\n",
    "from env import ElectromagneticDamperEnv\n",
    "from TD3 import GruTD3Agent\n",
    "from train import train_gru_td3\n",
    "from af import plot_training_results, save_checkpoint, load_checkpoint\n",
    "from fx import reward_function_combined\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"时间感知GRU-TD3电磁式阻尼器控制训练系统\")\n",
    "print(\"Time-Aware GRU-TD3 Electromagnetic Damper Control Training System\")\n",
    "print(\"=\"*80)\n",
    "print(\"PyTorch版本:\", torch.__version__)\n",
    "print(\"CUDA可用:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA设备:\", torch.cuda.get_device_name())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859bd70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系统参数配置\n",
    "print(\"配置训练参数...\")\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# 训练参数\n",
    "TOTAL_EPISODES = 2000\n",
    "MAX_STEPS = 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE_ACTOR = 1e-4\n",
    "LEARNING_RATE_CRITIC = 1e-3\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "NOISE_STD = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "POLICY_DELAY = 2\n",
    "MEMORY_SIZE = 100000\n",
    "\n",
    "# GRU特定参数\n",
    "SEQUENCE_LENGTH = 50\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "USE_TIME_INPUT = True  # 关键：启用时间感知\n",
    "\n",
    "# 环境参数\n",
    "INITIAL_DISPLACEMENT = 0.001  # 初始位移\n",
    "TIME_NOISE_STD = 0.1  # 时间噪声标准差\n",
    "VARIABLE_TIMESTEP = True  # 启用可变时间步长\n",
    "\n",
    "# 日志配置\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = \"gru_savedata\"\n",
    "checkpoint_dir = f\"{log_dir}/gru_checkpoints\"\n",
    "plot_dir = f\"{log_dir}/plots\"\n",
    "\n",
    "# 创建保存目录\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# 配置日志\n",
    "log_file = f\"{log_dir}/training_log_{timestamp}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"训练参数配置完成:\")\n",
    "print(f\"  总训练轮次: {TOTAL_EPISODES}\")\n",
    "print(f\"  序列长度: {SEQUENCE_LENGTH}\")\n",
    "print(f\"  隐藏单元数: {HIDDEN_SIZE}\")\n",
    "print(f\"  时间感知模式: {USE_TIME_INPUT}\")\n",
    "print(f\"  时间噪声标准差: {TIME_NOISE_STD}\")\n",
    "print(f\"  可变时间步长: {VARIABLE_TIMESTEP}\")\n",
    "print(f\"  检查点保存路径: {checkpoint_dir}\")\n",
    "print(f\"  训练日志: {log_file}\")\n",
    "\n",
    "logging.info(f\"开始时间感知GRU-TD3训练 - 种子: {SEED}, 轮次: {TOTAL_EPISODES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8655e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 扰动和奖励函数定义\n",
    "print(\"定义扰动和奖励函数...\")\n",
    "\n",
    "def time_aware_disturbance_function(t, amplitude_scale=1.0, time_noise_std=0.1):\n",
    "    \"\"\"\n",
    "    时间感知的多频率扰动函数\n",
    "    \n",
    "    Args:\n",
    "        t: 时间\n",
    "        amplitude_scale: 幅值缩放因子\n",
    "        time_noise_std: 时间噪声标准差\n",
    "    \"\"\"\n",
    "    # 添加时间噪声来模拟实际测量中的时间不确定性\n",
    "    noisy_t = t + np.random.normal(0, time_noise_std)\n",
    "    \n",
    "    # 多频率组合扰动（考虑时间噪声）\n",
    "    f1, f2, f3 = 1.0, 2.5, 4.0  # Hz\n",
    "    disturbance = amplitude_scale * (\n",
    "        0.5 * np.sin(2 * np.pi * f1 * noisy_t) +\n",
    "        0.3 * np.sin(2 * np.pi * f2 * noisy_t) +\n",
    "        0.2 * np.sin(2 * np.pi * f3 * noisy_t) +\n",
    "        0.1 * np.random.normal()  # 随机噪声\n",
    "    )\n",
    "    \n",
    "    return disturbance\n",
    "\n",
    "def time_aware_reward_function(state, action, next_state, time_info=None):\n",
    "    \"\"\"\n",
    "    时间感知的奖励函数\n",
    "    \n",
    "    Args:\n",
    "        state: 当前状态\n",
    "        action: 执行的动作\n",
    "        next_state: 下一状态\n",
    "        time_info: 时间信息字典，包含当前时间、时间步长等\n",
    "    \"\"\"\n",
    "    # 基础奖励（基于位移和速度）\n",
    "    x2 = next_state[3]  # 主结构位移\n",
    "    v2 = next_state[4]  # 主结构速度\n",
    "    \n",
    "    # 位移惩罚（非线性）\n",
    "    displacement_penalty = -(x2**2) * 1000\n",
    "    \n",
    "    # 速度惩罚\n",
    "    velocity_penalty = -(v2**2) * 100\n",
    "    \n",
    "    # 动作惩罚（避免过大控制力）\n",
    "    action_penalty = -(action**2) * 0.1\n",
    "    \n",
    "    # 时间感知的调节因子\n",
    "    time_factor = 1.0\n",
    "    if time_info is not None and 'current_time' in time_info:\n",
    "        current_time = time_info['current_time']\n",
    "        # 在特定时间段增强奖励权重（例如在共振频率附近）\n",
    "        if 0.8 <= (current_time % 1.0) <= 1.0:  # 每秒的最后0.2秒\n",
    "            time_factor = 1.2\n",
    "        \n",
    "        # 根据时间步长调整奖励\n",
    "        if 'timestep' in time_info:\n",
    "            dt = time_info['timestep']\n",
    "            # 较小的时间步长给予奖励加成（更精确的控制）\n",
    "            if dt < 0.01:\n",
    "                time_factor *= 1.1\n",
    "    \n",
    "    # 综合奖励\n",
    "    reward = time_factor * (displacement_penalty + velocity_penalty + action_penalty)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "print(\"扰动函数: 时间感知多频率组合（时间噪声标准差: {:.2f}）\".format(TIME_NOISE_STD))\n",
    "print(\"奖励函数: 时间感知综合奖励（位移、速度、动作、时间因子）\")\n",
    "print(\"时间感知特性:\")\n",
    "print(\"  - 时间噪声模拟测量不确定性\")\n",
    "print(\"  - 时间相关的奖励权重调节\")\n",
    "print(\"  - 可变时间步长适应性\")\n",
    "\n",
    "logging.info(\"时间感知扰动和奖励函数配置完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境初始化\n",
    "print(\"=\"*60)\n",
    "print(\"初始化时间感知训练环境\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 创建环境\n",
    "env = ElectromagneticDamperEnv(\n",
    "    disturbance_func=time_aware_disturbance_function,\n",
    "    reward_func=time_aware_reward_function,\n",
    "    initial_displacement=INITIAL_DISPLACEMENT,\n",
    "    time_noise_std=TIME_NOISE_STD,\n",
    "    variable_timestep=VARIABLE_TIMESTEP\n",
    ")\n",
    "\n",
    "print(\"环境配置:\")\n",
    "print(f\"  状态空间维度: {env.state_dim}\")\n",
    "print(f\"  动作空间维度: {env.action_dim}\")\n",
    "print(f\"  最大动作幅值: {env.max_action}\")\n",
    "print(f\"  时间步长: {env.Ts} s\")\n",
    "print(f\"  初始位移: {INITIAL_DISPLACEMENT} m\")\n",
    "print(f\"  时间噪声标准差: {TIME_NOISE_STD}\")\n",
    "print(f\"  可变时间步长: {VARIABLE_TIMESTEP}\")\n",
    "\n",
    "# 系统物理参数显示\n",
    "print(f\"\\n系统物理参数:\")\n",
    "print(f\"  主结构质量 m2: {env.m2} kg\")\n",
    "print(f\"  TMD质量 m1: {env.m1} kg\")\n",
    "print(f\"  主结构刚度 k2: {env.k2} N/m\")\n",
    "print(f\"  TMD刚度 k1: {env.k1} N/m\")\n",
    "print(f\"  主结构阻尼 c2: {env.c2} Ns/m\")\n",
    "print(f\"  TMD基础阻尼 c1: {env.c1} Ns/m\")\n",
    "print(f\"  电磁阻尼系数 μ: {env.mu}\")\n",
    "\n",
    "# 测试环境运行\n",
    "print(f\"\\n测试环境初始化...\")\n",
    "state = env.reset()\n",
    "print(f\"初始状态维度: {state.shape}\")\n",
    "print(f\"初始状态: {state}\")\n",
    "\n",
    "# 测试一步动作\n",
    "test_action = np.array([0.1])\n",
    "next_state, reward, done, info = env.step(test_action)\n",
    "print(f\"测试动作: {test_action}\")\n",
    "print(f\"奖励: {reward:.4f}\")\n",
    "print(f\"信息: {info}\")\n",
    "\n",
    "print(\"环境初始化完成！\")\n",
    "logging.info(f\"时间感知环境初始化完成 - 状态维度: {env.state_dim}, 动作维度: {env.action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间感知GRU-TD3智能体初始化\n",
    "print(\"=\"*60)\n",
    "print(\"初始化时间感知GRU-TD3智能体\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 确定设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 初始化时间感知GRU-TD3智能体\n",
    "agent = GruTD3Agent(\n",
    "    state_dim=env.state_dim,\n",
    "    action_dim=env.action_dim,\n",
    "    max_action=env.max_action,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    use_time_input=USE_TIME_INPUT,  # 关键：启用时间输入\n",
    "    lr_actor=LEARNING_RATE_ACTOR,\n",
    "    lr_critic=LEARNING_RATE_CRITIC,\n",
    "    gamma=GAMMA,\n",
    "    tau=TAU,\n",
    "    noise_std=NOISE_STD,\n",
    "    noise_clip=NOISE_CLIP,\n",
    "    policy_delay=POLICY_DELAY,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"时间感知GRU-TD3智能体配置:\")\n",
    "print(f\"  状态维度: {env.state_dim}\")\n",
    "print(f\"  动作维度: {env.action_dim}\")\n",
    "print(f\"  隐藏单元数: {HIDDEN_SIZE}\")\n",
    "print(f\"  GRU层数: {NUM_LAYERS}\")\n",
    "print(f\"  序列长度: {SEQUENCE_LENGTH}\")\n",
    "print(f\"  时间输入: {USE_TIME_INPUT}\")\n",
    "print(f\"  Actor学习率: {LEARNING_RATE_ACTOR}\")\n",
    "print(f\"  Critic学习率: {LEARNING_RATE_CRITIC}\")\n",
    "print(f\"  折扣因子: {GAMMA}\")\n",
    "print(f\"  软更新率: {TAU}\")\n",
    "print(f\"  探索噪声标准差: {NOISE_STD}\")\n",
    "print(f\"  策略延迟: {POLICY_DELAY}\")\n",
    "\n",
    "# 网络结构信息\n",
    "print(f\"\\n网络结构:\")\n",
    "print(f\"  Actor网络参数数量: {sum(p.numel() for p in agent.actor.parameters())}\")\n",
    "print(f\"  Critic1网络参数数量: {sum(p.numel() for p in agent.critic1.parameters())}\")\n",
    "print(f\"  Critic2网络参数数量: {sum(p.numel() for p in agent.critic2.parameters())}\")\n",
    "\n",
    "# 测试智能体\n",
    "print(f\"\\n测试智能体运行...\")\n",
    "test_state_sequence = torch.FloatTensor([state] * SEQUENCE_LENGTH).unsqueeze(0).to(device)\n",
    "if USE_TIME_INPUT:\n",
    "    test_time_sequence = torch.FloatTensor([[i * env.Ts for i in range(SEQUENCE_LENGTH)]]).to(device)\n",
    "    test_action = agent.select_action(test_state_sequence, test_time_sequence, add_noise=False)\n",
    "else:\n",
    "    test_action = agent.select_action(test_state_sequence, add_noise=False)\n",
    "\n",
    "print(f\"测试动作输出: {test_action}\")\n",
    "print(f\"动作形状: {test_action.shape}\")\n",
    "\n",
    "print(\"时间感知GRU-TD3智能体初始化完成！\")\n",
    "logging.info(f\"时间感知GRU-TD3智能体初始化完成 - 网络参数: \"\n",
    "            f\"Actor {sum(p.numel() for p in agent.actor.parameters())}, \"\n",
    "            f\"Critic1 {sum(p.numel() for p in agent.critic1.parameters())}, \"\n",
    "            f\"Critic2 {sum(p.numel() for p in agent.critic2.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练配置和检查点管理\n",
    "print(\"=\"*60)\n",
    "print(\"配置训练流程和检查点管理\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 训练配置\n",
    "training_config = {\n",
    "    'episodes': TOTAL_EPISODES,\n",
    "    'max_steps': MAX_STEPS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'memory_size': MEMORY_SIZE,\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'use_time_input': USE_TIME_INPUT,\n",
    "    'checkpoint_dir': checkpoint_dir,\n",
    "    'plot_dir': plot_dir,\n",
    "    'save_interval': 100,  # 每100轮保存一次\n",
    "    'eval_interval': 50,   # 每50轮评估一次\n",
    "    'print_interval': 10   # 每10轮打印一次\n",
    "}\n",
    "\n",
    "print(\"训练配置:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# 检查是否存在之前的检查点\n",
    "checkpoint_path = f\"{checkpoint_dir}/time_aware_gru_td3_latest.pth\"\n",
    "start_episode = 0\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"\\n发现检查点文件: {checkpoint_path}\")\n",
    "    choice = input(\"是否从检查点继续训练？(y/n): \").lower().strip()\n",
    "    \n",
    "    if choice == 'y':\n",
    "        try:\n",
    "            checkpoint = load_checkpoint(checkpoint_path)\n",
    "            agent.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "            agent.critic1.load_state_dict(checkpoint['critic1_state_dict'])\n",
    "            agent.critic2.load_state_dict(checkpoint['critic2_state_dict'])\n",
    "            agent.actor_target.load_state_dict(checkpoint['actor_target_state_dict'])\n",
    "            agent.critic1_target.load_state_dict(checkpoint['critic1_target_state_dict'])\n",
    "            agent.critic2_target.load_state_dict(checkpoint['critic2_target_state_dict'])\n",
    "            agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "            agent.critic1_optimizer.load_state_dict(checkpoint['critic1_optimizer'])\n",
    "            agent.critic2_optimizer.load_state_dict(checkpoint['critic2_optimizer'])\n",
    "            start_episode = checkpoint['episode'] + 1\n",
    "            \n",
    "            print(f\"成功加载检查点，从第 {start_episode} 轮开始继续训练\")\n",
    "            logging.info(f\"从检查点恢复训练 - 起始轮次: {start_episode}\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载检查点失败: {e}\")\n",
    "            print(\"将从头开始训练\")\n",
    "            logging.warning(f\"检查点加载失败: {e}\")\n",
    "    else:\n",
    "        print(\"选择从头开始训练\")\n",
    "else:\n",
    "    print(\"未发现检查点文件，从头开始训练\")\n",
    "\n",
    "# 训练结果记录\n",
    "rewards_log = []\n",
    "losses_log = {'actor': [], 'critic1': [], 'critic2': []}\n",
    "eval_rewards = []\n",
    "\n",
    "print(f\"\\n准备开始训练:\")\n",
    "print(f\"  起始轮次: {start_episode}\")\n",
    "print(f\"  目标轮次: {TOTAL_EPISODES}\")\n",
    "print(f\"  剩余轮次: {TOTAL_EPISODES - start_episode}\")\n",
    "print(f\"  检查点保存路径: {checkpoint_path}\")\n",
    "\n",
    "logging.info(f\"训练配置完成 - 起始轮次: {start_episode}, 目标轮次: {TOTAL_EPISODES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始时间感知GRU-TD3训练\n",
    "print(\"=\"*80)\n",
    "print(\"开始时间感知GRU-TD3训练\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # 执行训练\n",
    "    training_results = train_gru_td3(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        episodes=TOTAL_EPISODES,\n",
    "        max_steps=MAX_STEPS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        memory_size=MEMORY_SIZE,\n",
    "        sequence_length=SEQUENCE_LENGTH,\n",
    "        use_time_input=USE_TIME_INPUT,  # 关键：启用时间输入\n",
    "        start_episode=start_episode,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        save_interval=training_config['save_interval'],\n",
    "        eval_interval=training_config['eval_interval'],\n",
    "        print_interval=training_config['print_interval'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"训练完成！\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 解包训练结果\n",
    "    rewards_log = training_results['rewards']\n",
    "    losses_log = training_results['losses']\n",
    "    eval_rewards = training_results['eval_rewards']\n",
    "    \n",
    "    # 保存最终模型\n",
    "    final_checkpoint_path = f\"{checkpoint_dir}/time_aware_gru_td3_final.pth\"\n",
    "    save_checkpoint(agent, TOTAL_EPISODES-1, final_checkpoint_path)\n",
    "    \n",
    "    # 保存训练日志到CSV\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 奖励日志\n",
    "    rewards_df = pd.DataFrame({\n",
    "        'episode': range(len(rewards_log)),\n",
    "        'reward': rewards_log\n",
    "    })\n",
    "    rewards_csv_path = f\"{log_dir}/time_aware_gru_rewards_log{timestamp[4:]}.csv\"\n",
    "    rewards_df.to_csv(rewards_csv_path, index=False)\n",
    "    \n",
    "    # 评估奖励日志\n",
    "    if eval_rewards:\n",
    "        eval_df = pd.DataFrame({\n",
    "            'episode': [i * training_config['eval_interval'] for i in range(len(eval_rewards))],\n",
    "            'eval_reward': eval_rewards\n",
    "        })\n",
    "        eval_csv_path = f\"{log_dir}/time_aware_gru_eval_log{timestamp[4:]}.csv\"\n",
    "        eval_df.to_csv(eval_csv_path, index=False)\n",
    "    \n",
    "    print(f\"训练结果保存:\")\n",
    "    print(f\"  最终模型: {final_checkpoint_path}\")\n",
    "    print(f\"  奖励日志: {rewards_csv_path}\")\n",
    "    if eval_rewards:\n",
    "        print(f\"  评估日志: {eval_csv_path}\")\n",
    "    \n",
    "    # 训练统计\n",
    "    print(f\"\\n训练统计:\")\n",
    "    print(f\"  总训练轮次: {len(rewards_log)}\")\n",
    "    print(f\"  平均奖励: {np.mean(rewards_log):.4f}\")\n",
    "    print(f\"  最佳奖励: {np.max(rewards_log):.4f}\")\n",
    "    print(f\"  最终奖励: {rewards_log[-1]:.4f}\")\n",
    "    if eval_rewards:\n",
    "        print(f\"  最佳评估奖励: {np.max(eval_rewards):.4f}\")\n",
    "    \n",
    "    logging.info(f\"时间感知GRU-TD3训练完成 - 总轮次: {len(rewards_log)}, \"\n",
    "                f\"平均奖励: {np.mean(rewards_log):.4f}, \"\n",
    "                f\"最佳奖励: {np.max(rewards_log):.4f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n训练被用户中断\")\n",
    "    logging.info(\"训练被用户中断\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n训练过程中发生错误: {e}\")\n",
    "    logging.error(f\"训练错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc98d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练结果分析和可视化\n",
    "print(\"=\"*60)\n",
    "print(\"分析和可视化训练结果\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'rewards_log' in locals() and len(rewards_log) > 0:\n",
    "    \n",
    "    # 绘制训练奖励曲线\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 奖励曲线\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(rewards_log, 'b-', alpha=0.6, linewidth=1)\n",
    "    \n",
    "    # 计算移动平均\n",
    "    window_size = min(100, len(rewards_log) // 10)\n",
    "    if window_size > 1:\n",
    "        moving_avg = np.convolve(rewards_log, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(range(window_size-1, len(rewards_log)), moving_avg, 'r-', linewidth=2, \n",
    "                label=f'移动平均 (窗口={window_size})')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.title('时间感知GRU-TD3训练奖励曲线')\n",
    "    plt.xlabel('训练轮次')\n",
    "    plt.ylabel('奖励')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 评估奖励曲线\n",
    "    if eval_rewards:\n",
    "        plt.subplot(2, 2, 2)\n",
    "        eval_episodes = [i * training_config['eval_interval'] for i in range(len(eval_rewards))]\n",
    "        plt.plot(eval_episodes, eval_rewards, 'g-o', linewidth=2, markersize=4)\n",
    "        plt.title('评估奖励曲线')\n",
    "        plt.xlabel('训练轮次')\n",
    "        plt.ylabel('评估奖励')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 损失曲线\n",
    "    if losses_log['actor']:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(losses_log['actor'], 'r-', label='Actor损失', alpha=0.7)\n",
    "        plt.title('Actor网络损失')\n",
    "        plt.xlabel('更新步数')\n",
    "        plt.ylabel('损失')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if losses_log['critic1'] and losses_log['critic2']:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(losses_log['critic1'], 'b-', label='Critic1损失', alpha=0.7)\n",
    "        plt.plot(losses_log['critic2'], 'g-', label='Critic2损失', alpha=0.7)\n",
    "        plt.title('Critic网络损失')\n",
    "        plt.xlabel('更新步数')\n",
    "        plt.ylabel('损失')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plot_dir}/time_aware_gru_td3_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 奖励统计分析\n",
    "    print(\"训练奖励统计分析:\")\n",
    "    print(f\"  总轮次: {len(rewards_log)}\")\n",
    "    print(f\"  平均奖励: {np.mean(rewards_log):.4f}\")\n",
    "    print(f\"  标准差: {np.std(rewards_log):.4f}\")\n",
    "    print(f\"  最小奖励: {np.min(rewards_log):.4f}\")\n",
    "    print(f\"  最大奖励: {np.max(rewards_log):.4f}\")\n",
    "    print(f\"  最终奖励: {rewards_log[-1]:.4f}\")\n",
    "    \n",
    "    # 计算收敛性指标\n",
    "    last_100_rewards = rewards_log[-100:] if len(rewards_log) >= 100 else rewards_log\n",
    "    print(f\"  最后100轮平均奖励: {np.mean(last_100_rewards):.4f}\")\n",
    "    print(f\"  最后100轮标准差: {np.std(last_100_rewards):.4f}\")\n",
    "    \n",
    "    # 趋势分析\n",
    "    if len(rewards_log) > 200:\n",
    "        first_half = rewards_log[:len(rewards_log)//2]\n",
    "        second_half = rewards_log[len(rewards_log)//2:]\n",
    "        improvement = np.mean(second_half) - np.mean(first_half)\n",
    "        print(f\"  训练改进程度: {improvement:.4f}\")\n",
    "    \n",
    "    print(f\"\\n训练曲线已保存到: {plot_dir}/time_aware_gru_td3_training_curves.png\")\n",
    "    \n",
    "    logging.info(f\"训练结果分析完成 - 平均奖励: {np.mean(rewards_log):.4f}, \"\n",
    "                f\"最佳奖励: {np.max(rewards_log):.4f}, \"\n",
    "                f\"最后100轮平均: {np.mean(last_100_rewards):.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"警告: 没有找到训练结果数据\")\n",
    "    logging.warning(\"训练结果数据缺失\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间感知模型测试和性能评估\n",
    "print(\"=\"*60)\n",
    "print(\"时间感知模型性能测试\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 测试训练好的时间感知模型\n",
    "print(\"运行时间感知控制仿真测试...\")\n",
    "test_data_with_control = env.run_simulation(controller=agent)\n",
    "\n",
    "print(\"运行无控制仿真对比...\")\n",
    "test_data_no_control = env.run_simulation(controller=None)\n",
    "\n",
    "# 时间感知性能指标计算\n",
    "def calculate_time_aware_performance_metrics(controlled_data, uncontrolled_data):\n",
    "    \"\"\"计算时间感知控制性能指标\"\"\"\n",
    "    # 主结构位移（索引3）\n",
    "    x2_controlled = controlled_data['all_states'][:, 3]\n",
    "    x2_uncontrolled = uncontrolled_data['all_states'][:, 3]\n",
    "    \n",
    "    # 基础性能指标\n",
    "    rms_controlled = np.sqrt(np.mean(x2_controlled**2))\n",
    "    rms_uncontrolled = np.sqrt(np.mean(x2_uncontrolled**2))\n",
    "    \n",
    "    max_controlled = np.max(np.abs(x2_controlled))\n",
    "    max_uncontrolled = np.max(np.abs(x2_uncontrolled))\n",
    "    \n",
    "    # 减振效果\n",
    "    rms_reduction = (rms_uncontrolled - rms_controlled) / rms_uncontrolled * 100\n",
    "    max_reduction = (max_uncontrolled - max_controlled) / max_uncontrolled * 100\n",
    "    \n",
    "    # 控制能耗\n",
    "    control_energy = np.sum(np.array(controlled_data['actions'][1:])**2) * env.Ts\n",
    "    \n",
    "    # 时间感知特定指标\n",
    "    # 控制力平滑度（相邻时刻控制力变化的标准差）\n",
    "    control_actions = np.array(controlled_data['actions'][1:])\n",
    "    control_smoothness = np.std(np.diff(control_actions)) if len(control_actions) > 1 else 0\n",
    "    \n",
    "    # 响应时间（达到稳态的时间）\n",
    "    settling_time = 0\n",
    "    if len(x2_controlled) > 100:\n",
    "        final_value = np.mean(x2_controlled[-50:])  # 最后50个点的平均值作为稳态值\n",
    "        tolerance = 0.02 * np.max(np.abs(x2_controlled))  # 2%容限\n",
    "        for i in range(len(x2_controlled)-50, 0, -1):\n",
    "            if np.abs(x2_controlled[i] - final_value) > tolerance:\n",
    "                settling_time = i * env.Ts\n",
    "                break\n",
    "    \n",
    "    # 频域性能（在主要频率成分上的抑制效果）\n",
    "    from scipy import signal\n",
    "    freq, psd_controlled = signal.welch(x2_controlled, fs=1/env.Ts, nperseg=min(1024, len(x2_controlled)//4))\n",
    "    freq, psd_uncontrolled = signal.welch(x2_uncontrolled, fs=1/env.Ts, nperseg=min(1024, len(x2_uncontrolled)//4))\n",
    "    \n",
    "    # 在主要频率成分（1Hz, 2.5Hz, 4Hz）附近的抑制效果\n",
    "    target_freqs = [1.0, 2.5, 4.0]\n",
    "    freq_suppressions = []\n",
    "    for target_freq in target_freqs:\n",
    "        freq_idx = np.argmin(np.abs(freq - target_freq))\n",
    "        if freq_idx < len(psd_controlled) and freq_idx < len(psd_uncontrolled):\n",
    "            suppression = (psd_uncontrolled[freq_idx] - psd_controlled[freq_idx]) / psd_uncontrolled[freq_idx] * 100\n",
    "            freq_suppressions.append(suppression)\n",
    "    \n",
    "    avg_freq_suppression = np.mean(freq_suppressions) if freq_suppressions else 0\n",
    "    \n",
    "    return {\n",
    "        'rms_controlled': rms_controlled,\n",
    "        'rms_uncontrolled': rms_uncontrolled,\n",
    "        'rms_reduction': rms_reduction,\n",
    "        'max_controlled': max_controlled,\n",
    "        'max_uncontrolled': max_uncontrolled,\n",
    "        'max_reduction': max_reduction,\n",
    "        'control_energy': control_energy,\n",
    "        'control_smoothness': control_smoothness,\n",
    "        'settling_time': settling_time,\n",
    "        'avg_freq_suppression': avg_freq_suppression\n",
    "    }\n",
    "\n",
    "metrics = calculate_time_aware_performance_metrics(test_data_with_control, test_data_no_control)\n",
    "\n",
    "print(f\"性能指标（时间感知GRU-TD3）:\")\n",
    "print(f\"  RMS位移减少: {metrics['rms_reduction']:.1f}%\")\n",
    "print(f\"  最大位移减少: {metrics['max_reduction']:.1f}%\")\n",
    "print(f\"  平均频域抑制: {metrics['avg_freq_suppression']:.1f}%\")\n",
    "print(f\"  有控制RMS: {metrics['rms_controlled']:.4f} m\")\n",
    "print(f\"  无控制RMS: {metrics['rms_uncontrolled']:.4f} m\")\n",
    "print(f\"  有控制最大位移: {metrics['max_controlled']:.4f} m\")\n",
    "print(f\"  无控制最大位移: {metrics['max_uncontrolled']:.4f} m\")\n",
    "print(f\"  控制能耗: {metrics['control_energy']:.4f}\")\n",
    "print(f\"  控制平滑度: {metrics['control_smoothness']:.4f}\")\n",
    "print(f\"  调节时间: {metrics['settling_time']:.2f} s\")\n",
    "\n",
    "logging.info(f\"时间感知性能测试结果 - RMS减少: {metrics['rms_reduction']:.1f}%, \"\n",
    "            f\"最大位移减少: {metrics['max_reduction']:.1f}%, \"\n",
    "            f\"频域抑制: {metrics['avg_freq_suppression']:.1f}%, \"\n",
    "            f\"控制能耗: {metrics['control_energy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f049c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间感知对比可视化分析\n",
    "print(\"=\"*60)\n",
    "print(\"生成时间感知对比图表\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 设置时间轴\n",
    "time_test = np.arange(len(test_data_with_control['all_states'])) * env.Ts\n",
    "\n",
    "# 创建详细对比图\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "\n",
    "# 位移对比\n",
    "axes[0].plot(time_test, test_data_with_control['all_states'][:, 3], 'b-', \n",
    "            label='时间感知GRU-TD3控制', linewidth=1.5)\n",
    "axes[0].plot(time_test, test_data_no_control['all_states'][:, 3], 'r--', \n",
    "            label='无控制', linewidth=1.5, alpha=0.7)\n",
    "axes[0].set_ylabel('位移 (m)')\n",
    "axes[0].set_title('主结构位移响应对比（时间感知）')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 速度对比\n",
    "axes[1].plot(time_test, test_data_with_control['all_states'][:, 4], 'b-', \n",
    "            label='时间感知GRU-TD3控制', linewidth=1.5)\n",
    "axes[1].plot(time_test, test_data_no_control['all_states'][:, 4], 'r--', \n",
    "            label='无控制', linewidth=1.5, alpha=0.7)\n",
    "axes[1].set_ylabel('速度 (m/s)')\n",
    "axes[1].set_title('主结构速度响应对比（时间感知）')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 控制力时程\n",
    "control_actions = test_data_with_control['actions'][1:]\n",
    "axes[2].plot(time_test[1:], control_actions, 'g-', \n",
    "            label='时间感知GRU-TD3控制力', linewidth=1.5)\n",
    "axes[2].set_ylabel('控制力 (N)')\n",
    "axes[2].set_title('控制力时程（时间感知）')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# 扰动力对比\n",
    "if 'disturbances' in test_data_with_control:\n",
    "    disturbances = test_data_with_control['disturbances']\n",
    "    axes[3].plot(time_test[:len(disturbances)], disturbances, 'k-', \n",
    "                label='时间感知扰动', linewidth=1.0, alpha=0.7)\n",
    "    axes[3].set_ylabel('扰动力 (N)')\n",
    "    axes[3].set_xlabel('时间 (s)')\n",
    "    axes[3].set_title('扰动力时程（含时间噪声）')\n",
    "    axes[3].legend()\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "else:\n",
    "    # 如果没有扰动数据，显示控制力的频谱\n",
    "    from scipy.fft import fft, fftfreq\n",
    "    control_fft = np.abs(fft(control_actions))\n",
    "    freqs = fftfreq(len(control_actions), env.Ts)\n",
    "    positive_freqs = freqs[:len(freqs)//2]\n",
    "    positive_fft = control_fft[:len(control_fft)//2]\n",
    "    \n",
    "    axes[3].plot(positive_freqs, positive_fft, 'g-', linewidth=1.5)\n",
    "    axes[3].set_ylabel('幅值')\n",
    "    axes[3].set_xlabel('频率 (Hz)')\n",
    "    axes[3].set_title('控制力频谱')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    axes[3].set_xlim(0, 10)  # 显示0-10Hz\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plot_dir}/time_aware_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 时间感知特性分析图\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 频域分析\n",
    "from scipy import signal\n",
    "freq, psd_controlled = signal.welch(test_data_with_control['all_states'][:, 3], \n",
    "                                   fs=1/env.Ts, nperseg=min(1024, len(test_data_with_control['all_states'])//4))\n",
    "freq, psd_uncontrolled = signal.welch(test_data_no_control['all_states'][:, 3], \n",
    "                                     fs=1/env.Ts, nperseg=min(1024, len(test_data_no_control['all_states'])//4))\n",
    "\n",
    "axes[0,0].semilogy(freq, psd_controlled, 'b-', label='时间感知控制', linewidth=2)\n",
    "axes[0,0].semilogy(freq, psd_uncontrolled, 'r--', label='无控制', linewidth=2, alpha=0.7)\n",
    "axes[0,0].set_xlabel('频率 (Hz)')\n",
    "axes[0,0].set_ylabel('功率谱密度 (m²/Hz)')\n",
    "axes[0,0].set_title('位移响应功率谱密度对比')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].set_xlim(0, 6)\n",
    "\n",
    "# 控制力统计\n",
    "axes[0,1].hist(control_actions, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0,1].set_xlabel('控制力 (N)')\n",
    "axes[0,1].set_ylabel('频次')\n",
    "axes[0,1].set_title('控制力分布')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 控制力变化率\n",
    "if len(control_actions) > 1:\n",
    "    control_rate = np.diff(control_actions) / env.Ts\n",
    "    axes[1,0].plot(time_test[1:-1], control_rate, 'orange', linewidth=1.0)\n",
    "    axes[1,0].set_xlabel('时间 (s)')\n",
    "    axes[1,0].set_ylabel('控制力变化率 (N/s)')\n",
    "    axes[1,0].set_title('控制力变化率（平滑度指标）')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 误差分析\n",
    "position_error = test_data_with_control['all_states'][:, 3]  # 位移即为误差（目标为0）\n",
    "axes[1,1].plot(time_test, np.abs(position_error), 'purple', linewidth=1.5)\n",
    "axes[1,1].set_xlabel('时间 (s)')\n",
    "axes[1,1].set_ylabel('|位移误差| (m)')\n",
    "axes[1,1].set_title('位移误差绝对值')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plot_dir}/time_aware_detailed_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 性能总结\n",
    "print(\"\\n时间感知GRU-TD3性能总结:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"振动抑制效果:\")\n",
    "print(f\"  - RMS位移减少: {metrics['rms_reduction']:.1f}%\")\n",
    "print(f\"  - 最大位移减少: {metrics['max_reduction']:.1f}%\")\n",
    "print(f\"  - 频域平均抑制: {metrics['avg_freq_suppression']:.1f}%\")\n",
    "print(f\"\\n控制品质:\")\n",
    "print(f\"  - 控制能耗: {metrics['control_energy']:.4f}\")\n",
    "print(f\"  - 控制平滑度: {metrics['control_smoothness']:.4f}\")\n",
    "print(f\"  - 调节时间: {metrics['settling_time']:.2f} s\")\n",
    "print(f\"\\n时间感知特性:\")\n",
    "print(f\"  - 时间噪声标准差: {TIME_NOISE_STD}\")\n",
    "print(f\"  - 可变时间步长: {VARIABLE_TIMESTEP}\")\n",
    "print(f\"  - 时间输入启用: {USE_TIME_INPUT}\")\n",
    "\n",
    "print(f\"\\n所有分析图表已保存完成！\")\n",
    "print(f\"图表保存路径: {plot_dir}\")\n",
    "print(f\"  - 性能对比图: time_aware_performance_comparison.png\")\n",
    "print(f\"  - 详细分析图: time_aware_detailed_analysis.png\")\n",
    "\n",
    "logging.info(\"时间感知GRU-TD3训练和测试完成，所有结果已保存\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
