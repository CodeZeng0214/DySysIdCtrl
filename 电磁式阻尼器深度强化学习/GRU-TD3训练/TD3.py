## TD3 算法定义的函数

import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from typing import Tuple
from nn import Actor, Critic, ReplayBuffer, Gru_Actor, Gru_Critic, Gru_ReplayBuffer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

## TD3代理基类
class BaseTD3Agent:
    def __init__(self, state_dim=1, action_dim=1, hidden_dim=64, action_bound=5.0,
                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.99, tau=0.005, 
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, sigma=0.2, clip_grad=False):
        """初始化TD3代理\n
        - state_dim 状态维度
        - action_dim 动作维度
        - hidden_dim 隐藏层维度
        - action_bound 动作范围
        - actor_lr Actor学习率
        - critic_lr Critic学习率
        - gamma 折扣因子
        - tau 软更新参数
        - policy_noise 目标策略平滑正则化噪声
        - noise_clip 噪声裁剪范围
        - policy_freq 策略更新频率
        - sigma 探索噪声标准差
        - clip_grad 是否使用梯度裁剪
        """
        # 初始化参数
        self.state_dim = state_dim # 状态维度
        self.action_dim = action_dim # 动作维度
        self.hidden_dim = hidden_dim # 隐藏层维度
        self.action_bound = action_bound # 动作范围
        self.actor_lr = actor_lr # Actor网络学习率
        self.critic_lr = critic_lr # Critic网络学习率
        self.gamma = gamma # 折扣因子
        self.tau = tau # 软更新参数
        self.policy_noise = policy_noise # 目标策略平滑正则化噪声
        self.noise_clip = noise_clip # 噪声裁剪范围
        self.policy_freq = policy_freq # 策略更新频率
        self.sigma = sigma # 探索噪声标准差
        self.clip_grad = clip_grad # 是否使用梯度裁剪
        self.model_name = None
        self.total_it = 0 # 总迭代次数
        self.episode_rewards = [] # 存储每个回合的奖励
        self._init_nn()
        self._init_optimizer()

    def _init_nn(self):
        # 需要在子类中定义
        self.actor: Actor | Gru_Actor = None
        self.critic1: Critic | Gru_Critic = None
        self.critic2: Critic | Gru_Critic = None
        self.target_actor: Actor | Gru_Actor = None
        self.target_critic1: Critic | Gru_Critic = None
        self.target_critic2: Critic | Gru_Critic = None
        raise NotImplementedError("需要在子类中初始化神经网络结构")

    def _init_optimizer(self):
        # 优化器设置
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.critic_lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.critic_lr)
        
    def _soft_update(self, source: torch.nn.Module, target: torch.nn.Module):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
            
    def select_action(self, state: np.ndarray, add_noise=True, epsilon=1.0, rand_prob=0.05) -> float:
        """选择动作，支持探索"""
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            action_tensor: torch.Tensor = self.actor(state)
            action_np: np.ndarray = action_tensor.cpu().detach().numpy()
            action = action_np.flatten()
            
        if add_noise:
            noise = np.random.normal(0, self.action_bound * self.sigma * epsilon, size=self.action_dim)
            action += noise
            if np.random.random() < rand_prob:
                action = np.random.uniform(-self.action_bound, self.action_bound, self.action_dim)

        return float(np.clip(action, -self.action_bound, self.action_bound))

    def update(self, replay_buffer: ReplayBuffer) -> Tuple[float, float, float]:
        """更新Actor和Critic网络"""
        if len(replay_buffer) < replay_buffer.batch_size:
            return 0.0, 0.0, 0.0
        
        self.total_it += 1
        
        # 1. 从回放池中采样
        states, actions, rewards, next_states, dones = replay_buffer.sample()
        
        with torch.no_grad():
            # 目标策略平滑正则化
            noise = (torch.randn_like(actions) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)
            next_actions = (self.target_actor(next_states) + noise).clamp(-self.action_bound, self.action_bound)
            
            # 计算目标Q值，取两个Critic的最小值
            target_q1 = self.target_critic1(next_states, next_actions)
            target_q2 = self.target_critic2(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2)
            target_value = rewards + self.gamma * target_q * (1 - dones)
            
        # 2. 更新两个Critic网络
        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        
        critic1_loss = F.mse_loss(current_q1, target_value)
        critic2_loss = F.mse_loss(current_q2, target_value)
        
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        if self.clip_grad:
            torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), max_norm=10)
        self.critic1_optimizer.step()
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        if self.clip_grad:
            torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), max_norm=10)
        self.critic2_optimizer.step()
        
        critic_loss = (critic1_loss + critic2_loss) / 2
        actor_loss = 0.0
        
        # 3. 延迟策略更新
        if self.total_it % self.policy_freq == 0:
            # 更新Actor网络
            policy_actions = self.actor(states)
            actor_loss = -self.critic1(states, policy_actions).mean()
            
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            if self.clip_grad:
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=10)
            self.actor_optimizer.step()
            
            # 软更新目标网络
            self._soft_update(self.actor, self.target_actor)
            self._soft_update(self.critic1, self.target_critic1)
            self._soft_update(self.critic2, self.target_critic2)
            
            actor_loss = actor_loss.item()
        
        return critic_loss.item(), actor_loss, (critic1_loss.item() + critic2_loss.item()) / 2
    
## TD3 代理
class TD3Agent(BaseTD3Agent):
    def __init__(self, state_dim=1, action_dim=1, hidden_dim=64, action_bound=5.0,
                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.99, tau=0.005,
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, sigma=0.2, clip_grad=False):
        # 初始化参数
        super().__init__(state_dim=state_dim, action_dim=action_dim, hidden_dim=hidden_dim, action_bound=action_bound,
                 actor_lr=actor_lr, critic_lr=critic_lr, gamma=gamma, tau=tau,
                 policy_noise=policy_noise, noise_clip=noise_clip, policy_freq=policy_freq, sigma=sigma, clip_grad=clip_grad)
        self._init_nn()

    def _init_nn(self):
        # 网络初始化
        self.actor = Actor(self.state_dim, self.action_dim, self.hidden_dim, self.action_bound).to(device)
        self.critic1 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        self.critic2 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        
        self.target_actor = Actor(self.state_dim, self.action_dim, self.hidden_dim, self.action_bound).to(device)
        self.target_critic1 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        self.target_critic2 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        
        # 复制参数到目标网络
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())
        
## 基于GRU的TD3代理
class Gru_TD3Agent(BaseTD3Agent):
    def __init__(self, state_dim=1, action_dim=1, hidden_dim=64, action_bound=5.0,
                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.99, tau=0.005,
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, sigma=0.2, clip_grad=False, 
                 seq_len=10, num_layers=1):
        self.seq_len = seq_len  # 序列长度
        self.num_layers = num_layers  # GRU层数
        super().__init__(state_dim=state_dim, action_dim=action_dim, hidden_dim=hidden_dim, action_bound=action_bound,
                         actor_lr=actor_lr, critic_lr=critic_lr, gamma=gamma, tau=tau,
                         policy_noise=policy_noise, noise_clip=noise_clip, policy_freq=policy_freq, sigma=sigma, clip_grad=clip_grad)
        # 用于维护状态历史的缓冲区
        self.state_history = []
        self._init_nn()

    def _init_nn(self):
        # GRU网络初始化
        self.actor = Gru_Actor(self.state_dim, self.action_dim, self.hidden_dim, self.action_bound, self.seq_len, self.num_layers).to(device)
        self.critic1 = Gru_Critic(self.state_dim, self.action_dim, self.hidden_dim, self.seq_len, self.num_layers).to(device)
        self.critic2 = Gru_Critic(self.state_dim, self.action_dim, self.hidden_dim, self.seq_len, self.num_layers).to(device)
        
        self.target_actor = Gru_Actor(self.state_dim, self.action_dim, self.hidden_dim, self.action_bound, self.seq_len, self.num_layers).to(device)
        self.target_critic1 = Gru_Critic(self.state_dim, self.action_dim, self.hidden_dim, self.seq_len, self.num_layers).to(device)
        self.target_critic2 = Gru_Critic(self.state_dim, self.action_dim, self.hidden_dim, self.seq_len, self.num_layers).to(device)
        
        # 复制参数到目标网络
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())