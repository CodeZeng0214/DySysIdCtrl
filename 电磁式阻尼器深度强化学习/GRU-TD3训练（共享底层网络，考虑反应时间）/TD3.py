## TD3 ç®—æ³•å®šä¹‰çš„å‡½æ•°

import logging
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Union, List
from nn import Actor, Critic, GruPredictor_norm, ReplayBuffer, Gru_Actor, Gru_Critic, Gru_ReplayBuffer, GruPredictor_diff

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

## TD3ä»£ç†åŸºç±»
class BaseTD3Agent:
    def __init__(self, state_dim=1, action_dim=1, mlp_hidden_dim=128, gru_hidden_dim=64, action_bound=5.0, 
                 actor_lr=1e-3, critic_lr=1e-3, clip_grad=False, gamma=0.99, tau=0.005, 
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, action_sigma=0.2,
                 aware_dt: bool = False, aware_delay_time: bool = False,
                 delay_enabled: bool = False, delay_step: int = 5, delay_sigma: int = 2):
        """åˆå§‹åŒ–TD3ä»£ç†\n
        - state_dim çŠ¶æ€ç»´åº¦
        - action_dim åŠ¨ä½œç»´åº¦
        - mlp_hidden_dim MLPéšè—å±‚ç»´åº¦
        - gru_hidden_dim GRUéšè—å±‚ç»´åº¦
        - action_bound åŠ¨ä½œèŒƒå›´
        - actor_lr Actorå­¦ä¹ ç‡
        - critic_lr Criticå­¦ä¹ ç‡
        - clip_grad æ˜¯å¦ä½¿ç”¨æ¢¯åº¦è£å‰ª
        - gamma æŠ˜æ‰£å› å­
        - tau è½¯æ›´æ–°å‚æ•°
        - policy_noise ç›®æ ‡ç­–ç•¥å¹³æ»‘æ­£åˆ™åŒ–å™ªå£°
        - noise_clip å™ªå£°è£å‰ªèŒƒå›´
        - policy_freq ç­–ç•¥æ›´æ–°é¢‘ç‡
        - action_sigma æ¢ç´¢æœ€å¤§å™ªå£°
        - aware_dt æ˜¯å¦ä½¿ç”¨æ—¶é—´æ­¥é•¿ä½œä¸ºçŠ¶æ€çš„ä¸€éƒ¨åˆ†
        - aware_delay_time æ˜¯å¦å¯ç”¨å»¶è¿Ÿæ„ŸçŸ¥
        - delay_enabled æ˜¯å¦å¯ç”¨åŠ¨ä½œå»¶è¿Ÿ
        - delay_step å»¶è¿Ÿæ­¥æ•°
        - delay_sigma å»¶è¿Ÿæ­¥æ•°çš„æ ‡å‡†å·®
        """
        # åˆå§‹åŒ–å‚æ•°
        self.freeze_gru = False  # æ˜¯å¦å†»ç»“GRUå‚æ•°
        
        self.state_dim = state_dim # çŠ¶æ€ç»´åº¦
        self.action_dim = action_dim # åŠ¨ä½œç»´åº¦
        self.mlp_hidden_dim = mlp_hidden_dim # mlpéšè—å±‚ç»´åº¦
        self.gru_hidden_dim = gru_hidden_dim # gruéšè—å±‚ç»´åº¦
        self.action_bound = action_bound # åŠ¨ä½œèŒƒå›´
        
        self.clip_grad = clip_grad # æ˜¯å¦ä½¿ç”¨æ¢¯åº¦è£å‰ª
        self.actor_lr = actor_lr # Actorç½‘ç»œå­¦ä¹ ç‡
        self.critic_lr = critic_lr # Criticç½‘ç»œå­¦ä¹ ç‡
        self.gamma = gamma # æŠ˜æ‰£å› å­
        self.tau = tau # è½¯æ›´æ–°å‚æ•°
        
        self.policy_noise = policy_noise # ç›®æ ‡ç­–ç•¥å¹³æ»‘æ­£åˆ™åŒ–å™ªå£°
        self.noise_clip = noise_clip # å™ªå£°è£å‰ªèŒƒå›´
        self.policy_freq = policy_freq # ç­–ç•¥æ›´æ–°é¢‘ç‡
        self.action_sigma = action_sigma # æ¢ç´¢æœ€å¤§å™ªå£°
        
        self.aware_dt = aware_dt # æ˜¯å¦ä½¿ç”¨æ—¶é—´æ­¥é•¿ä½œä¸ºçŠ¶æ€çš„ä¸€éƒ¨åˆ†
        self.aware_delay_time = aware_delay_time # æ˜¯å¦å¯ç”¨å»¶è¿Ÿæ„ŸçŸ¥
        self.delay_enabled = delay_enabled # æ˜¯å¦å¯ç”¨åŠ¨ä½œå»¶è¿Ÿ
        self.delay_step = delay_step # å»¶è¿Ÿæ­¥æ•°
        self.delay_sigma = delay_sigma # å»¶è¿Ÿæ­¥æ•°çš„æ ‡å‡†å·®
        
        self.model_name = None
        self.total_it = 0 # æ€»è¿­ä»£æ¬¡æ•°
        self.episode_rewards = [] # å­˜å‚¨æ¯ä¸ªå›åˆçš„å¥–åŠ±

    def _init_nn(self):
        # éœ€è¦åœ¨å­ç±»ä¸­å®šä¹‰
        self.actor: Actor | Gru_Actor = None
        self.critic1: Critic | Gru_Critic = None
        self.critic2: Critic | Gru_Critic = None
        self.target_actor: Actor | Gru_Actor = None
        self.target_critic1: Critic | Gru_Critic = None
        self.target_critic2: Critic | Gru_Critic = None
        self.gru_predictor: GruPredictor_diff = None
        self.target_gru_predictor: GruPredictor_diff = None
        raise NotImplementedError("éœ€è¦åœ¨å­ç±»ä¸­åˆå§‹åŒ–ç¥ç»ç½‘ç»œç»“æ„")

    def _init_optimizer(self):
        # ä¼˜åŒ–å™¨è®¾ç½®
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.critic_lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.critic_lr)
        
    def _soft_update(self, source: torch.nn.Module, target: torch.nn.Module):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
            
    def select_action(self):
        """é€‰æ‹©åŠ¨ä½œ"""
        raise NotImplementedError("éœ€è¦åœ¨å­ç±»ä¸­å®ç°åŠ¨ä½œé€‰æ‹©æ–¹æ³•")
            
    def reset_history(self):
        """é‡ç½®çŠ¶æ€å†å²ï¼Œåœ¨æ–°çš„episodeå¼€å§‹æ—¶è°ƒç”¨"""        
        if hasattr(self, 'state_history'):
            self.state_history = []
        
    def update(self, replay_buffer: Union[ReplayBuffer, Gru_ReplayBuffer]) -> Tuple[float, float, float]:
        """æ›´æ–°Actorå’ŒCriticç½‘ç»œ"""
        if len(replay_buffer) < replay_buffer.batch_size:
            return 0.0, 0.0, 0.0
        
        self.total_it += 1
        
        # 1. ä»å›æ”¾æ± ä¸­é‡‡æ ·
        states, actions, rewards, next_states, dones = replay_buffer.sample()
        
        with torch.no_grad():
            # ç›®æ ‡ç­–ç•¥å¹³æ»‘æ­£åˆ™åŒ–
            noise = (torch.randn_like(actions) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)
            next_actions = (self.target_actor.forward(next_states) + noise).clamp(-self.action_bound, self.action_bound)
            
            # è®¡ç®—ç›®æ ‡Qå€¼ï¼Œå–ä¸¤ä¸ªCriticçš„æœ€å°å€¼
            target_q1 = self.target_critic1(next_states, next_actions)
            target_q2 = self.target_critic2(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2)
            target_value = rewards + self.gamma * target_q * (1 - dones)
        
        self._freeze_gru()
        
        # 2. æ›´æ–°ä¸¤ä¸ªCriticç½‘ç»œ
        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        
        critic1_loss = F.mse_loss(current_q1, target_value)
        critic2_loss = F.mse_loss(current_q2, target_value)
        
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        if self.total_it % 100 == 0: self.check_grad(self.critic1,threshold_high=int(self.clip_grad) if self.clip_grad else 10.0)
        if self.clip_grad:
            torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), max_norm=int(self.clip_grad) if self.clip_grad else 10.0)
        self.critic1_optimizer.step()
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        if self.total_it % 100 == 0: self.check_grad(self.critic2,threshold_high=int(self.clip_grad) if self.clip_grad else 10.0)
        if self.clip_grad:
            torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), max_norm=int(self.clip_grad) if self.clip_grad else 10.0)
        self.critic2_optimizer.step()
        
        critic_loss = (critic1_loss + critic2_loss) / 2
        
        actor_loss = 0.0
        # 3. å»¶è¿Ÿç­–ç•¥æ›´æ–°
        if self.total_it % self.policy_freq == 0:
            # æ›´æ–°Actorç½‘ç»œ
            policy_actions = self.actor(states)
            actor_loss = -self.critic1.forward(states, policy_actions).mean()
            
            self.actor_optimizer.zero_grad()
            actor_loss.backward()

            # æ‰“å°æ¢¯åº¦ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
            if self.total_it % 100 == 0: self.check_grad(self.actor,threshold_high=int(self.clip_grad) if self.clip_grad else 10.0)
            
            if self.clip_grad:
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=int(self.clip_grad) if self.clip_grad else 10.0)
            
            # # å†»ç»“gru_predictorå…±äº«çš„å‚æ•°
            # if self.gru_predictor is not None: self.gru_predictor.freeze_gru()
            
            self.actor_optimizer.step()
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self._soft_update(self.actor, self.target_actor)
            self._soft_update(self.critic1, self.target_critic1)
            self._soft_update(self.critic2, self.target_critic2)
            
            actor_loss = actor_loss.item()
        
        return critic_loss.item(), actor_loss, (critic1_loss.item() + critic2_loss.item()) / 2
  
    def _freeze_gru(self):
        if self.gru_predictor is not None and self.freeze_gru:
            self.actor.gru_predictor.freeze_gru()
            self.target_actor.gru_predictor.freeze_gru()
            self.critic1.gru_predictor.freeze_gru()
            self.target_critic1.gru_predictor.freeze_gru()
            self.critic2.gru_predictor.freeze_gru()
            self.target_critic2.gru_predictor.freeze_gru()
        else:
            pass

    
    def check_grad(self, model: torch.nn.Module, verbose=False, threshold_high=10.0, threshold_low=1e-8):
        """æ£€æŸ¥æ¨¡å‹çš„æ¢¯åº¦æƒ…å†µï¼Œæ”¯æŒè¿­ä»£æ£€æŸ¥å„å±‚
        
        Args:
            model: è¦æ£€æŸ¥çš„æ¨¡å‹
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†çš„æ¯å±‚æ¢¯åº¦ä¿¡æ¯
            threshold_high: æ¢¯åº¦è¿‡é«˜çš„é˜ˆå€¼
            threshold_low: æ¢¯åº¦è¿‡ä½çš„é˜ˆå€¼
        """
        total_grad_norm = 0
        param_count = 0
        layer_stats = {}  # å­˜å‚¨å„å±‚ç»Ÿè®¡ä¿¡æ¯
        
        # 1. è¿­ä»£æ£€æŸ¥å„å±‚æ¢¯åº¦
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.data.norm(2).item()
                total_grad_norm += grad_norm ** 2
                param_count += 1
                
                # æå–å±‚åï¼ˆå»æ‰å‚æ•°ç±»å‹åç¼€ï¼‰
                layer_name = '.'.join(name.split('.')[:-1]) if '.' in name else name
                
                # ç»Ÿè®¡å„å±‚æ¢¯åº¦
                if layer_name not in layer_stats:
                    layer_stats[layer_name] = {
                        'grad_norms': [],
                        'param_names': [],
                        'param_shapes': []
                    }
                
                layer_stats[layer_name]['grad_norms'].append(grad_norm)
                layer_stats[layer_name]['param_names'].append(name)
                layer_stats[layer_name]['param_shapes'].append(tuple(param.shape))
                
                # è¯¦ç»†æ¨¡å¼ï¼šæ‰“å°æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
                if verbose:
                    print(f"  ğŸ“ {name}: shape={param.shape}, grad_norm={grad_norm:.6f}")
                    logging.info(f"  ğŸ“ {name}: shape={param.shape}, grad_norm={grad_norm:.6f}")
        
        total_grad_norm = total_grad_norm ** 0.5
        
        # 2. æ‰“å°æ€»ä½“ç»Ÿè®¡
        # print(f"\nğŸ” æœ¬è½®ç¬¬{self.total_it}æ¬¡æ›´æ–°ï¼Œ {model.__name__} æ¢¯åº¦æ£€æŸ¥æŠ¥å‘Š:")
        logging.info(f"ğŸ” æœ¬è½®ç¬¬{self.total_it}æ¬¡æ›´æ–°ï¼Œ {model.__class__.__name__} æ¢¯åº¦æ£€æŸ¥æŠ¥å‘Š:")
        # print(f"  â””â”€æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")
        logging.info(f"  â””â”€ æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")

        # 3. æ‰“å°å„å±‚ç»Ÿè®¡
        for layer_name, stats in layer_stats.items():
            grad_norms = stats['grad_norms']
            avg_grad = np.mean(grad_norms)
            max_grad = np.max(grad_norms)
            min_grad = np.min(grad_norms)
            
            # åˆ¤æ–­å¼‚å¸¸çŠ¶æ€
            status = "âœ…"
            if max_grad > threshold_high:
                status = "ğŸ”´ è¿‡é«˜"
            elif max_grad < threshold_low:
                status = "âšª è¿‡ä½"
            
            # print(f"  {status} {layer_name}:")
            # print(f"      â”œâ”€ å¹³å‡æ¢¯åº¦: {avg_grad:.6e}")
            # print(f"      â”œâ”€ æœ€å¤§æ¢¯åº¦: {max_grad:.6e}")
            # print(f"      â”œâ”€ æœ€å°æ¢¯åº¦: {min_grad:.6e}")
            # print(f"      â””â”€ å‚æ•°: {stats['param_names']}")
            if verbose:
                # print("\nğŸ“Š å„å±‚æ¢¯åº¦ç»Ÿè®¡:")
                logging.info("ğŸ“Š å„å±‚æ¢¯åº¦ç»Ÿè®¡:")
                logging.info(f"{status} {layer_name} , å‚æ•°: {stats['param_names']} : ")
                logging.info(f"- å¹³å‡: {avg_grad:.6e}, æœ€å¤§: {max_grad:.6e}, æœ€å°: {min_grad:.6e}")
            
        # 4. æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å¼‚å¸¸
        # print("\nâš ï¸  å¼‚å¸¸æ£€æµ‹:")
        if total_grad_norm > threshold_high:
            msg = f"âŒâŒâŒ æ¢¯åº¦çˆ†ç‚¸! æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f} (é˜ˆå€¼: {threshold_high})"
            # print(msg)
            logging.warning(msg)
            
            # æ‰¾å‡ºæ¢¯åº¦æœ€å¤§çš„å±‚
            max_layer = max(layer_stats.items(), key=lambda x: np.max(x[1]['grad_norms']))
            # print(f"   â””â”€ æœ€å¤§æ¢¯åº¦æ¥è‡ª: {max_layer[0]} (æ¢¯åº¦èŒƒæ•°: {np.max(max_layer[1]['grad_norms']):.6f})")
            logging.warning(f"   â””â”€ æœ€å¤§æ¢¯åº¦æ¥è‡ª: {max_layer[0]} (æ¢¯åº¦èŒƒæ•°: {np.max(max_layer[1]['grad_norms']):.6f})")
            
        elif total_grad_norm < threshold_low:
            msg = f"âŒâŒâŒ æ¢¯åº¦æ¶ˆå¤±! æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6e} (é˜ˆå€¼: {threshold_low})"
            # print(msg)
            logging.warning(msg)
        else:
            # print("âœ… æ¢¯åº¦æ­£å¸¸")
            pass
            logging.info("âœ… æ¢¯åº¦æ­£å¸¸")

        # print("-" * 60)
        logging.info("-" * 60)
        
        return {
            'total_grad_norm': total_grad_norm,
            'param_count': param_count,
            'layer_stats': layer_stats
        }
    
## TD3 ä»£ç†
class TD3Agent(BaseTD3Agent):
    def __init__(self, state_dim=1, action_dim=1, hidden_dim=64, action_bound=5.0,
                 actor_lr=1e-3, critic_lr=1e-3, clip_grad=False, gamma=0.99, tau=0.005,
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, action_sigma=0.2,
                 aware_dt: bool = False, aware_delay_time: bool = False,
                 delay_enabled: bool = False, delay_step: int = 5, delay_sigma: int = 2):
        # åˆå§‹åŒ–å‚æ•°
        super().__init__(state_dim=state_dim, action_dim=action_dim, hidden_dim=hidden_dim, action_bound=action_bound,
                 actor_lr=actor_lr, critic_lr=critic_lr, clip_grad=clip_grad, gamma=gamma, tau=tau,
                 policy_noise=policy_noise, noise_clip=noise_clip, policy_freq=policy_freq, action_sigma=action_sigma,
                 aware_dt=aware_dt, aware_delay_time=aware_delay_time,
                 delay_enabled=delay_enabled, delay_step=delay_step, delay_sigma=delay_sigma)
        self._init_nn()
        self._init_optimizer()

    def _init_nn(self):
        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = Actor(self.state_dim, self.action_dim, self.mlp_hidden_dim, self.action_bound).to(device)
        self.critic1 = Critic(self.state_dim, self.action_dim, self.mlp_hidden_dim).to(device)
        self.critic2 = Critic(self.state_dim, self.action_dim, self.mlp_hidden_dim).to(device)
        
        self.target_actor = Actor(self.state_dim, self.action_dim, self.mlp_hidden_dim, self.action_bound).to(device)
        self.target_critic1 = Critic(self.state_dim, self.action_dim, self.mlp_hidden_dim).to(device)
        self.target_critic2 = Critic(self.state_dim, self.action_dim, self.mlp_hidden_dim).to(device)
        
        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())

    def select_action(self, state_history: List[np.ndarray], add_noise=True, epsilon=1.0, rand_prob=0.05, delay=1) -> float:
        """é€‰æ‹©åŠ¨ä½œï¼Œæ”¯æŒæ¢ç´¢"""
        # å¦‚æœå¯ç”¨åŠ¨ä½œå»¶è¿Ÿï¼Œä½¿ç”¨å»¶è¿Ÿæ­¥æ•°çš„é«˜æ–¯åˆ†å¸ƒé‡‡æ ·
        if self.delay_enabled:
            if len(state_history) < delay:
                state = state_history[-1]  # å¦‚æœå†å²é•¿åº¦ä¸å¤Ÿï¼Œä½¿ç”¨æœ€æ–°çŠ¶æ€
            else:
                state = state_history[-delay]  # ä½¿ç”¨å»¶è¿Ÿçš„çŠ¶æ€
                
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            action_tensor: torch.Tensor = self.actor(state)
            action_np: np.ndarray = action_tensor.cpu().detach().numpy()
            action = action_np.flatten()
            
        if add_noise:
            noise = np.random.normal(0, self.action_bound * self.action_sigma * epsilon, size=self.action_dim)
            action += noise
            if np.random.random() < rand_prob:
                action = np.random.uniform(-self.action_bound, self.action_bound, self.action_dim)

        return float(np.clip(action, -self.action_bound, self.action_bound))
        
## åŸºäºGRUçš„TD3ä»£ç†ï¼ˆåˆ†ç¦»å¼æ¶æ„ï¼‰
class Gru_TD3Agent(BaseTD3Agent):
    def __init__(self, norm: bool = False, simple_nn: bool = False,freeze_gru: bool = False,
                 state_dim=1, action_dim=1, mlp_hidden_dim=128, gru_hidden_dim=64, action_bound=5.0,
                 actor_lr=5e-4, critic_lr=1e-3, gru_predictor_lr=1e-3, clip_grad=False, gamma=0.99, tau=0.005,
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, action_sigma=0.2, 
                 aware_dt: bool = False, aware_delay_time: bool = False,
                 delay_enabled: bool = False, delay_step: int = 5, delay_sigma: int = 2,
                 seq_len=10, gru_layers=1, fc_seq_len=1):
        """åˆå§‹åŒ–GRU-TD3ä»£ç†ï¼ˆåˆ†ç¦»å¼æ¶æ„ï¼‰
        
        æ–°å¢å‚æ•°ï¼š
        - gru_predictor_lr: GRUé¢„æµ‹å™¨å­¦ä¹ ç‡
        """
        super().__init__(state_dim=state_dim, action_dim=action_dim, mlp_hidden_dim=mlp_hidden_dim, gru_hidden_dim=gru_hidden_dim, action_bound=action_bound,
                 actor_lr=actor_lr, critic_lr=critic_lr, clip_grad=clip_grad, gamma=gamma, tau=tau,
                 policy_noise=policy_noise, noise_clip=noise_clip, policy_freq=policy_freq, action_sigma=action_sigma,
                 aware_dt=aware_dt, aware_delay_time=aware_delay_time,
                 delay_enabled=delay_enabled, delay_step=delay_step, delay_sigma=delay_sigma)

        self.gru_layers = gru_layers  # GRUå±‚æ•°
        self.seq_len = seq_len  # åºåˆ—é•¿åº¦
        self.fc_seq_len = fc_seq_len  # é¢„æµ‹æ—¶é—´æ­¥é•¿åº¦
        self.gru_predictor_lr = gru_predictor_lr  # GRUé¢„æµ‹å™¨å­¦ä¹ ç‡
        self.freeze_gru = freeze_gru  # æ˜¯å¦å†»ç»“GRUå‚æ•°

        self._init_nn(norm=norm, simple_nn=simple_nn)
        self._init_optimizer()
    
    def _init_nn(self, norm: bool = False, simple_nn: bool = False):
        # åˆ›å»ºå…±äº«çš„GRUé¢„æµ‹å™¨
        # gru_state_dim = 2 + int(self.aware_dt) + int(self.aware_delay_time)  # çŠ¶æ€ç»´åº¦ + æ—¶é—´æ­¥é•¿ + å»¶è¿Ÿæ—¶é—´æ„ŸçŸ¥
        gru_state_dim = self.state_dim
        self.gru_predictor = GruPredictor_norm(norm=norm, simple_nn=simple_nn,freeze_gru=self.freeze_gru,
            state_dim=gru_state_dim, hidden_dim=self.gru_hidden_dim, num_layers=self.gru_layers, fc_seq_len=self.fc_seq_len,
            aware_dt=self.aware_dt, aware_delay_time=self.aware_delay_time
            ).to(device)
        self.gru_predictor1 = GruPredictor_norm(norm=norm, simple_nn=simple_nn,freeze_gru=self.freeze_gru,
            state_dim=gru_state_dim, hidden_dim=self.gru_hidden_dim, num_layers=self.gru_layers, fc_seq_len=self.fc_seq_len,
            aware_dt=self.aware_dt, aware_delay_time=self.aware_delay_time
            ).to(device)
        self.gru_predictor2 = GruPredictor_norm(norm=norm, simple_nn=simple_nn,freeze_gru=self.freeze_gru,
            state_dim=gru_state_dim, hidden_dim=self.gru_hidden_dim, num_layers=self.gru_layers, fc_seq_len=self.fc_seq_len,
            aware_dt=self.aware_dt, aware_delay_time=self.aware_delay_time
            ).to(device)
        self.target_gru_predictor = GruPredictor_norm(norm=norm, simple_nn=simple_nn,freeze_gru=self.freeze_gru,
            state_dim=gru_state_dim, hidden_dim=self.gru_hidden_dim, num_layers=self.gru_layers, fc_seq_len=self.fc_seq_len,
            aware_dt=self.aware_dt, aware_delay_time=self.aware_delay_time
            ).to(device)
        self.target_gru_predictor1 = GruPredictor_norm(norm=norm, simple_nn=simple_nn,freeze_gru=self.freeze_gru,
            state_dim=gru_state_dim, hidden_dim=self.gru_hidden_dim, num_layers=self.gru_layers, fc_seq_len=self.fc_seq_len,
            aware_dt=self.aware_dt, aware_delay_time=self.aware_delay_time
            ).to(device)
        self.target_gru_predictor2 = GruPredictor_norm(norm=norm, simple_nn=simple_nn,freeze_gru=self.freeze_gru,
            state_dim=gru_state_dim, hidden_dim=self.gru_hidden_dim, num_layers=self.gru_layers, fc_seq_len=self.fc_seq_len,
            aware_dt=self.aware_dt, aware_delay_time=self.aware_delay_time
            ).to(device)

        # GRUç½‘ç»œåˆå§‹åŒ–ï¼ˆä¼ å…¥å…±äº«çš„GRUé¢„æµ‹å™¨ï¼‰
        self.actor = Gru_Actor(self.gru_predictor, norm=norm, simple_nn=simple_nn,
                               state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.mlp_hidden_dim, action_bound=self.action_bound).to(device)

        self.critic1 = Gru_Critic(self.gru_predictor1, norm=norm, simple_nn=simple_nn,
                                  state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.mlp_hidden_dim, gru_hidden_dim=self.gru_hidden_dim).to(device)

        self.critic2 = Gru_Critic(self.gru_predictor2, norm=norm, simple_nn=simple_nn,
                                  state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.mlp_hidden_dim, gru_hidden_dim=self.gru_hidden_dim).to(device)

        # ç›®æ ‡ç½‘ç»œä½¿ç”¨ç›®æ ‡GRUé¢„æµ‹å™¨
        self.target_actor = Gru_Actor(self.target_gru_predictor, norm=norm, simple_nn=simple_nn,
                                       state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.mlp_hidden_dim, action_bound=self.action_bound).to(device)

        self.target_critic1 = Gru_Critic(self.target_gru_predictor1, norm=norm, simple_nn=simple_nn,
                                         state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.mlp_hidden_dim, gru_hidden_dim=self.gru_hidden_dim).to(device)

        self.target_critic2 = Gru_Critic(self.target_gru_predictor2, norm=norm, simple_nn=simple_nn,
                                         state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.mlp_hidden_dim, gru_hidden_dim=self.gru_hidden_dim).to(device)

        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())
        
        self.modelnn: List[torch.nn.Module] = [self.actor, self.critic1, self.critic2, self.target_actor, self.target_critic1, self.target_critic2,
                        self.gru_predictor, self.target_gru_predictor, self.gru_predictor1, self.target_gru_predictor1, self.gru_predictor2, self.target_gru_predictor2]

    def _init_optimizer(self):
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.critic_lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.critic_lr)
        
        # âœ… ä¿®æ­£ï¼šç»Ÿè®¡å‚æ•°å…ƒç´ æ•°é‡ï¼Œè€Œéå‚æ•°å¯¹è±¡æ•°é‡
        actor_param_count = sum(p.numel() for p in self.actor.parameters())
        gru_param_count = sum(p.numel() for p in self.gru_predictor.parameters())
        
        # éªŒè¯GRUå‚æ•°æ˜¯å¦è¢«åŒ…å«
        actor_param_ids = set(id(p) for p in self.actor.parameters())
        gru_param_ids = set(id(p) for p in self.gru_predictor.parameters())
        
        if not gru_param_ids.issubset(actor_param_ids):
            raise ValueError("âŒ GRUé¢„æµ‹å™¨å‚æ•°æœªåŒ…å«åœ¨Actorä¼˜åŒ–å™¨ä¸­ï¼")
        
        print(f"âœ… Actorç½‘ç»œæ€»å‚æ•°æ•°é‡: {actor_param_count:,}")
        logging.info(f"âœ… Actorç½‘ç»œæ€»å‚æ•°æ•°é‡: {actor_param_count:,}")
        print(f"âœ… å…¶ä¸­GRUé¢„æµ‹å™¨å‚æ•°æ•°é‡: {gru_param_count:,}")
        logging.info(f"âœ… å…¶ä¸­GRUé¢„æµ‹å™¨å‚æ•°æ•°é‡: {gru_param_count:,}")
        print(f"âœ… Actorä¼˜åŒ–å™¨åŒ…å« {len(actor_param_ids)} ä¸ªå‚æ•°å¯¹è±¡")
        logging.info(f"âœ… Actorä¼˜åŒ–å™¨åŒ…å« {len(actor_param_ids)} ä¸ªå‚æ•°å¯¹è±¡")
        print(f"âœ… å…¶ä¸­GRUé¢„æµ‹å™¨ {len(gru_param_ids)} ä¸ªå‚æ•°å¯¹è±¡")
        logging.info(f"âœ… å…¶ä¸­GRUé¢„æµ‹å™¨ {len(gru_param_ids)} ä¸ªå‚æ•°å¯¹è±¡")
        
        # è¯¦ç»†æ‰“å°å„å±‚å‚æ•°
        print("\nğŸ“Š Actorç½‘ç»œå‚æ•°è¯¦æƒ…:")
        logging.info("ğŸ“Š Actorç½‘ç»œå‚æ•°è¯¦æƒ…:")
        for name, param in self.actor.named_parameters():
            print(f"  - {name}: {param.shape} ({param.numel():,} ä¸ªå…ƒç´ )")
            logging.info(f"  - {name}: {param.shape} ({param.numel():,} ä¸ªå…ƒç´ )")
        
        print("\nğŸ“Š GRUé¢„æµ‹å™¨å‚æ•°è¯¦æƒ…:")
        logging.info("ğŸ“Š GRUé¢„æµ‹å™¨å‚æ•°è¯¦æƒ…:")
        for name, param in self.gru_predictor.named_parameters():
            print(f"  - {name}: {param.shape} ({param.numel():,} ä¸ªå…ƒç´ )")
            logging.info(f"  - {name}: {param.shape} ({param.numel():,} ä¸ªå…ƒç´ )")
    
    def select_action(self, state_history: List[np.ndarray], add_noise=True, epsilon=1.0, rand_prob=0.05, delay=1) -> float:
        """é€‰æ‹©åŠ¨ä½œï¼Œæ”¯æŒæ¢ç´¢"""

        # å¦‚æœå†å²é•¿åº¦ä¸å¤Ÿï¼Œä½¿ç”¨é›¶å¡«å……æˆ–é‡å¤å½“å‰çŠ¶æ€
        required_len = self.seq_len + delay - 1
        if len(state_history) < required_len:
            # ç”¨å½“å‰çŠ¶æ€å¡«å……ä¸è¶³çš„éƒ¨åˆ†
            padding_len = required_len - len(state_history)
            padded_history = [state_history[0]] * padding_len + list(state_history)
        else:
            padded_history = list(state_history)
        
        # å–å»¶è¿Ÿåçš„åºåˆ—
        if delay == 1:
            state_seq = padded_history[-self.seq_len:]
        else:
            state_seq = padded_history[-self.seq_len-delay+1:-delay+1]

        state_seq_tensor = torch.tensor(np.array(state_seq), dtype=torch.float32).unsqueeze(0).to(device)  # [1, seq_len, state_dim]
        
        with torch.no_grad():
            action_tensor: torch.Tensor = self.actor(state_seq_tensor)
            action_np: np.ndarray = action_tensor.cpu().detach().numpy()
            action = action_np.flatten()
            
        if add_noise:
            noise = np.random.normal(0, self.action_bound * self.action_sigma * epsilon, size=self.action_dim)
            action += noise
            if np.random.random() < rand_prob:
                action = np.random.uniform(-self.action_bound, self.action_bound, self.action_dim)

        return float(np.clip(action, -self.action_bound, self.action_bound))
    
        # with torch.no_grad():
        #     state = state_seq_tensor
        #     action_tensor: torch.Tensor = self.actor(state)
        #     action_np: np.ndarray = action_tensor.cpu().detach().numpy()
        #     action = action_np.flatten()
            
        # if add_noise:
        #     noise = np.random.normal(0, self.action_sigma * epsilon, size=self.action_dim)
        #     action += noise
        #     if np.random.random() < rand_prob:
        #         action = np.random.uniform(-self.action_bound, self.action_bound, self.action_dim)

        # return float(np.clip(action, -1, 1))