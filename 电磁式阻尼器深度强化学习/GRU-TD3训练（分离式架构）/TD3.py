## TD3 ç®—æ³•å®šä¹‰çš„å‡½æ•°

import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Union, List
from nn import Actor, Critic, ReplayBuffer, Gru_Actor, Gru_Critic, Gru_ReplayBuffer, GruPredictor, GruPredictorBuffer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

## TD3ä»£ç†åŸºç±»
class BaseTD3Agent:
    def __init__(self, state_dim=1, action_dim=1, hidden_dim=64, action_bound=5.0,
                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.99, tau=0.005, 
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, policy_sigma=0.2, clip_grad=False,
                 aware_dt: bool = False,
                 delay_enabled: bool = False, delay_step: int = 5, delay_sigma: int = 2, aware_delay_time: bool = False):
        """åˆå§‹åŒ–TD3ä»£ç†\n
        - state_dim çŠ¶æ€ç»´åº¦
        - action_dim åŠ¨ä½œç»´åº¦
        - hidden_dim éšè—å±‚ç»´åº¦
        - action_bound åŠ¨ä½œèŒƒå›´
        - actor_lr Actorå­¦ä¹ ç‡
        - critic_lr Criticå­¦ä¹ ç‡
        - gamma æŠ˜æ‰£å› å­
        - tau è½¯æ›´æ–°å‚æ•°
        - policy_noise ç›®æ ‡ç­–ç•¥å¹³æ»‘æ­£åˆ™åŒ–å™ªå£°
        - noise_clip å™ªå£°è£å‰ªèŒƒå›´
        - policy_freq ç­–ç•¥æ›´æ–°é¢‘ç‡
        - policy_sigma æ¢ç´¢å™ªå£°æ ‡å‡†å·®
        - clip_grad æ˜¯å¦ä½¿ç”¨æ¢¯åº¦è£å‰ª
        - delay_enabled: æ˜¯å¦å¯ç”¨åŠ¨ä½œå»¶è¿Ÿ
        - delay_step: å»¶è¿Ÿæ­¥æ•°
        - delay_sigma: å»¶è¿Ÿæ­¥æ•°çš„æ ‡å‡†å·®
        """
        # åˆå§‹åŒ–å‚æ•°
        self.state_dim = state_dim # çŠ¶æ€ç»´åº¦
        self.action_dim = action_dim # åŠ¨ä½œç»´åº¦
        self.hidden_dim = hidden_dim # éšè—å±‚ç»´åº¦
        self.action_bound = action_bound # åŠ¨ä½œèŒƒå›´
        self.actor_lr = actor_lr # Actorç½‘ç»œå­¦ä¹ ç‡
        self.critic_lr = critic_lr # Criticç½‘ç»œå­¦ä¹ ç‡
        self.gamma = gamma # æŠ˜æ‰£å› å­
        self.tau = tau # è½¯æ›´æ–°å‚æ•°
        self.policy_noise = policy_noise # ç›®æ ‡ç­–ç•¥å¹³æ»‘æ­£åˆ™åŒ–å™ªå£°
        self.noise_clip = noise_clip # å™ªå£°è£å‰ªèŒƒå›´
        self.policy_freq = policy_freq # ç­–ç•¥æ›´æ–°é¢‘ç‡
        self.policy_sigma = policy_sigma # æ¢ç´¢å™ªå£°æ ‡å‡†å·®
        self.clip_grad = clip_grad # æ˜¯å¦ä½¿ç”¨æ¢¯åº¦è£å‰ª
        self.aware_dt = aware_dt # æ˜¯å¦ä½¿ç”¨æ—¶é—´æ­¥é•¿ä½œä¸ºçŠ¶æ€çš„ä¸€éƒ¨åˆ†
        self.delay_enabled = delay_enabled # æ˜¯å¦å¯ç”¨åŠ¨ä½œå»¶è¿Ÿ
        self.delay_step = delay_step # å»¶è¿Ÿæ­¥æ•°
        self.delay_sigma = delay_sigma # å»¶è¿Ÿæ­¥æ•°çš„æ ‡å‡†å·®
        self.aware_delay_time = aware_delay_time # æ˜¯å¦å¯ç”¨å»¶è¿Ÿæ„ŸçŸ¥

        self.model_name = None
        self.total_it = 0 # æ€»è¿­ä»£æ¬¡æ•°
        self.episode_rewards = [] # å­˜å‚¨æ¯ä¸ªå›åˆçš„å¥–åŠ±

    def _init_nn(self):
        # éœ€è¦åœ¨å­ç±»ä¸­å®šä¹‰
        self.actor: Actor | Gru_Actor = None
        self.critic1: Critic | Gru_Critic = None
        self.critic2: Critic | Gru_Critic = None
        self.target_actor: Actor | Gru_Actor = None
        self.target_critic1: Critic | Gru_Critic = None
        self.target_critic2: Critic | Gru_Critic = None
        self.gru_predictor: GruPredictor = None
        raise NotImplementedError("éœ€è¦åœ¨å­ç±»ä¸­åˆå§‹åŒ–ç¥ç»ç½‘ç»œç»“æ„")

    def _init_optimizer(self):
        # ä¼˜åŒ–å™¨è®¾ç½®
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.critic_lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.critic_lr)
        
    def _soft_update(self, source: torch.nn.Module, target: torch.nn.Module):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
            
    def select_action(self):
        """é€‰æ‹©åŠ¨ä½œ"""
        raise NotImplementedError("éœ€è¦åœ¨å­ç±»ä¸­å®ç°åŠ¨ä½œé€‰æ‹©æ–¹æ³•")
            
    def reset_history(self):
        """é‡ç½®çŠ¶æ€å†å²ï¼Œåœ¨æ–°çš„episodeå¼€å§‹æ—¶è°ƒç”¨"""        
        if hasattr(self, 'state_history'):
            self.state_history = []
        
    def update(self, replay_buffer: Union[ReplayBuffer, Gru_ReplayBuffer]) -> Tuple[float, float, float]:
        """æ›´æ–°Actorå’ŒCriticç½‘ç»œ"""
        if len(replay_buffer) < replay_buffer.batch_size:
            return 0.0, 0.0, 0.0
        
        self.total_it += 1
        
        # 1. ä»å›æ”¾æ± ä¸­é‡‡æ ·
        states, actions, rewards, next_states, dones = replay_buffer.sample()
        
        # å¦‚æœä½¿ç”¨GRUé¢„æµ‹å™¨ï¼Œå…ˆé¢„æµ‹çŠ¶æ€
        if self.gru_predictor is not None:
            states = self.predict_states(states)
            next_states = self.predict_states(next_states)

        with torch.no_grad():
            # ç›®æ ‡ç­–ç•¥å¹³æ»‘æ­£åˆ™åŒ–
            noise = (torch.randn_like(actions) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)
            next_actions = (self.target_actor(next_states) + noise).clamp(-self.action_bound, self.action_bound)
            
            # è®¡ç®—ç›®æ ‡Qå€¼ï¼Œå–ä¸¤ä¸ªCriticçš„æœ€å°å€¼
            target_q1 = self.target_critic1(next_states, next_actions)
            target_q2 = self.target_critic2(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2)
            target_value = rewards + self.gamma * target_q * (1 - dones)
            
        # 2. æ›´æ–°ä¸¤ä¸ªCriticç½‘ç»œ
        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        
        critic1_loss = F.mse_loss(current_q1, target_value)
        critic2_loss = F.mse_loss(current_q2, target_value)
        
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        if self.clip_grad:
            torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), max_norm=10)
        self.critic1_optimizer.step()
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        if self.clip_grad:
            torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), max_norm=10)
        self.critic2_optimizer.step()
        
        critic_loss = (critic1_loss + critic2_loss) / 2
        actor_loss = 0.0
        
        # 3. å»¶è¿Ÿç­–ç•¥æ›´æ–°
        if self.total_it % self.policy_freq == 0:
            # æ›´æ–°Actorç½‘ç»œ
            policy_actions = self.actor(states)
            actor_loss = -self.critic1(states, policy_actions).mean()
            
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            
            # æ‰“å°æ¢¯åº¦ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
            total_grad_norm = 0
            param_count = 0
            for name, param in self.actor.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.data.norm(2)
                    total_grad_norm += grad_norm.item() ** 2
                    param_count += 1
                    if self.total_it % 1000 == 0:  # æ¯1000æ¬¡æ‰“å°ä¸€æ¬¡
                        pass # æ‰“å°æ¢¯åº¦ä¿¡æ¯
                        #print(f"  {name}: grad_norm={grad_norm:.6f}")
            total_grad_norm = total_grad_norm ** (1. / 2)
            if self.total_it % 1000 == 0:
                pass # æ‰“å°æ¢¯åº¦ä¿¡æ¯
                #print(f"ğŸ” Actoræ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}, å‚æ•°æ•°é‡: {param_count}")
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦çˆ†ç‚¸
            if self.total_it % 1000 == 0 and total_grad_norm > 10:
                pass # æ‰“å°æ¢¯åº¦ä¿¡æ¯
                #print(f"âš ï¸ è­¦å‘Š: Actoræ¢¯åº¦è¿‡é«˜! æ¢¯åº¦èŒƒæ•°: {total_grad_norm}")
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸ºé›¶
            if self.total_it % 1000 == 0 and total_grad_norm < 1e-8:
                pass # æ‰“å°æ¢¯åº¦ä¿¡æ¯
                #print(f"âš ï¸ è­¦å‘Š: Actoræ¢¯åº¦å‡ ä¹ä¸ºé›¶! æ¢¯åº¦èŒƒæ•°: {total_grad_norm}")
            
            if self.clip_grad:
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=10)
            self.actor_optimizer.step()
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self._soft_update(self.actor, self.target_actor)
            self._soft_update(self.critic1, self.target_critic1)
            self._soft_update(self.critic2, self.target_critic2)
            
            actor_loss = actor_loss.item()
        
        return critic_loss.item(), actor_loss, (critic1_loss.item() + critic2_loss.item()) / 2
    
    def predict_states(self, state_seq: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä½¿ç”¨GRUé¢„æµ‹æœªæ¥çŠ¶æ€åºåˆ—ï¼Œè¾“å‡ºæ‹¼æ¥åçš„çŠ¶æ€åºåˆ—å’Œéšè—çŠ¶æ€åºåˆ—"""
        # ä½¿ç”¨å…±äº«çš„GRUé¢„æµ‹å™¨è·å–é¢„æµ‹åºåˆ—
        with torch.no_grad():  # GRUé¢„æµ‹å™¨çš„è¾“å‡ºä¸å‚ä¸æ¢¯åº¦è®¡ç®—
            if self.aware_dt or self.aware_delay_time:  # åˆ¤æ–­æ˜¯å¦åŒ…å«æ„ŸçŸ¥çŠ¶æ€
                gru_state_seq = torch.cat((state_seq[:,:,0].unsqueeze(2),state_seq[:,:,3].unsqueeze(2),state_seq[:,:,6:]), dim=2) # å¤„ç†çŠ¶æ€åºåˆ—ï¼ŒæŠ›å¼ƒé€Ÿåº¦å’ŒåŠ é€Ÿåº¦ç»´åº¦
            else:
                gru_state_seq = torch.cat((state_seq[:,:,0].unsqueeze(2),state_seq[:,:,3].unsqueeze(2)), dim=2) # å¤„ç†çŠ¶æ€åºåˆ—ï¼ŒæŠ›å¼ƒé€Ÿåº¦å’ŒåŠ é€Ÿåº¦ç»´åº¦

            gru_state_fc_seq, h_fc_seq = self.gru_predictor.forward(gru_state_seq)  # [batch_size, fc_seq_len, state_dim], [batch_size, fc_seq_len, hidden_dim]

            # è®¡ç®—é€Ÿåº¦å’ŒåŠ é€Ÿåº¦ï¼Œæ‹¼æ¥åˆ°çŠ¶æ€åºåˆ—
            vel_fc_seq, acc_fc_seq = self.compute_derivatives(gru_state_fc_seq[:,:,0:2],gru_state_fc_seq[:,:,2] if self.aware_dt else None)
            state_fc_seq = torch.cat((gru_state_fc_seq[:,:,0].unsqueeze(2), vel_fc_seq[:,:,0].unsqueeze(2), acc_fc_seq[:,:,0].unsqueeze(2), 
                                      gru_state_fc_seq[:,:,1].unsqueeze(2), vel_fc_seq[:,:,1].unsqueeze(2), acc_fc_seq[:,:,1].unsqueeze(2), 
                                      gru_state_fc_seq[:,:,2:]), dim=2)  # [batch_size, fc_seq_len, state_dim]

            state_combined_seq = torch.cat((state_seq[:, -self.gru_predictor.fc_seq_len//2:, :], state_fc_seq), dim=1)  # [batch_size, 1.5*fc_seq_len, state_dim]
            h_combined_seq = torch.cat((self.gru_predictor.gru_out[:, -self.gru_predictor.fc_seq_len//2:, :], h_fc_seq), dim=1)  # [batch_size, 1.5*fc_seq_len, hidden_dim]
            
            return state_combined_seq, h_combined_seq

    def compute_derivatives(self, positions: torch.Tensor, dt_seq: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """é€šè¿‡æ•°å€¼å¾®åˆ†è®¡ç®—é€Ÿåº¦å’ŒåŠ é€Ÿåº¦
        
        Args:
            positions: ä½ç§»åºåˆ— [batch_size, seq_len, position_dim]
            dt_seq: æ—¶é—´æ­¥é•¿åºåˆ— [batch_size, seq_len-1] (å¯é€‰)
        
        Returns:
            velocities: é€Ÿåº¦åºåˆ— [batch_size, seq_len, position_dim]
            accelerations: åŠ é€Ÿåº¦åºåˆ— [batch_size, seq_len, position_dim]
        """
        batch_size, seq_len, pos_dim = positions.shape
        
        # å¦‚æœæ²¡æœ‰æä¾›æ—¶é—´æ­¥é•¿ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if dt_seq is None:
            dt_seq = torch.full((batch_size, seq_len - 1), 0.001, 
                                device=positions.device)
        
        # è®¡ç®—é€Ÿåº¦: v(t) = [x(t+1) - x(t)] / dt
        velocities = torch.zeros_like(positions)
        velocities[:, :-1, :] = (positions[:, 1:, :] - positions[:, :-1, :]) / dt_seq.unsqueeze(-1)
        # æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„é€Ÿåº¦é€šè¿‡å¤–æ¨ä¼°è®¡
        velocities[:, -1, :] = velocities[:, -2, :]
        
        # è®¡ç®—åŠ é€Ÿåº¦: a(t) = [v(t+1) - v(t)] / dt
        accelerations = torch.zeros_like(positions)
        accelerations[:, :-1, :] = (velocities[:, 1:, :] - velocities[:, :-1, :]) / dt_seq.unsqueeze(-1)
        # æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„åŠ é€Ÿåº¦é€šè¿‡å¤–æ¨ä¼°è®¡
        accelerations[:, -1, :] = accelerations[:, -2, :]
        
        return velocities, accelerations
    
## TD3 ä»£ç†
class TD3Agent(BaseTD3Agent):
    def __init__(self, state_dim=1, action_dim=1, hidden_dim=64, action_bound=5.0,
                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.99, tau=0.005,
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, sigma=0.2, clip_grad=False,
                 aware_dt: bool = False,
                 delay_enabled: bool = False, delay_step: int = 5, delay_sigma: int = 2, aware_delay_time: bool = False):
        # åˆå§‹åŒ–å‚æ•°
        super().__init__(state_dim=state_dim, action_dim=action_dim, hidden_dim=hidden_dim, action_bound=action_bound,
                 actor_lr=actor_lr, critic_lr=critic_lr, gamma=gamma, tau=tau,
                 policy_noise=policy_noise, noise_clip=noise_clip, policy_freq=policy_freq, policy_sigma=sigma, clip_grad=clip_grad,
                 aware_dt=aware_dt,
                 delay_enabled=delay_enabled, delay_step=delay_step, delay_sigma=delay_sigma, aware_delay_time=aware_delay_time)
        self._init_nn()
        self._init_optimizer()

    def _init_nn(self):
        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = Actor(self.state_dim, self.action_dim, self.hidden_dim, self.action_bound).to(device)
        self.critic1 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        self.critic2 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        
        self.target_actor = Actor(self.state_dim, self.action_dim, self.hidden_dim, self.action_bound).to(device)
        self.target_critic1 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        self.target_critic2 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        
        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())

    def select_action(self, state_history: List[np.ndarray], add_noise=True, epsilon=1.0, rand_prob=0.05, delay=1) -> float:
        """é€‰æ‹©åŠ¨ä½œï¼Œæ”¯æŒæ¢ç´¢"""
        # å¦‚æœå¯ç”¨åŠ¨ä½œå»¶è¿Ÿï¼Œä½¿ç”¨å»¶è¿Ÿæ­¥æ•°çš„é«˜æ–¯åˆ†å¸ƒé‡‡æ ·
        if self.delay_enabled:
            if len(state_history) < delay:
                state = state_history[-1]  # å¦‚æœå†å²é•¿åº¦ä¸å¤Ÿï¼Œä½¿ç”¨æœ€æ–°çŠ¶æ€
            else:
                state = state_history[-delay]  # ä½¿ç”¨å»¶è¿Ÿçš„çŠ¶æ€
                
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            action_tensor: torch.Tensor = self.actor(state)
            action_np: np.ndarray = action_tensor.cpu().detach().numpy()
            action = action_np.flatten()
            
        if add_noise:
            noise = np.random.normal(0, self.action_bound * self.policy_sigma * epsilon, size=self.action_dim)
            action += noise
            if np.random.random() < rand_prob:
                action = np.random.uniform(-self.action_bound, self.action_bound, self.action_dim)

        return float(np.clip(action, -self.action_bound, self.action_bound))
        
## åŸºäºGRUçš„TD3ä»£ç†ï¼ˆåˆ†ç¦»å¼æ¶æ„ï¼‰
class Gru_TD3Agent(BaseTD3Agent):
    def __init__(self, state_dim=1, action_dim=1, hidden_dim=64, action_bound=5.0,
                 actor_lr=1e-3, critic_lr=1e-3, gru_predictor_lr=1e-3,gamma=0.99, tau=0.005,
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, sigma=0.2, clip_grad=False, 
                 seq_len=10, gru_layers=1, fc_seq_len=1,
                 aware_dt: bool = False,
                 delay_enabled: bool = False, delay_step: int = 5, delay_sigma: int = 2, aware_delay_time: bool = False,
                 ):
        """åˆå§‹åŒ–GRU-TD3ä»£ç†ï¼ˆåˆ†ç¦»å¼æ¶æ„ï¼‰
        
        æ–°å¢å‚æ•°ï¼š
        - gru_predictor_lr: GRUé¢„æµ‹å™¨å­¦ä¹ ç‡
        """
        self.fc_seq_len = fc_seq_len  # é¢„æµ‹æ—¶é—´æ­¥é•¿åº¦
        self.seq_len = seq_len  # åºåˆ—é•¿åº¦
        self.gru_layers = gru_layers  # GRUå±‚æ•°
        self.gru_predictor_lr = gru_predictor_lr  # GRUé¢„æµ‹å™¨å­¦ä¹ ç‡
        
        super().__init__(state_dim=state_dim, action_dim=action_dim, hidden_dim=hidden_dim, action_bound=action_bound,
                         actor_lr=actor_lr, critic_lr=critic_lr, gamma=gamma, tau=tau,
                         policy_noise=policy_noise, noise_clip=noise_clip, policy_freq=policy_freq, policy_sigma=sigma, clip_grad=clip_grad,
                         aware_dt=aware_dt,
                         delay_enabled=delay_enabled, delay_step=delay_step, delay_sigma=delay_sigma, aware_delay_time=aware_delay_time)
        self._init_nn()
        self._init_optimizer()
    
    def _init_nn(self):
        # åˆ›å»ºå…±äº«çš„GRUé¢„æµ‹å™¨
        gru_state_dim = 2 + int(self.aware_dt) + int(self.aware_delay_time)  # çŠ¶æ€ç»´åº¦ + æ—¶é—´æ­¥é•¿ + å»¶è¿Ÿæ—¶é—´æ„ŸçŸ¥
        self.gru_predictor = GruPredictor(
            state_dim=gru_state_dim, hidden_dim=self.hidden_dim, num_layers=self.gru_layers, fc_seq_len=self.fc_seq_len
            ).to(device)
        
        # GRUç½‘ç»œåˆå§‹åŒ–ï¼ˆä¼ å…¥å…±äº«çš„GRUé¢„æµ‹å™¨ï¼‰
        self.actor = Gru_Actor(state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.hidden_dim, action_bound=self.action_bound).to(device)
        
        self.critic1 = Gru_Critic(state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.hidden_dim).to(device)
        
        self.critic2 = Gru_Critic(state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.hidden_dim).to(device)
        
        # ç›®æ ‡ç½‘ç»œä½¿ç”¨ç›®æ ‡GRUé¢„æµ‹å™¨
        self.target_actor = Gru_Actor(state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.hidden_dim, action_bound=self.action_bound).to(device)

        self.target_critic1 = Gru_Critic(state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.hidden_dim).to(device)

        self.target_critic2 = Gru_Critic(state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.hidden_dim).to(device)
        
        self.target_critic2 = Gru_Critic(state_dim=self.state_dim, action_dim=self.action_dim, hidden_dim=self.hidden_dim).to(device)

        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())

    def _init_optimizer(self):
        # GRUé¢„æµ‹å™¨ä¼˜åŒ–å™¨
        self.gru_predictor_optimizer = optim.Adam(self.gru_predictor.parameters(), lr=self.gru_predictor_lr)
        
        # Actorå’ŒCriticä¼˜åŒ–å™¨ï¼ˆä¸åŒ…å«GRUé¢„æµ‹å™¨å‚æ•°ï¼‰
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.critic_lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.critic_lr)
    
    def update_gru_predictor(self, predictor_buffer: GruPredictorBuffer) -> float:
        """å•ç‹¬æ›´æ–°GRUé¢„æµ‹å™¨
        Args:
            predictor_buffer: GRUé¢„æµ‹å™¨ä¸“ç”¨å›æ”¾æ± 
        Returns:
            predictor_loss: é¢„æµ‹æŸå¤±
        """
        if len(predictor_buffer) < predictor_buffer.batch_size:
            return 0.0
        
        # é‡‡æ ·è®­ç»ƒæ•°æ®
        delayed_seqs, true_future_seqs = predictor_buffer.sample()
        
        # å‰å‘ä¼ æ’­
        predicted_seqs, _ = self.gru_predictor(delayed_seqs)
        
        # è®¡ç®—MSEæŸå¤±
        predictor_loss = F.mse_loss(predicted_seqs, true_future_seqs)
        
        # åå‘ä¼ æ’­
        self.gru_predictor_optimizer.zero_grad()
        predictor_loss.backward()
        if self.clip_grad:
            torch.nn.utils.clip_grad_norm_(self.gru_predictor.parameters(), max_norm=10)
        self.gru_predictor_optimizer.step()
        
        return predictor_loss.item()

    def select_action(self, state_history: List[np.ndarray], add_noise=True, epsilon=1.0, rand_prob=0.05, delay=1) -> float:
        """é€‰æ‹©åŠ¨ä½œï¼Œæ”¯æŒæ¢ç´¢"""

        # å¦‚æœå†å²é•¿åº¦ä¸å¤Ÿï¼Œä½¿ç”¨é›¶å¡«å……æˆ–é‡å¤å½“å‰çŠ¶æ€
        if len(state_history) < (self.seq_len + delay - 1):
            # ç”¨å½“å‰çŠ¶æ€å¡«å……ä¸è¶³çš„éƒ¨åˆ†
            if len(state_history) < delay:
                # å¦‚æœå†å²é•¿åº¦è¿delayéƒ½ä¸å¤Ÿï¼Œç”¨èµ·å§‹çŠ¶æ€å¡«å……
                padded_history = [state_history[0]] * (self.seq_len + delay - 1)
            else:
                # ä¿®å¤delay=1æ—¶çš„åˆ‡ç‰‡é—®é¢˜
                if delay == 1:
                    padded_history = np.concatenate([[state_history[0]] * (self.seq_len - len(state_history)), state_history])
                else:
                    padded_history = np.concatenate([[state_history[0]] * ((self.seq_len + delay-1) - len(state_history)), state_history[:-(delay-1)]])
        else:
            # ä¿æŒæœ€è¿‘çš„seq_lenä¸ªçŠ¶æ€
            if delay == 1:
                padded_history = state_history[-self.seq_len:]  # å½“delay=1æ—¶ï¼Œç›´æ¥å–æœ€åseq_lenä¸ªå…ƒç´ 
            else:
                padded_history = state_history[-self.seq_len-(delay-1):-(delay-1)]        # æ„å»ºçŠ¶æ€åºåˆ—
        state_seq = np.array(padded_history)  # [seq_len, state_dim]
        state_seq_tensor = torch.tensor(state_seq, dtype=torch.float32).unsqueeze(0).to(device)  # [1, seq_len, state_dim]

        with torch.no_grad():
            if self.gru_predictor is not None:
                states = self.predict_states(state_seq_tensor)  # ä½¿ç”¨GRUé¢„æµ‹æœªæ¥çŠ¶æ€åºåˆ—
            action_tensor: torch.Tensor = self.actor(states)
            action_np: np.ndarray = action_tensor.cpu().detach().numpy()
            action = action_np.flatten()
            
        if add_noise:
            noise = np.random.normal(0, self.action_bound * self.policy_sigma * epsilon, size=self.action_dim)
            action += noise
            if np.random.random() < rand_prob:
                action = np.random.uniform(-self.action_bound, self.action_bound, self.action_dim)

        return float(np.clip(action, -self.action_bound, self.action_bound))