# GRU-TD3 分离式架构使用说明

## 架构概述

### 新架构特点
1. **分离的GRU预测器**：GRU预测层被独立出来，单独进行在线训练
2. **共享预测层**：Actor和Critic网络共享同一个GRU预测器作为第一层
3. **注意力机制**：在GRU预测后使用注意力层处理时间延迟
4. **在线训练**：GRU预测器在训练过程中独立更新，学习从时延状态序列预测真实未来状态

### 网络结构
```
输入时延状态序列 -> [共享GRU预测器] -> 预测未来状态序列
                                          ↓
                                      [注意力层]
                                          ↓
                                   加权上下文向量
                                          ↓
                            ┌─────────────┴─────────────┐
                            ↓                           ↓
                    [Actor全连接层]              [Critic全连接层 + 动作]
                            ↓                           ↓
                         动作输出                    Q值输出
```

## 主要组件

### 1. GruPredictor（独立GRU预测器）
- **功能**：从时延输入序列预测未来状态序列
- **输入**：时延状态序列 `[batch_size, seq_len, state_dim]`
- **输出**：预测的未来状态序列 `[batch_size, pre_seq_len, state_dim]`
- **训练**：使用MSE损失，目标是真实的未来状态（从环境的无时延状态历史中获取）

### 2. Gru_Actor（策略网络）
- 使用共享的GRU预测器
- 注意力层处理预测序列
- 全连接层输出动作
- GRU预测器参数在Actor训练时被冻结

### 3. Gru_Critic（价值网络）
- 使用共享的GRU预测器
- 注意力层处理预测序列
- 将加权状态与动作拼接后通过全连接层输出Q值
- GRU预测器参数在Critic训练时被冻结

### 4. GruPredictorBuffer（GRU预测器专用回放池）
- 存储时延状态序列和对应的真实未来状态序列
- 从完整的无时延状态历史中提取训练样本
- 支持不同延迟步数的数据收集

### 5. Gru_TD3Agent（改进的代理）
- 包含独立的GRU预测器训练方法 `update_gru_predictor()`
- Actor和Critic的更新不影响GRU预测器
- 支持GRU预测器的软更新到目标网络

## 使用示例

```python
import numpy as np
from env import ElectromagneticDamperEnv
from TD3 import Gru_TD3Agent
from nn import Gru_ReplayBuffer, GruPredictorBuffer
from train import train_td3
import fx

# 1. 设置系统参数
m1, m2 = 10, 100
c1, c2 = 50, 100
k1, k2 = 1000, 5000
A = np.array([[0, 1, 0, 0],
              [-(k1+k2)/m1, -(c1+c2)/m1, k2/m1, c2/m1],
              [0, 0, 0, 1],
              [k2/m2, c2/m2, -k2/m2, -c2/m2]])
B = np.array([[0], [1/m1], [0], [0]])
C = np.array([[-(k1+k2)/m1, -(c1+c2)/m1, k2/m1, c2/m1],
              [k2/m2, c2/m2, -k2/m2, -c2/m2]])
D = np.array([[1/m1], [0]])
E = np.array([[0, 0], [k1/m1, c1/m1], [0, 0], [0, 0]])

# 2. 创建环境
env = ElectromagneticDamperEnv(
    A=A, B=B, C=C, D=D, E=E, 
    Ts=0.001, T=10,
    z_func=fx.sin_wave(amplitude=0.01, frequency=1.0),
    r_func=fx.tolerance_if_rf(tolerance=1e-3),
    obs_indices=[3],  # 只观测平台位移
    use_dt_noise=True,  # 使用时间步长噪声
    dt_noise_std=0.01
)

# 3. 创建GRU-TD3代理（分离式架构）
agent = Gru_TD3Agent(
    state_dim=1,           # 状态维度
    action_dim=1,          # 动作维度
    hidden_dim=128,        # 隐藏层维度
    action_bound=5.0,      # 动作范围
    actor_lr=3e-4,         # Actor学习率
    critic_lr=3e-4,        # Critic学习率
    gru_predictor_lr=1e-3, # GRU预测器学习率（新增）
    gamma=0.99,
    tau=0.005,
    policy_noise=0.2,
    noise_clip=0.5,
    policy_freq=2,
    sigma=0.2,
    clip_grad=True,
    seq_len=10,            # 输入序列长度
    gru_layers=1,          # GRU层数
    pre_seq_len=5,         # 预测未来时间步数
    delay_enabled=True,    # 启用延迟
    delay_step=5,          # 延迟步数
    delay_sigma=2          # 延迟标准差
)

# 4. 创建回放池
replay_buffer = Gru_ReplayBuffer(
    capacity=100000,
    batch_size=64,
    seq_len=10
)

# 5. 创建GRU预测器专用回放池（重要！）
predictor_buffer = GruPredictorBuffer(
    capacity=100000,
    batch_size=64,
    seq_len=10,          # 输入序列长度
    pre_seq_len=5        # 预测的未来时间步数
)

# 6. 开始训练
train_datasets = train_td3(
    env=env,
    agent=agent,
    replay_buffer=replay_buffer,
    predictor_buffer=predictor_buffer,  # 传入GRU预测器回放池
    predictor_update_freq=1,             # 每步更新GRU预测器
    n_episodes=200,
    min_buffer_size=1000,
    print_interval=5,
    save_interval=10,
    project_path="./savedata/分离式架构训练",
    save_checkpoint_path="./savedata/分离式架构训练/checkpoints",
    save_plot_path="./savedata/分离式架构训练/plots"
)
```

## 关键改进点

### 1. GRU预测器独立训练
- **训练数据来源**：环境维护的完整无时延状态历史
- **训练目标**：从时延状态序列预测真实的未来状态
- **训练时机**：每个episode结束后，将完整状态历史提取为训练样本
- **更新频率**：可通过`predictor_update_freq`参数控制

### 2. 时延场景处理
- **训练时**：GRU预测器输入是带延迟的状态序列，目标是真实的未来状态
- **推理时**：Actor和Critic接收GRU预测的未来状态序列作为输入
- **优势**：预测器学习补偿时延，提供更准确的状态估计

### 3. 注意力机制
- 对预测的未来状态序列计算注意力权重
- 自动选择最重要的时间步信息
- 提高对不同延迟情况的适应性

### 4. 参数冻结策略
- GRU预测器在Actor/Critic训练时参数被冻结
- Actor/Critic只训练自己的注意力层和全连接层
- GRU预测器通过独立的更新步骤训练
- 软更新机制确保目标网络同步

## 训练监控

训练日志中新增`predictor_loss`列，记录GRU预测器的MSE损失：
```
episode, rewards, critic_loss, actor_loss, epsilon, simu_reward, predictor_loss
```

## 注意事项

1. **必须传入predictor_buffer**：使用Gru_TD3Agent时必须创建并传入GruPredictorBuffer
2. **序列长度匹配**：确保agent、replay_buffer和predictor_buffer的序列长度参数一致
3. **预测长度设置**：`pre_seq_len`应根据延迟步数合理设置（建议与delay_step接近或略大）
4. **学习率调整**：GRU预测器可能需要与Actor/Critic不同的学习率
5. **延迟设置**：确保`delay_step + seq_len - 1 + pre_seq_len`不超过episode长度

## 相比原架构的优势

1. **更清晰的模块化**：GRU预测器独立，职责明确
2. **更好的训练稳定性**：预测器和策略网络分离训练，避免相互干扰
3. **更强的泛化能力**：预测器在无时延数据上训练，学到更准确的动态特性
4. **更灵活的调整**：可以独立调整GRU预测器的容量和学习率
5. **支持预训练**：可以先预训练GRU预测器，再训练Actor/Critic
