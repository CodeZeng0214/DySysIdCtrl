## TD3 ç®—æ³•å®šä¹‰çš„å‡½æ•°

import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Union, List
from nn import Actor, Critic, ReplayBuffer, Gru_Actor, Gru_Critic, Gru_ReplayBuffer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

## TD3ä»£ç†åŸºç±»
class BaseTD3Agent:
    def __init__(self, state_dim=1, action_dim=1, hidden_dim=64, action_bound=5.0,
                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.99, tau=0.005, 
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, policy_sigma=0.2, clip_grad=False,
                 aware_dt: bool = False,
                 delay_enabled: bool = False, delay_step: int = 5, delay_sigma: int = 2, aware_delay_time: bool = False):
        """åˆå§‹åŒ–TD3ä»£ç†\n
        - state_dim çŠ¶æ€ç»´åº¦
        - action_dim åŠ¨ä½œç»´åº¦
        - hidden_dim éšè—å±‚ç»´åº¦
        - action_bound åŠ¨ä½œèŒƒå›´
        - actor_lr Actorå­¦ä¹ ç‡
        - critic_lr Criticå­¦ä¹ ç‡
        - gamma æŠ˜æ‰£å› å­
        - tau è½¯æ›´æ–°å‚æ•°
        - policy_noise ç›®æ ‡ç­–ç•¥å¹³æ»‘æ­£åˆ™åŒ–å™ªå£°
        - noise_clip å™ªå£°è£å‰ªèŒƒå›´
        - policy_freq ç­–ç•¥æ›´æ–°é¢‘ç‡
        - policy_sigma æ¢ç´¢å™ªå£°æ ‡å‡†å·®
        - clip_grad æ˜¯å¦ä½¿ç”¨æ¢¯åº¦è£å‰ª
        - delay_enabled: æ˜¯å¦å¯ç”¨åŠ¨ä½œå»¶è¿Ÿ
        - delay_step: å»¶è¿Ÿæ­¥æ•°
        - delay_sigma: å»¶è¿Ÿæ­¥æ•°çš„æ ‡å‡†å·®
        """
        # åˆå§‹åŒ–å‚æ•°
        self.state_dim = state_dim # çŠ¶æ€ç»´åº¦
        self.action_dim = action_dim # åŠ¨ä½œç»´åº¦
        self.hidden_dim = hidden_dim # éšè—å±‚ç»´åº¦
        self.action_bound = action_bound # åŠ¨ä½œèŒƒå›´
        self.actor_lr = actor_lr # Actorç½‘ç»œå­¦ä¹ ç‡
        self.critic_lr = critic_lr # Criticç½‘ç»œå­¦ä¹ ç‡
        self.gamma = gamma # æŠ˜æ‰£å› å­
        self.tau = tau # è½¯æ›´æ–°å‚æ•°
        self.policy_noise = policy_noise # ç›®æ ‡ç­–ç•¥å¹³æ»‘æ­£åˆ™åŒ–å™ªå£°
        self.noise_clip = noise_clip # å™ªå£°è£å‰ªèŒƒå›´
        self.policy_freq = policy_freq # ç­–ç•¥æ›´æ–°é¢‘ç‡
        self.policy_sigma = policy_sigma # æ¢ç´¢å™ªå£°æ ‡å‡†å·®
        self.clip_grad = clip_grad # æ˜¯å¦ä½¿ç”¨æ¢¯åº¦è£å‰ª
        self.aware_dt = aware_dt # æ˜¯å¦ä½¿ç”¨æ—¶é—´æ­¥é•¿ä½œä¸ºçŠ¶æ€çš„ä¸€éƒ¨åˆ†
        self.delay_enabled = delay_enabled # æ˜¯å¦å¯ç”¨åŠ¨ä½œå»¶è¿Ÿ
        self.delay_step = delay_step # å»¶è¿Ÿæ­¥æ•°
        self.delay_sigma = delay_sigma # å»¶è¿Ÿæ­¥æ•°çš„æ ‡å‡†å·®
        self.aware_delay_time = aware_delay_time # æ˜¯å¦å¯ç”¨å»¶è¿Ÿæ„ŸçŸ¥

        self.model_name = None
        self.total_it = 0 # æ€»è¿­ä»£æ¬¡æ•°
        self.episode_rewards = [] # å­˜å‚¨æ¯ä¸ªå›åˆçš„å¥–åŠ±

    def _init_nn(self):
        # éœ€è¦åœ¨å­ç±»ä¸­å®šä¹‰
        self.actor: Actor | Gru_Actor = None
        self.critic1: Critic | Gru_Critic = None
        self.critic2: Critic | Gru_Critic = None
        self.target_actor: Actor | Gru_Actor = None
        self.target_critic1: Critic | Gru_Critic = None
        self.target_critic2: Critic | Gru_Critic = None
        raise NotImplementedError("éœ€è¦åœ¨å­ç±»ä¸­åˆå§‹åŒ–ç¥ç»ç½‘ç»œç»“æ„")

    def _init_optimizer(self):
        # ä¼˜åŒ–å™¨è®¾ç½®
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.critic_lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.critic_lr)
        
    def _soft_update(self, source: torch.nn.Module, target: torch.nn.Module):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
            
    def select_action(self):
        """é€‰æ‹©åŠ¨ä½œ"""
        raise NotImplementedError("éœ€è¦åœ¨å­ç±»ä¸­å®ç°åŠ¨ä½œé€‰æ‹©æ–¹æ³•")
            
    def reset_history(self):
        """é‡ç½®çŠ¶æ€å†å²ï¼Œåœ¨æ–°çš„episodeå¼€å§‹æ—¶è°ƒç”¨"""        
        if hasattr(self, 'state_history'):
            self.state_history = []
        
    def update(self, replay_buffer: Union[ReplayBuffer, Gru_ReplayBuffer]) -> Tuple[float, float, float]:
        """æ›´æ–°Actorå’ŒCriticç½‘ç»œ"""
        if len(replay_buffer) < replay_buffer.batch_size:
            return 0.0, 0.0, 0.0
        
        self.total_it += 1
        
        # 1. ä»å›æ”¾æ± ä¸­é‡‡æ ·
        states, actions, rewards, next_states, dones = replay_buffer.sample()
        
        with torch.no_grad():
            # ç›®æ ‡ç­–ç•¥å¹³æ»‘æ­£åˆ™åŒ–
            noise = (torch.randn_like(actions) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)
            next_actions = (self.target_actor(next_states) + noise).clamp(-self.action_bound, self.action_bound)
            
            # è®¡ç®—ç›®æ ‡Qå€¼ï¼Œå–ä¸¤ä¸ªCriticçš„æœ€å°å€¼
            target_q1 = self.target_critic1(next_states, next_actions)
            target_q2 = self.target_critic2(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2)
            target_value = rewards + self.gamma * target_q * (1 - dones)
            
        # 2. æ›´æ–°ä¸¤ä¸ªCriticç½‘ç»œ
        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        
        critic1_loss = F.mse_loss(current_q1, target_value)
        critic2_loss = F.mse_loss(current_q2, target_value)
        
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        if self.clip_grad:
            torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), max_norm=10)
        self.critic1_optimizer.step()
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        if self.clip_grad:
            torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), max_norm=10)
        self.critic2_optimizer.step()
        
        critic_loss = (critic1_loss + critic2_loss) / 2
        actor_loss = 0.0
        
        # 3. å»¶è¿Ÿç­–ç•¥æ›´æ–°
        if self.total_it % self.policy_freq == 0:
            # æ›´æ–°Actorç½‘ç»œ
            policy_actions = self.actor(states)
            actor_loss = -self.critic1(states, policy_actions).mean()
            
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            
            # æ‰“å°æ¢¯åº¦ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
            total_grad_norm = 0
            param_count = 0
            for name, param in self.actor.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.data.norm(2)
                    total_grad_norm += grad_norm.item() ** 2
                    param_count += 1
                    if self.total_it % 1000 == 0:  # æ¯1000æ¬¡æ‰“å°ä¸€æ¬¡
                        pass # æ‰“å°æ¢¯åº¦ä¿¡æ¯
                        #print(f"  {name}: grad_norm={grad_norm:.6f}")
            total_grad_norm = total_grad_norm ** (1. / 2)
            if self.total_it % 1000 == 0:
                pass # æ‰“å°æ¢¯åº¦ä¿¡æ¯
                #print(f"ğŸ” Actoræ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}, å‚æ•°æ•°é‡: {param_count}")
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦çˆ†ç‚¸
            if self.total_it % 1000 == 0 and total_grad_norm > 10:
                pass # æ‰“å°æ¢¯åº¦ä¿¡æ¯
                #print(f"âš ï¸ è­¦å‘Š: Actoræ¢¯åº¦è¿‡é«˜! æ¢¯åº¦èŒƒæ•°: {total_grad_norm}")
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸ºé›¶
            if self.total_it % 1000 == 0 and total_grad_norm < 1e-8:
                pass # æ‰“å°æ¢¯åº¦ä¿¡æ¯
                #print(f"âš ï¸ è­¦å‘Š: Actoræ¢¯åº¦å‡ ä¹ä¸ºé›¶! æ¢¯åº¦èŒƒæ•°: {total_grad_norm}")
            
            if self.clip_grad:
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=10)
            self.actor_optimizer.step()
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self._soft_update(self.actor, self.target_actor)
            self._soft_update(self.critic1, self.target_critic1)
            self._soft_update(self.critic2, self.target_critic2)
            
            actor_loss = actor_loss.item()
        
        return critic_loss.item(), actor_loss, (critic1_loss.item() + critic2_loss.item()) / 2
    
## TD3 ä»£ç†
class TD3Agent(BaseTD3Agent):
    def __init__(self, state_dim=1, action_dim=1, hidden_dim=64, action_bound=5.0,
                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.99, tau=0.005,
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, sigma=0.2, clip_grad=False,
                 aware_dt: bool = False,
                 delay_enabled: bool = False, delay_step: int = 5, delay_sigma: int = 2, aware_delay_time: bool = False):
        # åˆå§‹åŒ–å‚æ•°
        super().__init__(state_dim=state_dim, action_dim=action_dim, hidden_dim=hidden_dim, action_bound=action_bound,
                 actor_lr=actor_lr, critic_lr=critic_lr, gamma=gamma, tau=tau,
                 policy_noise=policy_noise, noise_clip=noise_clip, policy_freq=policy_freq, policy_sigma=sigma, clip_grad=clip_grad,
                 aware_dt=aware_dt,
                 delay_enabled=delay_enabled, delay_step=delay_step, delay_sigma=delay_sigma, aware_delay_time=aware_delay_time)
        self._init_nn()
        self._init_optimizer()

    def _init_nn(self):
        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = Actor(self.state_dim, self.action_dim, self.hidden_dim, self.action_bound).to(device)
        self.critic1 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        self.critic2 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        
        self.target_actor = Actor(self.state_dim, self.action_dim, self.hidden_dim, self.action_bound).to(device)
        self.target_critic1 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        self.target_critic2 = Critic(self.state_dim, self.action_dim, self.hidden_dim).to(device)
        
        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())

    def select_action(self, state_history: List[np.ndarray], add_noise=True, epsilon=1.0, rand_prob=0.05, delay=1) -> float:
        """é€‰æ‹©åŠ¨ä½œï¼Œæ”¯æŒæ¢ç´¢"""
        # å¦‚æœå¯ç”¨åŠ¨ä½œå»¶è¿Ÿï¼Œä½¿ç”¨å»¶è¿Ÿæ­¥æ•°çš„é«˜æ–¯åˆ†å¸ƒé‡‡æ ·
        if self.delay_enabled:
            if len(state_history) < delay:
                state = state_history[-1]  # å¦‚æœå†å²é•¿åº¦ä¸å¤Ÿï¼Œä½¿ç”¨æœ€æ–°çŠ¶æ€
            else:
                state = state_history[-delay]  # ä½¿ç”¨å»¶è¿Ÿçš„çŠ¶æ€
                
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            action_tensor: torch.Tensor = self.actor(state)
            action_np: np.ndarray = action_tensor.cpu().detach().numpy()
            action = action_np.flatten()
            
        if add_noise:
            noise = np.random.normal(0, self.action_bound * self.policy_sigma * epsilon, size=self.action_dim)
            action += noise
            if np.random.random() < rand_prob:
                action = np.random.uniform(-self.action_bound, self.action_bound, self.action_dim)

        return float(np.clip(action, -self.action_bound, self.action_bound))
        
## åŸºäºGRUçš„TD3ä»£ç†
class Gru_TD3Agent(BaseTD3Agent):
    def __init__(self, state_dim=1, action_dim=1, hidden_dim=64, action_bound=5.0,
                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.99, tau=0.005,
                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, sigma=0.2, clip_grad=False, 
                 seq_len=10, gru_layers=1,
                 aware_dt: bool = False,
                 delay_enabled: bool = False, delay_step: int = 5, delay_sigma: int = 2, aware_delay_time: bool = False):
        self.seq_len = seq_len  # åºåˆ—é•¿åº¦
        self.gru_layers = gru_layers  # GRUå±‚æ•°
        super().__init__(state_dim=state_dim, action_dim=action_dim, hidden_dim=hidden_dim, action_bound=action_bound,
                         actor_lr=actor_lr, critic_lr=critic_lr, gamma=gamma, tau=tau,
                         policy_noise=policy_noise, noise_clip=noise_clip, policy_freq=policy_freq, policy_sigma=sigma, clip_grad=clip_grad,
                         aware_dt=aware_dt,
                         delay_enabled=delay_enabled, delay_step=delay_step, delay_sigma=delay_sigma, aware_delay_time=aware_delay_time)
        self._init_nn()
        self._init_optimizer()

    def _init_nn(self):
        # GRUç½‘ç»œåˆå§‹åŒ–
        self.actor = Gru_Actor(self.state_dim, self.action_dim, self.hidden_dim, self.action_bound, self.seq_len, self.gru_layers).to(device)
        self.critic1 = Gru_Critic(self.state_dim, self.action_dim, self.hidden_dim, self.seq_len, self.gru_layers).to(device)
        self.critic2 = Gru_Critic(self.state_dim, self.action_dim, self.hidden_dim, self.seq_len, self.gru_layers).to(device)
        
        self.target_actor = Gru_Actor(self.state_dim, self.action_dim, self.hidden_dim, self.action_bound, self.seq_len, self.gru_layers).to(device)
        self.target_critic1 = Gru_Critic(self.state_dim, self.action_dim, self.hidden_dim, self.seq_len, self.gru_layers).to(device)
        self.target_critic2 = Gru_Critic(self.state_dim, self.action_dim, self.hidden_dim, self.seq_len, self.gru_layers).to(device)
        
        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())

    def select_action(self, state_history: List[np.ndarray], add_noise=True, epsilon=1.0, rand_prob=0.05, delay=1) -> float:
        """é€‰æ‹©åŠ¨ä½œï¼Œæ”¯æŒæ¢ç´¢"""

        # å¦‚æœå†å²é•¿åº¦ä¸å¤Ÿï¼Œä½¿ç”¨é›¶å¡«å……æˆ–é‡å¤å½“å‰çŠ¶æ€
        if len(state_history) < (self.seq_len + delay - 1):
            # ç”¨å½“å‰çŠ¶æ€å¡«å……ä¸è¶³çš„éƒ¨åˆ†
            if len(state_history) < delay:
                # å¦‚æœå†å²é•¿åº¦è¿delayéƒ½ä¸å¤Ÿï¼Œç”¨èµ·å§‹çŠ¶æ€å¡«å……
                padded_history = [state_history[0]] * (self.seq_len + delay - 1)
            else:
                # ä¿®å¤delay=1æ—¶çš„åˆ‡ç‰‡é—®é¢˜
                if delay == 1:
                    padded_history = np.concatenate([[state_history[0]] * (self.seq_len - len(state_history)), state_history])
                else:
                    padded_history = np.concatenate([[state_history[0]] * ((self.seq_len + delay-1) - len(state_history)), state_history[:-(delay-1)]])
        else:
            # ä¿æŒæœ€è¿‘çš„seq_lenä¸ªçŠ¶æ€
            if delay == 1:
                padded_history = state_history[-self.seq_len:]  # å½“delay=1æ—¶ï¼Œç›´æ¥å–æœ€åseq_lenä¸ªå…ƒç´ 
            else:
                padded_history = state_history[-self.seq_len-(delay-1):-(delay-1)]        # æ„å»ºçŠ¶æ€åºåˆ—
        state_seq = np.array(padded_history)  # [seq_len, state_dim]
        state_seq_tensor = torch.tensor(state_seq, dtype=torch.float32).unsqueeze(0).to(device)  # [1, seq_len, state_dim]

        with torch.no_grad():
            action_tensor: torch.Tensor = self.actor(state_seq_tensor)
            action_np: np.ndarray = action_tensor.cpu().detach().numpy()
            action = action_np.flatten()
            
        if add_noise:
            noise = np.random.normal(0, self.action_bound * self.policy_sigma * epsilon, size=self.action_dim)
            action += noise
            if np.random.random() < rand_prob:
                action = np.random.uniform(-self.action_bound, self.action_bound, self.action_dim)

        return float(np.clip(action, -self.action_bound, self.action_bound))