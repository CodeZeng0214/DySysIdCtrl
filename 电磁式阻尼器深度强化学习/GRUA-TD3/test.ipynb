{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13ec886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python list spend 0.026s\n",
      "Numpy array spend 0.006s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "# 对比运行时间\n",
    "def list_and_numpy():\n",
    "    t0 = time.time()\n",
    "    # python list\n",
    "    l = list(range(100))\n",
    "    for _ in range(10000):\n",
    "        for i in range(len(l)):\n",
    "            l[i] += 1\n",
    "    t1 = time.time()\n",
    "    # numpy array\n",
    "    a = np.array(l)\n",
    "    for _ in range(10000):\n",
    "        a += 1\n",
    "    print(\"Python list spend {:.3f}s\".format(t1 - t0))\n",
    "    print(\"Numpy array spend {:.3f}s\".format(time.time() - t1))\n",
    "list_and_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce88ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预分配NumPy数组耗时：0.067432秒\n",
      "动态扩展列表耗时：0.029510秒\n",
      "动态扩展初始化的列表耗时：0.018999秒\n",
      "NumPy比动态扩展列表快 0.44 倍\n",
      "NumPy比动态扩展初始化的列表快 0.28 倍\n"
     ]
    }
   ],
   "source": [
    "n = 100000\n",
    "# 场景1：预分配NumPy数组\n",
    "start = time.time()\n",
    "state_array = np.zeros((n, 3))  # 假设每个状态是3维向量\n",
    "for i in range(n):\n",
    "    state_array[i] = np.array([i*0.1, i*0.2, i*0.3])  # 模拟状态赋值\n",
    "array_time = time.time() - start\n",
    "\n",
    "# 场景3：动态扩展Python列表\n",
    "start = time.time()\n",
    "state_list = []\n",
    "for i in range(n):\n",
    "    state_list.append([i*0.1, i*0.2, i*0.3])  # 模拟状态赋值\n",
    "list1_time = time.time() - start\n",
    "\n",
    "# 场景3：动态扩展初始化的Python列表\n",
    "start = time.time()\n",
    "state_list = [None] * n # 预分配列表空间\n",
    "for i in range(n):\n",
    "    state_list[i] = [i*0.1, i*0.2, i*0.3]  # 模拟状态赋值\n",
    "list2_time = time.time() - start\n",
    "\n",
    "print(f\"预分配NumPy数组耗时：{array_time:.6f}秒\")\n",
    "print(f\"动态扩展列表耗时：{list1_time:.6f}秒\")\n",
    "print(f\"动态扩展初始化的列表耗时：{list2_time:.6f}秒\")\n",
    "print(f\"NumPy比动态扩展列表快 {list1_time/array_time:.2f} 倍\")\n",
    "print(f\"NumPy比动态扩展初始化的列表快 {list2_time/array_time:.2f} 倍\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce38ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入PyTorch库，用于构建神经网络和进行张量计算\n",
    "import torch\n",
    "# 导入PyTorch的神经网络模块\n",
    "import torch.nn as nn\n",
    "# 导入PyTorch的优化器模块\n",
    "import torch.optim as optim\n",
    "# 导入PyTorch的函数模块，包含各种激活函数等\n",
    "import torch.nn.functional as F\n",
    "# 导入PyTorch的分布模块，用于创建概率分布\n",
    "from torch.distributions import Categorical\n",
    "# 导入NumPy库，用于数值计算\n",
    "import numpy as np\n",
    "# 导入OpenAI Gym库，用于强化学习环境\n",
    "import gym\n",
    "\n",
    "# 定义PPO算法的超参数类\n",
    "class PPOConfig:\n",
    "    def __init__(self):\n",
    "        self.lr = 3e-4  # 学习率\n",
    "        self.gamma = 0.99  # 折扣因子，用于计算未来奖励的现值\n",
    "        self.lamda = 0.95  # GAE(广义优势估计)的参数\n",
    "        self.eps_clip = 0.2  # PPO剪切参数，限制策略更新的幅度\n",
    "        self.K_epochs = 4  # 每次数据收集后执行的训练轮数\n",
    "        self.batch_size = 64  # 每次参数更新使用的批量大小\n",
    "        self.buffer_size = 2048  # 经验回放缓冲区大小\n",
    "        self.entropy_coef = 0.01  # 熵奖励系数，鼓励探索\n",
    "        self.value_coef = 0.5  # 价值函数损失系数\n",
    "        self.hidden_dim = 64  # 神经网络隐藏层维度\n",
    "        self.max_episodes = 10000  # 最大训练回合数\n",
    "        self.update_freq = self.buffer_size  # 更新频率，等于缓冲区大小\n",
    "\n",
    "# 定义Actor-Critic网络结构\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # 定义共享的网络层\n",
    "        self.fc_shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),  # 全连接层，输入状态维度，输出隐藏层维度\n",
    "            nn.ReLU()  # ReLU激活函数\n",
    "        )\n",
    "        # 定义策略网络（Actor）\n",
    "        self.fc_actor = nn.Linear(hidden_dim, action_dim)  # 输出动作维度\n",
    "        # 定义价值网络（Critic）\n",
    "        self.fc_critic = nn.Linear(hidden_dim, 1)  # 输出单个值（状态价值）\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 前向传播，通过共享层\n",
    "        x = self.fc_shared(x)\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, x):\n",
    "        # 获取动作和对应的对数概率\n",
    "        hidden = self.forward(x)  # 通过共享层\n",
    "        logits = self.fc_actor(hidden)  # 通过策略头\n",
    "        dist = Categorical(logits=logits)  # 创建分类分布\n",
    "        action = dist.sample()  # 从分布中采样动作\n",
    "        log_prob = dist.log_prob(action)  # 计算动作的对数概率\n",
    "        return action.item(), log_prob  # 返回动作值和对数概率\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        # 获取状态价值\n",
    "        hidden = self.forward(x)  # 通过共享层\n",
    "        value = self.fc_critic(hidden)  # 通过价值头\n",
    "        return value\n",
    "\n",
    "# 定义PPO算法类\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, config):\n",
    "        self.config = config  # 存储配置参数\n",
    "        self.policy = ActorCritic(state_dim, action_dim, config.hidden_dim)  # 创建策略网络\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=config.lr)  # 使用Adam优化器\n",
    "        # 创建旧策略网络，用于计算重要性采样比率\n",
    "        self.old_policy = ActorCritic(state_dim, action_dim, config.hidden_dim)\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())  # 初始与当前策略相同\n",
    "        self.buffer = []  # 经验回放缓冲区\n",
    "        self.mse_loss = nn.MSELoss()  # 均方误差损失，用于价值函数\n",
    "    \n",
    "    def update(self):\n",
    "        # 从缓冲区提取数据并转换为张量\n",
    "        states = torch.FloatTensor(np.array([t[0] for t in self.buffer]))  # 状态\n",
    "        actions = torch.LongTensor(np.array([t[1] for t in self.buffer])).unsqueeze(1)  # 动作\n",
    "        old_log_probs = torch.FloatTensor(np.array([t[2] for t in self.buffer])).unsqueeze(1)  # 旧策略的对数概率\n",
    "        rewards = torch.FloatTensor(np.array([t[3] for t in self.buffer]))  # 奖励\n",
    "        next_states = torch.FloatTensor(np.array([t[4] for t in self.buffer]))  # 下一个状态\n",
    "        dones = torch.FloatTensor(np.array([t[5] for t in self.buffer]))  # 终止标志\n",
    "        \n",
    "        # 计算广义优势估计(GAE)和回报\n",
    "        with torch.no_grad():  # 不计算梯度\n",
    "            values = self.old_policy.get_value(states)  # 当前状态价值\n",
    "            next_values = self.old_policy.get_value(next_states)  # 下一状态价值\n",
    "            deltas = rewards + self.config.gamma * next_values * (1 - dones) - values  # 计算TD误差\n",
    "            advantages = torch.zeros_like(rewards)  # 初始化优势函数\n",
    "            advantage = 0  # 初始化优势值\n",
    "            # 反向计算GAE\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                advantage = deltas[t] + self.config.gamma * self.config.lamda * advantage * (1 - dones[t])\n",
    "                advantages[t] = advantage\n",
    "            returns = advantages + values.squeeze(1)  # 计算回报\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)  # 标准化优势\n",
    "        \n",
    "        # 多次更新策略\n",
    "        for _ in range(self.config.K_epochs):\n",
    "            indices = torch.randperm(len(self.buffer))  # 随机打乱索引\n",
    "            # 小批量更新\n",
    "            for start in range(0, len(self.buffer), self.config.batch_size):\n",
    "                end = start + self.config.batch_size\n",
    "                idx = indices[start:end]  # 获取当前批次的索引\n",
    "                # 获取当前批次的数据\n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_advantages = advantages[idx].unsqueeze(1)\n",
    "                batch_returns = returns[idx].unsqueeze(1)\n",
    "                \n",
    "                # 计算新策略的概率和值\n",
    "                hidden = self.policy(batch_states)  # 通过共享层\n",
    "                logits = self.policy.fc_actor(hidden)  # 通过策略头\n",
    "                dist = Categorical(logits=logits)  # 创建动作分布\n",
    "                log_probs = dist.log_prob(batch_actions.squeeze(1)).unsqueeze(1)  # 新策略对数概率\n",
    "                entropy = dist.entropy().mean()  # 计算熵（用于鼓励探索）\n",
    "                values = self.policy.get_value(batch_states)  # 计算状态价值\n",
    "                \n",
    "                # 计算重要性采样比率\n",
    "                ratios = torch.exp(log_probs - batch_old_log_probs)\n",
    "                # 计算剪切目标函数\n",
    "                surr1 = ratios * batch_advantages  # 未剪切的目标\n",
    "                surr2 = torch.clamp(ratios, 1-self.config.eps_clip, 1+self.config.eps_clip) * batch_advantages  # 剪切后的目标\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()  # 策略损失（取负是因为我们要最大化）\n",
    "                value_loss = self.mse_loss(values, batch_returns)  # 价值函数损失\n",
    "                # 总损失 = 策略损失 + 价值损失系数*价值损失 - 熵系数*熵（最大化熵）\n",
    "                loss = policy_loss + self.config.value_coef * value_loss - self.config.entropy_coef * entropy\n",
    "                \n",
    "                # 执行梯度下降\n",
    "                self.optimizer.zero_grad()  # 清空梯度\n",
    "                loss.backward()  # 反向传播\n",
    "                self.optimizer.step()  # 更新参数\n",
    "        \n",
    "        # 更新旧策略网络\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "        self.buffer = []  # 清空缓冲区\n",
    "    \n",
    "    def store_transition(self, transition):\n",
    "        # 存储转移样本到缓冲区\n",
    "        self.buffer.append(transition)\n",
    "        # 当缓冲区满时执行更新\n",
    "        if len(self.buffer) == self.config.update_freq:\n",
    "            self.update()\n",
    "\n",
    "# 训练函数\n",
    "def train():\n",
    "    env = gym.make('CartPole-v1')  # 创建CartPole环境\n",
    "    config = PPOConfig()  # 创建配置\n",
    "    state_dim = env.observation_space.shape[0]  # 获取状态维度\n",
    "    action_dim = env.action_space.n  # 获取动作维度\n",
    "    agent = PPO(state_dim, action_dim, config)  # 创建PPO智能体\n",
    "    \n",
    "    # 训练循环\n",
    "    for episode in range(config.max_episodes):\n",
    "        state = env.reset()  # 重置环境\n",
    "        episode_reward = 0  # 初始化回合奖励\n",
    "        \n",
    "        while True:\n",
    "            # 与环境交互\n",
    "            action, log_prob = agent.policy.get_action(torch.FloatTensor(state))  # 选择动作\n",
    "            next_state, reward, done, _ = env.step(action)  # 执行动作\n",
    "            # 存储转移样本\n",
    "            agent.store_transition((state, action, log_prob, reward, next_state, done))\n",
    "            state = next_state  # 更新状态\n",
    "            episode_reward += reward  # 累计奖励\n",
    "            if done:  # 回合结束\n",
    "                break\n",
    "        \n",
    "        print(f'Episode {episode}, Reward: {episode_reward}')  # 打印回合信息\n",
    "        # 定期测试\n",
    "        if episode % 10 == 0:\n",
    "            test_reward = test(env, agent)  # 执行测试\n",
    "            print(f'Test Reward: {test_reward}')  # 打印测试结果\n",
    "\n",
    "# 测试函数\n",
    "def test(env, agent, test_episodes=5):\n",
    "    total_reward = 0  # 总奖励\n",
    "    for _ in range(test_episodes):\n",
    "        state = env.reset()  # 重置环境\n",
    "        episode_reward = 0  # 回合奖励\n",
    "        while True:\n",
    "            action, _ = agent.policy.get_action(torch.FloatTensor(state))  # 选择动作\n",
    "            next_state, reward, done, _ = env.step(action)  # 执行动作\n",
    "            episode_reward += reward  # 累计奖励\n",
    "            state = next_state  # 更新状态\n",
    "            if done:  # 回合结束\n",
    "                break\n",
    "        total_reward += episode_reward  # 累计总奖励\n",
    "    return total_reward / test_episodes  # 返回平均奖励\n",
    "\n",
    "# 主程序入口\n",
    "if __name__ == '__main__':\n",
    "    train()  # 开始训练"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1310_DySysIdCtrl (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
