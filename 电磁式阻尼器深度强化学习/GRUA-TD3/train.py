import os
import time
from collections import deque
from typing import Optional, Callable
import numpy as np
from controller import BaseController
from buffer import ReplayBuffer
from data import EpisodeRecorder, TrainingHistory, save_checkpoint, load_checkpoint, latest_checkpoint, make_dirs, plot_data
from env import ElectromagneticDamperEnv
from tqdm import tqdm
import logging

def delayed_sequence(window: deque, delay_steps: int, seq_len: int) -> np.ndarray:
    """Delayed sequence for GRU policy input."""
    delay = max(0, min(delay_steps, len(window) - 1))
    hist = list(window)
    end = len(hist) - delay
    start = max(0, end - seq_len)
    view = hist[start:end]
    if len(view) < seq_len:
        pad = [hist[0]] * (seq_len - len(view))
        view = pad + view
    return np.stack(view, axis=0)


def delayed_obs(window: deque, delay_steps: int) -> np.ndarray:
    delay = max(0, min(delay_steps, len(window) - 1))
    return list(window)[-1 - delay]

def train(project_name: str, 
          env: ElectromagneticDamperEnv, controller: BaseController, buffer: ReplayBuffer,
          explore_noise_trend: str = 'linear',
          n_episodes=200, min_buffer_size=1000, save_interval=5,
          resume=False,
          state0: Optional[np.ndarray] = None,
          z_func: Optional[Callable] = None, f_func: Optional[Callable] = None,
          batch_size: int = 64
          ) -> None:
    """è®­ç»ƒå‡½æ•°ã€‚\n
    å‚æ•°ï¼š\n
        project_name: é¡¹ç›®åç§°ï¼Œç”¨äºŽåˆ›å»ºä¿å­˜è·¯å¾„ã€‚
        env: å¼ºåŒ–å­¦ä¹ çŽ¯å¢ƒå®žä¾‹ã€‚
        controller: å¼ºåŒ–å­¦ä¹ æŽ§åˆ¶å™¨å®žä¾‹ã€‚
        buffer: ç»éªŒå›žæ”¾æ± å®žä¾‹ã€‚
        explore_noise_trend: æŽ¢ç´¢å™ªå£°å˜åŒ–è¶‹åŠ¿ï¼Œ'linear' æˆ– 'exp'ã€‚
        n_episodes: è®­ç»ƒæ€»å›žåˆæ•°ã€‚
        min_buffer_size: æœ€å°ç»éªŒæ± å¤§å°ï¼Œè¾¾åˆ°åŽå¼€å§‹è®­ç»ƒã€‚
        save_interval: ä¿å­˜æ¨¡åž‹å’Œæ—¥å¿—çš„é—´éš”å›žåˆæ•°ã€‚
        resume: æ˜¯å¦ä»Žä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤è®­ç»ƒã€‚
        state0: åˆå§‹çŠ¶æ€ï¼Œè‹¥ä¸º None åˆ™éšæœºç”Ÿæˆã€‚
        z_func: çŠ¶æ€æƒ©ç½šå‡½æ•°ã€‚
        f_func: å¤–éƒ¨æ¿€åŠ±å‡½æ•°ã€‚
        batch_size: è®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚"""
    history = TrainingHistory()
    # é¡¹ç›®ç›¸å…³çš„è·¯å¾„è®¾ç½®
    project_path, ckpt_dir, plot_path = make_dirs(project_name)
    now_time = f"{time.strftime('%m%d_%H%M%S')}"

    nc_recorder = EpisodeRecorder()  # ç”¨äºŽè®°å½•æ— æŽ§åˆ¶å™¨æ—¶çš„è¡¨çŽ°
    nc_recorder = env.run_episode(controller=None, state0=state0, z_func=z_func, f_func=f_func)
    nc_x_values=nc_recorder.as_numpy(keys='time_history').reshape(-1, 1)[:,[0,0,0,0,0,0]]
    nc_y_values=nc_recorder.as_numpy(keys='state_history')[:,[0,1,2,3,4,5]]
    plot_data(x_values=nc_x_values, y_values=nc_y_values, sub_shape=(3,2),
              legends=[('å¸æŒ¯å™¨ä½ç§»',),('ä¸»ç»“æž„ä½ç§»',),('å¸æŒ¯å™¨é€Ÿåº¦',),('ä¸»ç»“æž„é€Ÿåº¦',),('å¸æŒ¯å™¨åŠ é€Ÿåº¦',),('ä¸»ç»“æž„åŠ é€Ÿåº¦',)], legend_loc='upper right', 
              sub_group=[(0,),(3,),(1,),(4,),(2,),(5,)],plot_title=f'{now_time}_åˆé€Ÿåº¦æ¡ä»¶-çŽ¯å¢ƒæ— æŽ§åˆ¶å“åº”', save_path=plot_path, show=False)

    # 
    if resume:
        last = latest_checkpoint(ckpt_dir)
        if last:
            payload = load_checkpoint(last)
            controller.load_state(payload["agent"])
            history = TrainingHistory.from_dict(payload["history"])
            print(f"âœ… Resumed from {last}")

    # åˆ›å»ºå¥–åŠ±æ—¥å¿—æ–‡ä»¶
    rewards_log_file = os.path.join(project_path, f"td3_rewards_log_{time.strftime('%Y%m%d_%H%M%S')}.csv") if project_path else None
    if rewards_log_file:
        with open(rewards_log_file, "w") as f:
            f.write(f"{'episode':>8}, {'rewards':>12}, {'simu_reward':>12}, {'actor_loss':>12}, {'critic_loss':>12}, {'explore_noise':>8}\n")

    # è®­ç»ƒä¸»å¾ªçŽ¯
    for ep in tqdm(range(history.current_episode, n_episodes)):
        ep_recorder = EpisodeRecorder() # è®°å½•å½“å‰å›žåˆæ•°æ®

        # å›žåˆç›¸å…³å˜é‡åˆå§‹åŒ–
        ep_reward_sum = 0.0
        ep_actor_loss_sum = 0.0
        ep_critic_loss_sum = 0.0
        updates = 0 # æŽ§åˆ¶å™¨æ›´æ–°è®¡æ•°

        controller.reset() # é‡ç½®æŽ§åˆ¶å™¨çŠ¶æ€
        env.reset(state0=state0, z_func=z_func, f_func=f_func) # é‡ç½®çŽ¯å¢ƒ

        if explore_noise_trend == 'linear':
        # è®¡ç®—å½“å‰æŽ¢ç´¢å™ªå£°çš„å¤§å°ï¼Œä½¿ç”¨çº¿æ€§è¡°å‡
            explore_noise = max(1.0 - ep / ((history.current_episode + n_episodes) * 0.7), 0.1)
        elif explore_noise_trend == 'exp':
        # è®¡ç®—å½“å‰æŽ¢ç´¢å™ªå£°çš„å¤§å°ï¼Œä½¿ç”¨æŒ‡æ•°è¡°å‡
            explore_noise = 0.1 + (1.0 - 0.1) * np.exp(-0.01 * ep)
        else:
            explore_noise = 0.1 # é»˜è®¤å™ªå£°å€¼
        if ep >= n_episodes * 0.8: 
            explore_noise = 0 # æœ€åŽ20%çš„è½®æ¬¡ä¸ä½¿ç”¨æŽ¢ç´¢å™ªå£°

        # ä»¿çœŸè½®æ¬¡å¾ªçŽ¯
        done = False
        while not done:
            obs = env.observe() # èŽ·å–è§‚æµ‹å€¼

            action = controller.select_action(obs=obs, noise_scale=explore_noise) # é€‰æ‹©åŠ¨ä½œ
            next_obs, reward, done, info = env.step(action)

            buffer.add(obs, action, reward, next_obs, done, delay=info["delay_step"]) # æ·»åŠ åˆ°ç»éªŒå›žæ”¾æ± 
            ep_reward_sum += reward

            ep_recorder.append(obs_history=obs.copy(), state_history=info["state"], action_history=action, reward_history=reward, 
                                time_history=info["time"], dt_history=info["dt"],  delay_time=info["delay_time"]) # è®°å½•å½“å‰æ­¥æ•°æ®
            
            # æŽ§åˆ¶å™¨æ›´æ–°
            if len(buffer) > min_buffer_size:
                critic_loss, actor_loss = controller.update(replay_buffer=buffer, batch_size=batch_size)
                ep_critic_loss_sum += critic_loss
                ep_actor_loss_sum += actor_loss
                updates += 1 # ä½¿æŽ§åˆ¶å™¨æ›´æ–°è®¡æ•°åŠ ä¸€

        ep_actor_loss_avg = ep_actor_loss_sum / max(1, updates)
        ep_critic_loss_avg = ep_critic_loss_sum / max(1, updates)

        # è®°å½•è®­ç»ƒåŽ†å²
        history.log(reward_history=ep_reward_sum, actor_loss_history=ep_actor_loss_avg, critic_loss_history=ep_critic_loss_avg, explore_noise_history=explore_noise)


        ep_sim_reward_sum = ep_reward_sum # æ¨¡æ‹Ÿè¿è¡ŒçŽ¯å¢ƒï¼ˆæ— å™ªå£°ï¼‰çš„å¥–åŠ±æ€»å’Œ
        if ep % save_interval == 0:
            # è¿è¡Œæœ‰æŽ§åˆ¶å™¨çš„çŽ¯å¢ƒï¼Œè®°å½•æ•°æ®
            c_recorder = env.run_episode(controller=controller, state0=state0, z_func=z_func, f_func=f_func)
            c_x_values = c_recorder.as_numpy(keys='time_history').reshape(-1, 1)
            c_y_values = c_recorder.as_numpy(keys='state_history')[:, [0, 1, 2, 3, 4, 5]]
            plot_data(x_values=c_x_values, y_values=np.concatenate((c_y_values,nc_y_values[:,[3]]), axis=1), 
                      legends=[('å¸æŒ¯å™¨ä½ç§»',),('æ— æŽ§åˆ¶-ä¸»ç»“æž„ä½ç§»','GRUATD3æŽ§åˆ¶-ä¸»ç»“æž„ä½ç§»'),('å¸æŒ¯å™¨é€Ÿåº¦',),('ä¸»ç»“æž„é€Ÿåº¦',),('å¸æŒ¯å™¨åŠ é€Ÿåº¦',),('ä¸»ç»“æž„åŠ é€Ÿåº¦',)], legend_loc='upper right',
                      sub_shape=(3, 2), sub_group=[(0,), (6,3), (1,), (4,), (2,), (5,)],
                      plot_title=f'{now_time}_åˆé€Ÿåº¦æ¡ä»¶å›žåˆ{ep}æŽ§åˆ¶å™¨å“åº”', save_path=plot_path, show=False)
            c_action_values = c_recorder.as_numpy(keys='action_history').reshape(-1, 1)
            c_reward_values = c_recorder.as_numpy(keys='reward_history').reshape(-1, 1)
            c_delay_time_values = c_recorder.as_numpy(keys='delay_time').reshape(-1, 1)
            c_dt_values = c_recorder.as_numpy(keys='dt_history').reshape(-1, 1)
            plot_data(x_values=c_x_values, y_values=np.concatenate((c_action_values, c_reward_values, c_delay_time_values, c_dt_values), axis=1),
                      sub_shape=(2, 2), sub_group=[(0,), (1,), (2,), (3,)],
                      legends=[('åŠ¨ä½œ',), ('å¥–åŠ±',), ('å»¶è¿Ÿæ—¶é—´',), ('æ—¶é—´æ­¥é•¿',)], legend_loc='upper right',
                      plot_title=f'{now_time}_åˆé€Ÿåº¦æ¡ä»¶å›žåˆ{ep}æŽ§åˆ¶å™¨åŠ¨ä½œç­‰', save_path=plot_path, show=False)
            ep_sim_reward_sum = c_recorder.as_numpy(keys='reward_history').sum() # ä»¿çœŸå¥–åŠ±æ€»å’Œ
            
            # ä¿å­˜æ¨¡åž‹å’Œè®­ç»ƒå½“å‰çš„åŽ†å²
            history.checkpoint_name = f"{time.strftime('%m%d_%H%M%S')}_ep{ep}ckpt"
            ckpt_path = os.path.join(ckpt_dir, f"{history.checkpoint_name}.pth")
            save_checkpoint(ckpt_path, controller.export_state(), ep_recorder, history)
            logging.info(f"ðŸ’¾ saved checkpoint {ckpt_path}")

        # å†™å…¥è®­ç»ƒä¿¡æ¯åˆ°csvæ—¥å¿—æ–‡ä»¶
        if rewards_log_file:
            with open(rewards_log_file, "a") as f:
                
                f.write(f"{ep:>8}, {ep_reward_sum:>12.4f}, {ep_sim_reward_sum:>12.4f}, {ep_actor_loss_avg:>12.4f}, {ep_critic_loss_avg:>12.4f}, {explore_noise:>8.4f}\n")

    print("Training finished.")


if __name__ == "__main__":
    train()
