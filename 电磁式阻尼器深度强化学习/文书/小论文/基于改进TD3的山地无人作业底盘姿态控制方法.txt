基于改进 TD3 的山地无人作业底盘姿态

控制方法 ∗

李希明1 ,刘业通1 ,彭世康2 ,吴湘柠2 ,李恒强1 ,蒙艳玫1

(1 广西大学机械工程学院 ,南宁 530004;

2 广西机械工业研究院有限公司 ,南宁 530001)

摘要 :针对山地无人作业底盘在复杂道路下姿态不平稳 ,传统控制方法适应性、鲁棒性差等问题 ,提出了一种基于牛顿-拉弗森优化(Newton-Raphson-Based Optimizer,NRBO)算法、极致梯度提升树(eXtreme Gradient Boosting,XGBoost)算法和双延迟深度确定性策略梯度(Twin Delayed Deep Deterministic policy gradient,TD3)算法的底盘姿态控制策略 。首先 ,搭建七自由度主动悬架振动模型环境 ;然后 ,训练 NRBO-XGBoost 的状态预测模型 ,在 TD3 算法中加入状态预测模型并在网络中加入注意力机制 ,增强 TD3 智能体在复杂环境下的决策能力和适应能力 ,同时设计奖励函数并训练 TD3 智能体 ,实现在复杂道路环境下的底盘姿态控制 ;最后 ,基于 Matlab 2023a/Simulink 软件开展仿真 。仿真结果表明 ,基于改进TD3 的底盘姿态控制策略能够有效抑制无人作业底盘在复杂道路下的姿态变化 ,其俯仰角、侧倾角和垂向位移分别抑制了 61.4 %、84.9 %和 84.9 % ,显著提高了平稳性 ;相比传统 DDPG、PPO 和 TD3 强化学习控制策略 ,改进 TD3 算法下的俯仰角分别改善了 49.1 %、7.4 %和 37.2 % ,侧倾角分别改善了 83.3 %、36.5 %和 34.7 % ,垂向位移分别改善了 70.7 %、 77.5 %和 64.0 % ,垂向位移加速度分别改善了 67.7 %、42.1 %和 49.7 % ,控制效果更好 ,具有更好的适应性与鲁棒性。

关键词 :山地无人作业底盘 ;主动悬架控制 ;改进 TD3 算法 ; 自注意力机制

中图分类号 :TP273 文献标志码 :A 文章编号 :1671-3133(2025)05-0001-11

DOI:10.16731/j.cnki.1671-3133.2025.05.001

Attitude control method for mountain unmanned operation chassis

based on improved TD3

LI Ximing1 ,LIU Yetong1 ,PENG Shikang2 ,WU Xiangning2 ,LI Hengqiang1 ,MENG Yanmei1 (1 School of Mechanical Engineering,Guangxi University,Nanning 530004,China;

2 Guangxi Machinery Industry Research Institute Co. ,Ltd. ,Nanning 530001,China)

Abstract:Aiming at the problems of unstable attitude of mountain unmanned operation chassis under complex roads and poor a- daptability and robustness of traditional control methods,a chassis attitude control strategy based on Newton-Raphson-Based Opti- mizer (NRBO) algorithm,eXtreme Gradient Boosting (XGBoost) algorithm and Twin Delayed Deep Deterministic policy gradient (TD3) algorithm was proposed. Firstly,the seven-degree-of-freedom active suspension vibration model environment was built;then the state prediction model of NRBO-XGBoost was trained,the state prediction model was added to the TD3 algorithm and the attention mechanism was added to the network to enhance the decision-making ability and adaptive ability of the TD3 intelligences in com- plex environments,and at the same time,the reward function was designed and the TD3 intelligences were trained to realize the chassis attitude control in complex road environments; finally,simulations were carried out based on Matlab 2023a/Simulink soft- ware. The simulation results show that the chassis attitude strategy method based on the improved TD3 can effectively suppress the attitude change of unmanned operation chassis under complex roads,and the pitch angle,lateral inclination angle,and vertical displacement are suppressed by 61. 4 % , 84. 9 % , and 84. 9 % , respectively,which significantly improves the smoothness; compared with the traditional DDPG,PPO,and TD3 reinforcement learning control strategies,with the improved TD3 algorithm the pitch angle is improved by 49. 1 % ,7.4 % and 37.2 % ,respectively,the lateral inclination angle is improved by 83.3 % ,

∗ 国家自然科学基金项目(52365001) ;广西科技重大专项项目(桂科 AA23062040-3)

36.5 % and 34.7 % ,respectively,the vertical displacement is improved by 70.7 % ,77.5 % and 64.0 % ,respectively,and the ver- tical displacement acceleration is improved by 67.7 % ,42.1 % and 49.7 % ,respectively,which provides a better control effect with better adaptability and robustness.

Keywords:mountain unmanned operation chassis;active suspension control;improved TD3 algorithm;self-attention mechanism

0 引言

由于丘陵山地的地形复杂 ,农业和林业机械化面临挑战[1-2] ,无人作业底盘在丘陵山地复杂的地形下 ,平顺性和通过性差 ,工作效率低 。近年来 ,大量学者在无人作业底盘控制研究上取得了一定成果[3-8] 。丘陵山地路面多为岩石和坑洼 ,对无人作业底盘的动力和悬架系统性能要求较高[9] 。

传统的被动悬架系统虽适用于平坦路面 ,但在丘陵地形下性能不足[10] ,难以保证车辆底盘的稳定性和平顺性 。相比之下 ,主动悬架系统通过车载传感器实时获取路面状况 ,并通过算法控制作动器 ,以适应复杂路面并显著提升行驶的平顺性、操纵稳定性及安全性 ;因此 ,主动悬架系统研究对于提高丘陵山地无人作业车辆行驶的平稳性和作业效率至关重要。

目前 ,大多数关于主动悬架的研究[11-13] 集中在1/4悬架模型的控制上 。尽管这些研究在减小车身垂直振动 ,提高车辆平顺性方面取得了一定的进展 ,但它们主要针对局部悬架系统 ,未能充分考虑整车底盘的姿态控制 。整车底盘的俯仰角、侧倾角以及各自由度之间的耦合对车辆的动态稳定性具有重要影响 ,而单独优化车辆的垂向加速度无法保证底盘姿态在复杂动态环境下的平稳性和安全性 。为了克服 1/4 悬架模型的局限性 ,也有学者提出了基于七自由度整车悬架振动模型的研究 ,并采用不同的控制策略对底盘姿态进行优化 。高晋等人[14] 提出了结合 PID 和 LQR 控制理论的 PID-LQR 控制器 ,该控制器在仿真中表现出比传统被动悬架和单独使用 LQR 控制更优的控制效果 ,有效优化了车辆的动态性能。

然而 ,这些研究在实际应用中仍面临着一系列问题 。首先 ,这些控制算法通常高度依赖于精确的车辆动力学模型 ,而在实际复杂的地形环境中 ,车辆系统参数具有不确定性和非线性 ,精确建模难度大 ,导致这些基于模型的控制方法难以保证其在实际多变环境中的 可 靠 性 。其 次 , 传 统 的 控 制 方 法 , 如 LQR、 H ∞ [15] 等 ,往往需要较高的计算资源和精确的硬件支持 ,计算复杂度高 ,限制了这些方法在实际应用中的可行性 。此外 ,这些控制算法的适应性和鲁棒性较

差 ,难以有效应对丘陵山地这种复杂、动态且多变的路面条件。

近年来 ,深度强化学习(DRL)算法逐渐被引入主动和半主动悬架系统的控制研究 ,凭借其对精确数学模型依赖性小、能够自主学习控制策略的特点 ,展现出了很大的潜力 。LEE 等人[16] 提出了一种基于 DRL的半主动悬挂控制器 ,该控制器通过优化算法和状态规范化提升了训练收敛性和泛化性能 ,从而改善了车辆的乘坐舒适度 。FARES 等人[17] 则采用在线强化学习创建了主动悬架系统的控制器 ,使用 TD 优势演员评论家算法 ,并与传统控制技术进行了比较 。HAN 等人 [18] 利用 PPO 算法对半主动悬架系统的振动控制进行了优化 ,通过调整奖励函数以适应不同的道路条件 。LIU 等人[19] 提出的改进 DDPG 算法则能有效适应各种道路和速度 ,进一步提升了车辆的性能 。这些研究证实了深度强化学习算法在主动悬架控制上的有效 性 与 实 用 性 。然 而 , 这 些 研 究 大 多 仍 然 基 于1/4悬架模型 ,无法有效解决整车底盘的姿态控制问题 。更重要的是 ,深度强化学习算法在实际应用中存在收敛速度慢、训练时间长以及处理复杂时序任务能力不足的问题 ;尤其在面对非平稳环境和不确定因素时 ,系统的鲁棒性表现不佳。

针对上述问题 ,本文提出一种结合双延迟深度确定性策略梯度(Twin Delayed Deep Deterministic policy gradient,TD3)算法与状态预测模型的底盘姿态控制策略 。TD3 算法作为深度强化学习算法 ,通过与环境交互学习最优控制策略 ,减少对精确动力学模型的依赖 ,适用于复杂动态环境 。TD3 算法的双重 Q 值网络和策略延迟更新机制减少了 Q 值估计的偏差 ,提升了控制的稳定性 。为增强系统在时序任务中的表现 ,本文引入极致梯度提升树 (eXtreme Gradient Boosting , XGBoost)算法进行状态预测 ,通过集成学习和牛顿-拉弗森优化(Newton-Raphson-Based Optimizer,NRBO)算法来优化参数 ,提升预测精度 ,增强了智能体在多变环境下的决策能力 。同时 ,TD3 算法的 Critic 网络引入了多头自注意力机制来捕捉状态与动作的长期依赖 ,以提升算法在动态环境中的收敛速度和泛化能力。

综上所述 ,本文基于七自由度主动悬架振动模型

搭建训练环境 ,构建改进的 TD3 智能体对底盘姿态进行控制 。通过仿真 ,验证该方法在复杂丘陵地形下的有效性 。仿真结果表明 ,改进的 TD3 算法显著提升了车辆的平稳性和姿态控制 ,尤其在复杂动态任务和不确定条件下展现出了良好的鲁棒性和适应性 ,为无人作业底盘在丘陵山地的应用提供了技术支持。

1 七自由度主动悬架振动模型

1.1 随机路面振动模型

ISO 8608 : 2016 标准将路面不平度分为八级[20] ,即 A~ H。研究显示 ,丘陵山区路面主要是包括砂石、硬土和软土的路面 ,凹凸不平的路面状况显著 ,对车辆有较大的振动和冲击 ,影响车辆操控稳定性和耐用性 [21] ;因此 ,可将丘陵山地视为道路标准 ISO 8608 : 2016 中的 D 级路面 。D 级路面不平度系数的几何平均值为 1.024× 10-3 m3 ,根据丘陵山地车的实际情况 ,选取车速为 20 km/h。

单轮路面激励时域的表达式为 :

·

q (t)= -2πn1vq(t)+2πn0 vGq (n0 ) w(t) (1)

式中 :t 为时间;q (t) 为路面激励幅值 ; (t) 为路面激

励变化率 ;n0 为空间参考频率 ,取值 0.1/m;n1 为空间截止频率 ,取值 0.011/m;Gq (n0 ) 为路面不平度系数 ; w(t)为均值为 0、功率谱密度为 1 的理想白噪声 ;v 为车速 ,取值 20 km/h。

参考文献[22] ,四轮路面激励微分方程为 :



式中 :q1 (t)、q2 (t)、q3 (t)和 q4 (t)分别为左前、右前、左后和右后轮的路面激励幅值;q1 (t)、q2 (t)、q3 (t) 和

4 (t)分别为左前、右前、左后和右后轮的路面激励变

化率;L1 为车体前后车轮轮距 ;w1 (t)、w2 (t)分别为左前轮和右前轮所受均值为 0、功率谱密度为 1 的理想白噪声。

1.2 七自由度主动悬架振动模型

参考文献[22]搭建七自由度主动悬架振动模型 ,如图 1 所示 ,将 4 个轮分别通过 1/4 独立悬架系统建模。



图 1 七自由度主动悬架振动模型

ìïm1

m4

ï

ïm2

í

ïm3

… · ·

zt1 +kt1 [zt1 -q1 (t) ]+ku1(zt1 -z1 )+c1 ( z t1 -z 1 )+u1 =0

… · ·

zt2 +kt2 [zt2 -q2 (t) ]+ku2(zt2 -z2 )+c2 ( z t2 -z2 )+u2 =0

… · ·

zt3 +kt3 [zt3 -q3 (t) ]+ku3(zt3 -z3 )+c3 ( z t3 -z3 )+u3 =0

… · ·

zt4 +kt4 [zt4 -q4 (t) ]+ku4(zt4 -z4 )+c4 ( z t4 -z4 )+u4 =0

(3)

式中 :mi 为簧下质量 ,i = 1,2,3,4;zti 为各车轮的垂向位移 ;zi 为各车轮上方车身垂向位移 ; i 为各车轮上方车身垂向速度 ; ti 为各车轮的垂向速度 ;z… ti 为各车轮的垂向加速度;qi (t) 为各轮路面激励幅值;kti 为各轮轮胎刚度;kui 为各悬架刚度 ;ci 为各悬架阻尼 ;ui 为各主动悬架控制力。

对各悬架进行受力分析 ,可得各悬架对车身施加的作用力 Fi 为 :



对底盘姿态进行分析 ,定义绕 X 轴旋转的角度为车身侧倾角 φ ,绕 Y 轴旋转的角度为车身俯仰角 θ ,则整车姿态表达式为 :

z1 =z- sinθ+sinφ ï

ïz2 =z- sinθ - sinφ

í (5)

ï L1 L2

ïz3 =z+ 2 sinθ+ 2 sinφ

ï

ï L1 L2

z4 =z+ 2 sinθ - 2 sinφ

式中:z 为整车车身垂向位移 ,向上为正;L2 为左右车

轮轮距。

对整车进行动力学分析 ,其微分方程为 :



式中 :M 为车体质量;θ为整车车身俯仰角加速度;φ为整车车身侧倾角加速度 ;z… 为整车车身垂向位移加速度 ;Iy 为整车绕 Y 轴的转动惯量;Ix 为整车绕 X 轴的转动惯量;F1、F2、F3 和 F4 分别为左前、右前、左后和右后轮悬架对车身的作用力。

2 基于 NRBO-XGBoost 的状态预测模型

2.1 NRBO 算法

NRBO 算 法 是 由 SOWMYA 等 人[23] 于 2024 年2 月提出的一种面向复杂优化问题的强大且高效的元启发式算法 ,结合了牛顿-拉弗森搜索规则(NRSR) 和陷阱规避算子(TAO) ,通过这些机制来平衡算法的搜索和开发能力 ,从而提高搜索全局最优解的效率。 NRBO 算法流程如下。

2.1.1 种群初始化

x =lb+rand×(ub-lb)

n= 1,2, … ,Np j= 1,2, … ,dim (7)

式中 :x 为第 n 个个体在第j 个维度上的位置;ub 和 lb分别为变量的上界和下界;rand 为一个在(0,1)之间的随机数;Np 为基于事实的种群数量;dim 为问题的维度。

种群的矩阵形式为 :

Xn  Np×dim (8)式中 :Xn 为描述所有维度的种群矩阵。

2.1.2 Newton-Raphson 搜索规则(NRSR)

NRBO 算法的搜索路径由牛顿-拉夫森搜索规则(NRSR)决定 ,其目标是提高算法的搜索能力 ,并通过引入随机性元素来增强全局搜索效果 。NRSR 的具体表达式为 :



式中:randn 为均值为 0、方差为 1 的正态分布随机数 ;

Xw 为最差位置;Xb 为最佳位置;xn 为当前根。

式(9) 中更新步长 Δx 的表达式为 :

Δx=rand(1,dim) ×  Xb -XT  (10)

式中:rand( 1,dim) 为具有 dim 维度决策变量的随机数 ;IT 为当前迭代次数;XT 为当前迭代次数 IT 中其他群体的位置。

更新根 xn+1 的位置用 NRSR 可表示为 :

xn+1 =xn -NRSR (11)引入参数 ρ 将种群引导至正确方向 ,即 :

ρ = ∂×(Xb -XT )+b×(X-X ) (12)

式中:∂、b 分别为(0,1)之间的随机数 ;r1、r2 均为随机正整数。

矢量 X1T 当前位置的更新公式为 :



[ ∂×(Xb -XT )+b×(X-X ) ] (13)

式中 :X1T 为通过更新 XT 得到的新位置 ;xT 为当前迭代次数中其他种群的近似解。

为更清晰展示 NRBO 流程 ,将 NRSR 改写为 :



式中:yw、yb 分别为 xn+1 和 xn 生成的 2 个矢量位置 ; Mean( )为求均值函数。

根据式(14)将式(13)更新为 :



[ ∂×(Xb -XT )+b×(X-X ) ] (15)

用最佳位置 Xb 替换式( 15)中的 XT 位置来构造新的矢量 X2T ,即 :





下一轮迭代的新位置矢量 X3T 可表示为 :



式中 :xT+1 为 IT+ 1 次迭代中其他种群的近似解;δ 为自适应系数 ,其计算式如式(18)所示。

 (18)

式中 :Max_IT 为最大迭代次数。

2.1.3 陷阱规避算子(TAO)

TAO 能够有效避免算法陷入局部最优 ,TAO 通过结合最佳位置 Xb 的解 xb 和当前位置 XT 得到效果更好的解 XO ,其计算式为 :



式中 :XT+1 为 IT+1 次迭代中其他群体的位置;θ1、θ2、 μ 1 和 μ2 均为随机数 ,使得种群变得多样性 ,能够防止陷入局部最优;XIT为当前迭代次数中所有种群的位置。

2.2 极致梯度提升树(XGBoost)算法

XGBoost(eXtreme Gradient Boosting)算法[24] 是一种基于决策树的集成学习算法 ,属于梯度提升框架。其核心原理是通过逐步建立多个弱学习器(通常是决策树) ,使每个新决策树尽量纠正前一系列决策树的错误预测 。XGBoost 算法通过最小化损失函数来优化模型 ,采用二阶导数信息来加速优化过程 ,这使得它能够更精确地拟合复杂的数据结构。

2.3 NRBO-XGBoost 算法实现流程

2.3.1 数据预处理

数据集采用在标准 ISO 8608 ∶ 2016 中 20 km/h车速、D 级道路工况下随机路面激励 10 s 的数据 ,数据长度为 4 000,将 前 80 % 数 据 划 分 为 训 练 集 ,后20 %数据为测试集 。NRBO 算法要优化的参数 ξ 分别为 XGBoost 算法的迭代次数、树的最大深度和学习率。

2.3.2 交叉验证的均方根误差 RMSE 计算

模型的训练可以表示为对每一折进行交叉验证。

基于训练集(pt(ri)n ,tt(ri)n )和参数 ξ ,代入 XGBoost 算法拟

合出函数ξ(k) 为 :

ξ(k) =XGBoost(pt(ri)n ,tt(ri)n ,ξ) k= 1,2, … ,K (20)

式中 :XGBoost ( · ) 为 XGBoost 算法 ;pt(ri)n、tt(ri)n 分别为

第 k 折的训练集特征矩阵和标签向量。

对预测同时计算其误差 ε (k) ,即 :



式中:tv(ld,q 和 pv(ld,q 分别为第 k 折的验证集标签向量

和特征矩阵 ;nk 为验证集样本数。

2.3.3 目标函数

最终目标函数 O(ξ)为所有折 RMSE 的平均值 ,即 :



(22)

该函数的目标是最小化模型在交叉验证过程中RMSE 的平均值 ,从而找到最优的参数。

2.4 随机路面激励预测结果与性能对比

随机路面激励预测仿真在 Matlab 2023a 软件中进行 。NRBO-XGBoost 算法将选择 15 个时间步的历史数据 ,预测下一个时间步的数据 。为了验证 NRBO- XGBoost 算法的预测性能 ,与 LSTM 预测模型进行对比 。NRBO-XGBoost 算法的最大迭代次数 Max_IT = 15,参数维度为 3,下界向量 LB = [ 10,10,0.01] ,上界向量 UB= [20,20,1] ,2 组向量的每一维度分别对应式(7) 中的 lb 和 ub ,决策因子 DF=0.6,当 TAO 中随机数值小于 DF 时 ,则触发 TAO 操作 。NRBO-XGBoost和 LSTM 预测模型仿真的 RMSE 结果如表 1 所示 ,仿真结果的拟合效果如图 2 所示。

表 1 NRBO-XGBoost 和 LSTM 预测模型仿真的 RMSE 结果

预测对象

RMSE/m

NRBO-XGBoost

LSTM

q1

0.001 254 9

0.002 476 3

q2

0.001 317 1

0.002 556 8

q3

0.001 379 4

0.002 654 4

q4

0.002 989 3

0.004 752 6



图 2 NRBO-XGBoost 和 LTSM 预测模型仿真结果的拟合效果

从表 1 和图 2 所示的仿真结果可以看出 ,NRBO- XGBoost 算法在预测精度方面显著优于 LSTM 预测模型;具体而言 ,NRBO-XGBoost 算法的预测误差 RMSE相比 LSTM 预测模型降低了近一倍 ,其在处理复杂时间序列数据时具有更高的准确性和鲁棒性 ,故本文选取该算法来预测车辆环境状态。

3 基于状态预测的 TD3 底盘姿态控制

3.1 MDP 建模

将底盘姿态控制过程建模成马尔可夫决策过程(Markov Decision Process,MDP ) 。以本文第 1. 1 节搭建的随机路面振动模型和第 1.2 节搭建的七自由度主动悬架振动模型作为 TD3 算法的学习环境 ,将其观测的状态集 S 定义为 :

… … …

S= ( θ ,θ ,φ ,φ ,z,z) (23)

在崎岖地形和动态驾驶中 ,较大的控制力有助于提升车辆的姿态稳定性 ;适中的控制力则能够有效吸收地面振动 ,提升舒适性并优化能耗 ,从而确保系统在复杂环境中快速响应 ,并实现长时间稳定运行[25] 。基于丘陵山地的作业需求 ,兼顾底盘稳定性与能耗之间的平衡 ,将智能体的动作定义为 4 个主动悬架的控

制力 ,控制力范围为[-2 000 N,2 000 N] ,以适应多变的地形条件 。将其动作集 A 定义为 :

A= (u1 ,u2 ,u3 ,u4 ) (24)式中 :u1、u2、u3 和 u4 分别为左前、右前、左后和右后轮主动悬架的控制力。

3.2 本文提出的方法

为实 现 基 于 NRBO-XGBoost 算 法 状 态 预 测 的TD3 算法底盘姿态控制 ,首先将训练好的 NRBO-XG- Boost 算 法 放 入 TD3 的 算 法 框 架 , NRBO-XGBoost- TD3 算法(简称改进 TD3 算法)框架如图 3 所示。

TD3 算法训练时 ,NRBO-XGBoost 算法根据环境中的历史随机路面激励幅值 qi (t)来预测下一时刻的

随机路面激励幅值i (t) 。将预测的下一时刻的随机

路面激励幅值i (t) 和当前时刻智能体执行的动作 ai

输入七自由度主动悬架振动模型 ,由其计算并输出下

一时刻的状态i+1 ,TD3 智能体的观测值由原来的 si 扩

展为(si ,i+1 ) 。最后 ,结合环境的当前奖励 ri ,将当前

动作 ai 以及下一时刻的状态(si+1 ,i+2 )放入经验池。

智能体训练流程大致如下。



图 3 NRBO-XGBoost-TD3 算法框架

训练初期 ,Actor 网络与 Critic 网络的输出都是随机的 。 目标策略网络基于平滑正则化 ,为 Critic 网络中的 2 个目标 Q 值网络各自提供一个动作 ai ′ = μ ′ + clip[N(0, σ ) ,-c,c] ,μ ′ 为目标网络策略;clip [N (0,

σ ) ,-c,c] 为将均值为 0、方差为 σ 的噪声 N 裁剪到(-c,c)范围中 。双重目标 Q 值网络计算出 Q ′值 ,取最小的 Q ′值计算出 TD 目标 y=r+γmin(Q1 ′ ,Q2 ′ ) ,其中 , r 为奖励值;γ 为奖励折扣率 。随后 ,利用梯度下降法

计算最小化损失函数L(ϕ) ,L ( ϕ) = (y-Qe )2 (e = 1, 2) 。对 Critic 网络中的 2 个主 Q 值网络参数 ϕ 进行更新 ,ϕ←ϕ-α ∇L(ϕ) ,其中 ,α 为 Critic 网络的学习率。

2 个目标 Q 值网络参数更新的方式为软更新 ,即 : ϕ ′ = τϕ+(1-τ)ϕ ′ ,其中 ,ϕ ′ 为目标 Q 值网络参数 ;τ 为是一个很小的正数 ,用于控制更新的速率 。TD3 算法采用延迟更新 ,Critic 网络参数多次更新后 ,Actor 网络才启动更新 。Actor 网络通过 Critic 网络中的 Q 值 ,计算出策略梯度∇J(ω ) ,其网络参数 ω 通过梯度上升的方法更新 ,以最大化期望回报 ω←ω+β∇J(ω ) ,其中 , β 为该网络的学习率 ;同时 Actor 网络根据当前策略 μ计算出当前状态下的动作ai 。TD3算法通过经验池存

储交互样本 ,训练时从中采样更新 Critic 网络和 Actor网络 ,提升样本利用效率并保持学习稳定性。

3.3 TD3 算法网络结构的设计

为提高 TD3 算法处理复杂环境的能力 ,并有效利

用扩展后的观测值(si ,i+1 ) ,在 Critic 网络中引入多头

自注意力机制 ,TD3 算法网络结构如图 4 所示。

Critic 网络在状态输入层之后接入全连接层 FC Layer,随后接入头数为 4、键向量的维度为 256 的多头自注意力机制层 ,之后为 4 层全连接层。

Actor 网络中全连接层为 5 层 ,输出采用 Tanh 层 ,使其动作范围限制在[-1,1]之间 ;全连接层神经元个数均为 128。Actor 网络结构见图 4。



图 4 TD3 算法网络结构

3.4 奖励函数设计

本文根据主动悬架和深度强化学习的特性 ,设计3 个部分的奖励函数 ,其中 ,密集奖励 rN 为 :

rN = -(k1 θ2 +k2 θ… 2 +k3 φ2 +k4 φ… 2 +k5z2 +k6 z… 2 ) (25)

式中 :k1、k2、k3、k4、k5 和 k6 分别为各优化目标的权重参数。

本文通过在奖励函数中加入式(26)所示的即时奖励 rI 来不断启发智能体往正确的搜索方向进行训练 。只要当前时刻 θ、θ…、φ、φ…、z 和z… 的绝对值大于上一时刻 θi-1、θ… i-1、φi-1、φ… i-1、zi-1 和z… i-1 的绝对值 ,就提供一个较小的奖励 Δrj。

rI  W1 Δrj (26)

式中 :W1 为即时奖励的权重系数 ,取值为 0.5。

只要 θ、θ…、φ、φ…、z 和z… 的绝对值降低到其初始值 θ ′、 θ… ′、φ ′、φ… ′、z ′和z… ′绝对值的 10 % ( 即被动悬架条件下各项参数的 10 %) ,就会获得一个较大的奖励 ΔRj ,累计获得的奖励即为稀疏奖励 rS ,即 :

rS = KRj (27)

式中 :K1 为稀疏奖励权重系数。

综上所述 ,总体的奖励值 r 为 :

r=rN +rI +rS (28)

本文将悬架动行程  zi -zti  约束在 0.2 m 范围内 ,防止智能体的决策使悬架动行程超出了最大行程 ,即训练终止条件为 :

 zi -zti  <0.2 (29)

通过多次仿真试验分析 ,得到奖励函数的权重参数值 ,如表 2 所示 ,模型训练效果优异。

表 2 奖励函数的权重参数值

k1

k2

k3

k4

k5

k6

K1

W1

7 250

0.05

325

0.2

500

0.2

0.2

0.5

4 仿真试验与分析

本文基于 Matlab 2023a/Simulink 软件平台和 Inter Core i5-13600KF CPU、NVIDIA GeForce RTX 3060 GPU计算机硬件平台实现仿真 。在 Simulink 软件中搭建七自由度主动悬架振动模型 ,仿真时长 Tf = 10 s,步长Ts =0.025 s。由于不同神经网络的超参数会影响训练的效果[26] ,通过多次仿真分析 ,得出最终 NRBO-XG- Boost-TD3 算法的主要超参数 ,如表 3 所示 。整车主动悬架参数如表 4 所示。

表 3 NRBO-XGBoost-TD3 算法的主要超参数

参数名称

参数值

Critic 网络学习率

0.001

Actor 网络学习率

0.000 1

奖励折扣率 γ

0.99

小批量样本数量

256

经验池大小

106

每回合步数

400

训练回合数

2 000

延迟更新频率

2

软更新频率

0.001

表 4 整车主动悬架参数

主要参数

数值

M/kg

1 270

m1 ,m2/kg

44

m3 ,m4/kg

35

L1/m

3.0

L2/m

1.6

ku1 ,ku2/(kN ·m-1 )

20

ku3 ,ku4/(kN ·m-1 )

23

kt1 ,kt2 ,kt3 ,kt4/(kN ·m-1 )

268

c1/(N ·s ·m-1 )

2 000

c2/(N ·s ·m-1 )

2 200

Ix/(kg ·m2 )

537

Iy/(kg ·m2 )

1 537

4.1 训练过程分析

为验证 NRBO-XGBoost-TD3 算法在底盘姿态优化控制上的优势 ,同时设计了 TD3、DDPG 和 PPO 传统强化学习算法的底盘姿态优化控制系统 ,相应网络层

数、各项参数和奖励函数均与 NRBO-XGBoost-TD3 算法一致 。经过 2 000 次的训练 ,各算法的训练结果如图 5 所示。



图 5 各算法的训练结果

由图 5 可知 ,本文所提 NRBO-XGBoost-TD3 算法的平均奖励值上升的最快 ,并且最终的平均奖励最高值约为 238,而 PPO 算法的平均奖励最高值约为 153,其次为 TD3 算法 ,其平均奖励最高值约为 80,而 DDPG算法的训练效果最差 ,其平均奖励最高值约为 35。

从训练的收敛性层面分析 ,TD3 算法与 DDPG 算法过早收敛 ,陷入了局部最优;PPO 算法的平均奖励值虽然逐步上升但是收敛速度过慢 ,训练回合达到2 000 次 ,平均奖励值依然低于 NRBO-XGBoost-TD3 算法 。造成这一训练结果的主要原因是本文在 Critic 网络中加入了多头自注意力机制 ,有效提升了网络对复杂环境的适应能力 ,并加快了智能体的训练收敛速度 。同时 ,由于状态预测 ,智能体能够接收的状态变量范围更广 ,维度更高;并能够根据未来预测的状态对动作策略进行合理调整 ,最终的奖励也达到了最高。

4.2 对比仿真结果分析

各项性能指标采用 RMS(均方根)进行评价 ,各算法下底盘姿态性能 RMS 和较被动悬架下的优化比如表 5 所示 ,其中负值表示性能恶化 。本文底盘姿态控制策略、文献[22] 中增程式 PID 控制策略和文献[27]的姿态补偿模糊控制策略下相较被动悬架的优化比对比如图 6 所示。

由图 6 可知 ,本文所提改进 TD3 算法在姿态控制方面相较于传统控制方法性能更好 ,俯仰角、侧倾角和垂向位移指标的优化比皆高于文献[22]和文献[27] 中的控制策略 ,能够更有效地抑制底盘姿态的变化。

NRBO-XGBoost-TD3 算法与被动悬架底盘姿态控制效果对比如图7 所示。

表 5 各算法下底盘姿态性能 RMS 和较被动悬架下的优化比

性能指标

被动悬架RMS

DDPG

PPO

TD3

改进 TD3

RMS

优化比/%

RMS

优化比/%

RMS

优化比/%

RMS

优化比/%

θ

0.007 0

0.005 3

24.3

0.002 9

58.6

0.004 3

38.6

0.002 7

61.4

θ

1.639 5

1.811 4

-10.5

1.643 3

-0.2

1.799 6

-9.7

1.582 6

3.5

φ

0.031 1

0.028 2

9.3

0.007 4

76.2

0.007 2

76.8

0.004 7

84.9

φ

0.508 1

0.975 2

-91.9

0.680 6

-33.9

0.734 9

-44.6

0.595 3

-17.2

z

0.017 8

0.009 2

48.3

0.012 0

32.6

0.007 5

57.9

0.002 7

84.9

z

0.358 6

0.961 2

-168.0

0.537 2

-49.8

0.617 7

-72.3

0.310 9

13.3



图 6 不同控制策略相较被动悬架的优化比对比

相比于被动悬架 ,NRBO-XGBoost-TD3算法在控

制各个姿态参数方面表现最好 ,具体表现在俯仰角、侧倾角及垂向位移分别减少 至 被 动 悬 架 条 件 下 的61.4 %、84.9 %和 84.9 %;对应的俯仰角加速度和垂向位移加速度也分别略微降低了 3.5 %和 13.3 %;然而 ,侧倾角加速度却出现了 17.2 %的恶化。

相比之下 ,传统的 PPO 算法、TD3 算法和 DDPG算法虽在姿态控制上也取得了一定提升 ,但其代价是牺牲了加速度相关指标 。尤其是 DDPG 算法 ,其俯仰角加速度、侧倾角加速度和垂向位移加速度相较于被动悬架均出现了不同程度的恶化 ,侧倾角加速度和垂向位移 加 速 度 的 恶 化 程 度 甚 至 高 达 - 91. 9 % 和-168.0 % ,在实际应用中 ,这种控制效果是不可接受的。



图 7 NRBO-XGBoost-TD3 算法与被动悬架底盘姿态控制效果对比

由仿真结果可知 ,七自由度主动悬架振动模型中的各项参数往往相互耦合 ,优化维度高且目标存在冲突 ,同时优化所有参数会导致优化过程复杂 ,计算负担重 ,难以找到全局最优解 。例如 ,提升俯仰角控制精度可能会对俯仰角加速度的控制效果产生不利影

响 。尽管如此 ,本文仍采用七自由度主动悬架振动模型 ,其理由在于 :首先 ,七自由度主动悬架振动模型能够全面模拟车辆的纵向、横向和垂向运动及俯仰、侧倾和偏航角度 ,提供更高的动态精确性 ,尤其适用于崎岖地形和复杂工况 ,便于优化底盘姿态控制策略提

升整车的平稳性和适应性 ;其次 ,实际应用中应该在关键的姿态变化上进行优化控制 ,选择对底盘姿态影响较大的俯仰角、侧倾角和垂向位移进行控制 ,而对于次要的加速度参数指标则可适当妥协 ,以保证整体的控制效果 ;最后 ,七自由度主动悬架振动模型能够通过强化学习算法来减少模型计算负担 ,并且本文所提算法各性能指标相较于传统强化学习算法均得到改善 ,改善效果如表 6 所示。

表 6 改进 TD3 算法各性能指标改善效果

性能指标

相对于 DDPG优化比/%

相对于 PPO优化比/%

相对于 TD3优化比/%

θ

49.1

7.4

37.2

θ

12.6

3.7

12.1

φ

83.3

36.5

34.7

φ

39.0

12.5

19.0

z

70.7

77.5

64.0

z

67.7

42.1

49.7

由表 6 可知 ,相比传统 DDPG、PPO 和 TD3 强化学习控制策略 ,本文改进 TD3 算法下的俯仰角分别改善了 49. 1 %、7.4 % 和 37.2 % ,侧倾角分别改善了83.3 %、36. 5 % 和 34. 7 % , 垂 向 位 移 分 别 改 善 了70.7 %、77.5 %和 64.0 % ,垂向位移加速度分别改善了 67.7 %、42.1 %和 49.7 %。另外 ,由表 5 可知 ,改进TD3 算法性能指标中 ,只有侧倾角加速度指标略微恶化 ,其余 5 项性能指标均得到了优化 ,而其他算法相应的加速度指标恶化程度显著 ,只优化了姿态指标 ,牺牲了相应的加速度指标 。由此可知 ,本文改进 TD3 算法下受到系统耦合的影响是最小的 ,验证了本文方法在处理强耦合非线性系统时有着显著优势 ,整体的姿态控制效果更好 ,有效提升了无人作业底盘在复杂道路情况下的平稳性和适应性。

4.3 模型泛化性能分析

为验证本文所提算法对不同车速具有一定的泛化能力 ,选取不同车速 ,分析各项性能指标的优化情况 。本文仿真的车速以 20 km/h 为标准 ,但根据复杂道路车辆的具体情况 ,车速的提升会引起车身姿态剧烈变化 。故选取车速分别为 20、30、40、50 和 60 km/h时的情况进行仿真分析 ,不同车速下改进 TD3 算法相对于被动悬架的优化比如图 8 所示 。由图 8 可知 ,随着车速的提升 ,本文改进 TD3 算法依然能提供较好的控制效果 ,并具有一定的泛化能力 ,验证了本文所提算法面对多变、复杂的环境因素时具有较强的鲁棒性。

不同车速下传统 TD3 算法相对于被动悬架的优化比如图 9 所示 。由图 8 和图 9 可知 ,随着车速的提

高 ,传统 TD3 算法控制效果优化比的变化幅度比改进TD3 算法的变化幅度更大 ,特别是当车速达到 50 km/ h 时 ,其各项性能指标控制优化比的浮动较大 ,而反观本文改进 TD3 算法的控制优化比变化幅度较为平稳 ,验证了本文改进 TD3 算法的泛化性能比传统 TD3 算法更强 ,对于不同车速情况 ,系统具有一定的鲁棒性。



图 8 不同车速下改进 TD3 算法相对于被动悬架的优化比



图 9 不同车速下传统 TD3 算法相对于被动悬架的优化比

5 结语

针对无人作业底盘在复杂丘陵山地道路下姿态不平稳问题 ,对七 自 由度主动悬架控制系统展开研究 。考虑到它的非线性、强耦合问题 ,采用深度强化学习 TD3 算法来解决传统控制器对数学模型精度的依赖性 。考虑到深度强化学习算法在实际应用中存在收敛速度慢、训练时间长及处理复杂时序任务能力不足的问题 ,尤其在面对非平稳环境和不确定因素时 ,系 统 的 鲁 棒 性 表 现 不 佳 , 提 出 了 一 种 改 进 的TD3 算法 。将状态预测与强化学习有机结合 ,有效提升了智能体对复杂环境的决策能力和适应能力 。并在智能体的 Critic 网络中加入多头自注意力机制 ,在增强了智能体对多状态输入适应能力的同时 ,加快了训练收敛速度。

仿真结果表明 ,本文方法在无人作业底盘姿态控制方面具有显著优势 ,其俯仰角、侧倾角和垂向位移姿态指标分别抑制了 61.4 %、84.9 %和 84.9 %。对比传统 DDPG、PPO 和 TD3 强化学习的底盘姿态控制策略 ,其俯仰角分别改善了 49.1 %、7.4 %和 37.2 % ,侧倾角分别改善了 83.3 %、36.5 %和 34.7 % ,垂向位移分别改善了 70.7 %、77.5 %和 64.0 % ,垂向位移加速度分别改善了 67.7 %、42.1 %和 49.7 %。不仅控制效果优于传统强化学习算法 ,而且智能体的训练收敛速度更快 ,显著提升了山地无人作业底盘在复杂路况下的行驶平稳性 。通过不同车速下的泛化性能分析 ,验证了改进后的 TD3 算法在面对复杂多变的环境时 ,比传统 TD3 算法具有更强的鲁棒性和适应能力。

参 考 文 献 :

[1] 郭庆.浅析如何补齐丘陵山区农机化短板[J].农机质量与监督 ,2023(10) :14,33.

[2] 王玮举 ,李华 ,郑荣华.发展智能农机助力丘陵山区农业现代化[J].农机科技推广 ,2020(11) :53-55.

[3] 吕凤玉 ,李晓康 ,贺成柱 ,等.全向姿态调整农机履带底盘设计与试验[J]. 中国农机化学报 ,2024,45(8) :132-137.

[4] 赖晓 ,程健华 ,李尚平 ,等.丘陵履带式甘蔗收获机底盘调平机构设计与试验 [J]. 农业机械学报 ,2024,55 (12) : 100-109.

[5] 刘凤霞 ,逄焕晓 ,翟国强 ,等.丘陵山区单行玉米收获机自适应姿态调整系统研究[J/OL].农机化研究:1-8[2024- 10-29 ]. http://kns. cnki. net/kcms/detail/23. 1233. S. 20241012.1503.002.html.

[6] 杨怡婷 ,李广棵 ,吴磊. 车辆半主动悬架模糊变权重因子自适应控制研究[J].现代制造工程 ,2024(9) :73-82.

[7] 张艳兵 ,徐鹏跃 ,李卓 ,等.农用机器人底盘控制系统的设计[ J/OL ]. 农 机 化 研 究: 1-6 [ 2024-10-29 ]. https://doi. org/10.13427/j.issn.1003-88X.2025.06.032.

[8] 张喜清 ,王博 ,连晋毅 ,等.模糊 PID 控制的步履式底盘姿态自适应调整研究[J].工程机械 ,2023,54(9) :50-57,9.

[9] 刘平义 ,彭凤娟 ,李海涛 ,等.丘陵山区农用 自适应调平底盘设计与试验[J].农业机械学报 ,2017,48(12) :42-47.

[10] 吴伟斌 ,李泽艺 ,洪添胜 ,等.基于山地果园路谱的轮式运输车钢板弹簧悬架优化设计[J].华中农业大学学报 , 2018,37(4) :7-14.

[11] 董朝闻 ,黄龙. 基于模糊控制的汽车半主动悬架研究[J].工程机械 ,2024,55(2) :60-64,8.

[12] 金贤建 ,王佳栋 ,徐利伟 ,等.轮毂电机驱动电动汽车主动悬架 μ 综合鲁棒控制研究[J].机械工程学报 ,2024, 60(16) :259-269.

作者简介:李希明 ,硕士研究生 ,主要研究方向为主动悬架控制策略研究。

蒙艳玫 ,通信作者。

E-mail :1361064345@ qq.com

收稿日期 :2024-11-06

[13] 贾继良 ,赵清海 ,杨景周 ,等.汽车半主动座椅悬架 自适

应模糊神经滑模控制[J].机械设计 ,2024,41(4) :28-35.

[14] 高晋 ,李晖 ,杨秀建.7 自 由度半主动悬架整车模型 PID- LQR 控制研究[J].车辆与动力技术 ,2023(3) :1-6,23.

[15] 王刚 ,李昆鹏 ,景晖 ,等.基于 Q 学 习 的整车主动悬架免参数 H ∞ 控制[J].汽车工程 ,2023,45(12) :2260-2271.

[16] LEE D,JIN S,LEE C. Deep reinforcement learning of semi- active suspension controller for vehicle ride comfort[J]. IEEE Transactions on Vehicular Technology,2022,72(1) :327-339.

[17] FARES A,BANI Y A. Online reinforcement learning-based con- trol of an active suspension system using the actor critic approach[J]. Applied Sciences,2020,10(22) :8060-8073.

[18] HAN S Y,LIANG T. Reinforcement-learning-based vibration control for a vehicle semi-active suspension system via the PPO approach[J]. Applied Sciences,2022,12(6) :3078-3095.

[19] LIU M, LI Y, RONG X, et al. Semi-active suspension control based on deep reinforcement learning [ J ]. IEEE Access,2020,8:9978-9986.

[20] 陈盟 ,龙海洋 ,琚立颖 ,等. 随机路面时域模型的建模与仿真[J].机械工程与 自动化 ,2017,52(2) :40-41.

[21] 黄健. 丘 陵 山 地 拖 拉 机 路 面 谱 测 试 及 再 现 方 法 研 究[D].长春:吉林大学 ,2018.

[22] 刘国辉,郝称意,李民赞,等.半主动悬架山地拖拉机姿态控制系统设计与仿真[J].农业机械学报,2022,53(S2) : 338-348.

[23] SOWMYA R, PREMKUMAR M, JANGIR P. Newton- Raphson-based optimizer:A new population-based metaheu- ristic algorithm for continuous optimization problems [ J ]. Engineering Applications of Artificial Intelligence, 2024, 128:107532.

[24] CHEN T,GUESTRIN C. Xgboost:A scalable tree boosting system[C]//Proceedings of the 22nd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Min- ing. [S.l. ] :[s.n. ] ,2016:785-794.

[25] YU M,EVANGELOU S A,DINID. Advances in Active Sus- pension Systems for Road Vehicles[J]. Engineering,2024, 33:160-177.

[26] SHEN D. A Study on Active Suspension System with Reinforce- ment Learning[D]. Sydney:University of Technology,2022.

[27] 潘公宇 ,范菲阳 ,冯鑫.基于主动悬架的整车车身姿态控制策略研究[J]. 电子测量技术 ,2024,47(2) :79-88.