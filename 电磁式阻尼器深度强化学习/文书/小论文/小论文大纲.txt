% 注释： # 代表章节标题， ## 代表小节标题，* 代表要点，$ 代表公式,{} 代表正文，[]引用文献或参考，@代表图表，&待补充

# 小论文大纲：基于时间感知GRU-Attention-TD3的电磁式阻尼器振动控制研究

## 1. 标题 (Title)
**中文标题**：基于时间感知GRU-Attention-TD3的电磁式阻尼器振动控制研究
**英文标题**：Time-Aware GRU-Attention-TD3 for Vibration Control of Electromagnetic Dampers Considering Time Delays and Sampling Uncertainty

## 2. 摘要 (Abstract)
*   **背景**：电磁式阻尼器作为一种半主动/主动控制装置，在结构振动抑制中具有重要应用前景。然而，实际控制系统中普遍存在的时滞、采样时间抖动（Jitter）以及计算延迟，严重影响了传统控制算法及标准强化学习算法的性能与稳定性。
*   **问题**：现有强化学习振动控制研究多基于理想的固定时间步长假设，忽略了实际物理系统中时间参数的不确定性，导致仿真策略在实际部署时效果下降甚至失稳。此外，单纯的RNN/GRU网络难以区分长序列中各时间步的重要性，可能导致关键时序信息被忽略。
*   **方法**：提出一种基于时间感知的门控循环单元-注意力机制-双延迟深度确定性策略梯度（Time-Aware GRU-Attention-TD3）控制算法。
    1.  **算法框架改进**：采用TD3算法作为基础框架，利用其双重评价网络和延迟更新机制提高策略学习的稳定性。
    2.  **时序特征提取与增强**：在Actor和Critic网络前端集成GRU层以提取时序特征，并引入自注意力（Self-Attention）机制层，动态调整对预测序列中各时间步的关注度，增强对关键历史信息的捕捉能力。
    3.  **时间感知机制**：显式地将采样时间间隔（$\Delta t$）及时滞估计（$\tau$）作为状态向量的一部分输入网络，使智能体能够感知时间维度的变化并动态调整控制策略。
    4.  **随机变步长训练**：构建包含随机时滞与变步长的仿真训练环境，提高算法对时间不确定性的鲁棒性。
*   **结果**：通过二自由度电磁阻尼器系统的数值仿真验证。
*   **结论**：相比于传统PID、标准TD3及无注意力机制的GRU-TD3，所提算法在变步长和时滞环境下具有更优的控制效果和更强的鲁棒性。

## 3. 引言 (Introduction)
*   **研究背景**：
    *   结构振动危害及控制技术。
    *   电磁式阻尼器的优势。
    *   深度强化学习（DRL）在振动控制中的应用现状。
*   **现有问题与挑战**：
    *   **时滞与采样不确定性**：传感器采集、信号传输、算法计算及执行器响应均存在时滞；实际嵌入式系统中控制周期存在随机抖动（Jitter）。
    *   **POMDP问题**：时滞和变步长破坏了MDP假设，导致标准RL算法性能下降。
    *   **长序列信息提取**：传统GRU/LSTM在处理长序列时可能无法有效聚焦关键时刻，引入注意力机制（Attention Mechanism）可提升特征提取效率（参考交通信号控制等领域的应用）。
*   **本文贡献**：
    *   提出Time-Aware GRU-Attention-TD3算法架构。
    *   结合GRU与Attention机制处理时滞与变步长带来的POMDP问题。
    *   设计包含时间信息的增强状态空间。

## 4. 系统建模与问题描述 (System Modeling and Problem Formulation)
*   **受控对象动力学模型**：
    *   建立二自由度结构系统动力学方程（主结构 + 电磁作动器）。
    *   电磁阻尼器力学模型。
*   **时滞与采样不确定性建模**：
    *   定义总时滞 $\tau = \tau_{sensor} + \tau_{compute} + \tau_{actuator}$。
    *   定义变步长采样：$t_{k+1} = t_k + \Delta t + \delta_k$，其中 $\delta_k$ 为随机扰动。
*   **强化学习环境构建**：
    *   **状态空间 (State Space)**：$S_t = [x_t, \dot{x}_t, \ddot{x}_t, \Delta t_{last}, \tau_{delay}]$。引入历史观测序列 $H_t = \{S_{t-n}, ..., S_t\}$。
    *   **动作空间 (Action Space)**：电磁阻尼器的控制力 $F$。
    *   **奖励函数 (Reward Function)**：综合考虑振动抑制效果（位移/加速度均方根）与控制能耗。

## 5. 基于时间感知的GRU-Attention-TD3控制算法 (Proposed Method)
*   **算法整体框架**：基于TD3（Twin Delayed DDPG）架构。
    *   双Critic网络、延迟更新、目标策略平滑噪声。
*   **网络架构设计**：
    *   **GRU层**：在Actor和Critic网络前端加入GRU层，用于从历史状态序列中提取隐含的系统动态特征，补偿时滞影响。
    *   **Attention层**：在GRU层后集成注意力机制（如Self-Attention），计算预测序列的权重分布，使网络聚焦于对当前决策最重要的历史时刻。
*   **时间感知机制 (Time-Awareness)**：
    *   **输入层设计**：将时间间隔 $\Delta t$ 和时滞 $\tau$ 归一化后拼接到网络输入中。
    *   **物理意义**：告知网络当前决策距离上一次决策经过了多久，以及当前的观测滞后程度。
*   **训练策略**：
    *   **随机环境训练**：在训练过程中引入随机时滞和随机步长。
    *   **经验回放**：使用序列经验回放以适配GRU训练。

## 6. 仿真实验与结果分析 (Simulation and Results)
*   **实验设置**：
    *   二自由度仿真对象参数。
    *   地震波/随机激励输入。
    *   对比算法：Passive (被动), PID, Standard TD3, GRU-TD3 (无Attention), Time-Aware GRU-Attention-TD3 (本文)。
*   **性能指标**：
    *   减振率 (Vibration Reduction Rate)。
    *   峰值响应、均方根响应 (RMS)。
*   **结果分析**：
    *   **工况1：理想环境**。验证算法基本学习能力。
    *   **工况2：时滞环境**。对比GRU架构对时滞的补偿效果。
    *   **工况3：变步长与随机时滞环境**。重点展示Time-Aware机制和Attention机制的优势，对比标准TD3的性能衰减与本文算法的稳定性。
    *   **消融实验**：验证Attention模块的有效性。

## 7. 结论 (Conclusion)
*   总结Time-Aware GRU-Attention-TD3在处理非理想时间因素下的优势。
*   指出Attention机制在增强时序特征提取方面的作用。
*   未来展望。

## 参考文献 (References)
*   列出相关的RL振动控制、时滞控制、GRU及Attention应用等文献。

% 正文
{
# 基于时间感知GRU-Attention-TD3的电磁式阻尼器振动控制研究

## 摘要

## 引言

## 系统建模与问题描述

    ### 受控对象动力学模型
        本文以如图所示的二自由度结构系统为例，其中，m1为电磁作动器质量，m2为待减振对象即主结构质量，c1 和c2为分别为电磁吸振器阻尼和主结构阻尼，k1 和k2为分别为电磁吸振器器刚度和主结构刚度，w1为电磁作动器位移，w2为主结构位移，z为地基位移，f为外激励，F为电磁吸振器给予的控制力。
        @ 图1 二自由度结构系统示意图
        则该系统的动力学方程为：$ m_1\ddot{w_1}+c_1(\dot{w_1}-\dot{w_2})+k_1(w_1-w_2)+F=0$，$ m_2\ddot{w_2}+c_1(\dot{w_2}-\dot{w_1})+k_2w_2-f-F=0$。
    
        深度强化学习是一种基于马尔可夫决策过程（MDP）的数据驱动控制方法，通过智能体与环境的交互学习最优控制策略。本文采用TD3算法作为基础算法框架。TD3算法通过引入双重Critic网络和延迟更新机制，有效缓解了过估计偏差问题，提高了策略的稳定性和收敛速度。
        基于TD3算法训练神经网络控制器的算法流程如图所示。强化学习环境由振动系统动力学方程和奖励函数构成。在训练的每一个控制步中，策略网络接收振动系统当前状态并计算动作(即控制力)，振动系统在动作的作用下推进到下一时刻状态，由奖励函数计算奖励，并将经验元组存入经验池中。随后，从经验池中抽取个经验元组用于更新神经网络参数。目标价值网络和将分别计算状态-动作对的目标价值，并取其中最小值作为实际目标价值。价值网络和分别计算状态-动作对的当前价值，并通过与目标价值的误差更新自身参数。当延迟条件满足时，策略网络朝最大化当前价值的方向更新自身参数，并通过软更新方法更新目标网络参数。神经网络参数更新完成后，推进到下一个控制步，并重复上述步骤，直到训练结束。策略网络即为所训练的神经网络控制器。[蒋纪元，2025硕士毕业论文]
        状态-动作对的目标价值可表示为
        其中，为折扣因子，表示正态分布，表示噪声的标准差，为噪声大小的上界。由状态-动作对的当前价值和目标价值构造价值网络的损失函数为
            $ $
        策略网络的学习目标为在价值函数的指导下做出最优动作，其损失函数为 
            $ $
        为了提高训练过程中的稳定性，采用软更新的方式更新目标网络，即
            $ $
        其中，为软更新参数。并且采用了延迟更新策略，即策略网络和目标网络仅在评价网络更新 次后更新一次。

    ### 时滞与采样不确定性建模
        标准假设下环境的时间步长恒定为Δt，但实际嵌入式系统中，控制周期并非严格固定，存在随机抖动（Jitter）。此外，传感器采集、信号传输、算法计算及执行器响应均存在时滞，导致系统状态观测滞后。本文将总时滞定义为$\tau = \tau_{sensor} + \tau_{compute} + \tau_{actuator}$，并将变步长采样定义为$t_{k+1} = t_k + \Delta t + \delta_k$，其中$\delta_k$为随机扰动。

    基于以上问题建模，采用TD3算法构建出系统的控制流程如图所示：
        @ 图2 系统控制流程图[变循环航空发动机固定时间滑模推力控制]

    
## 基于GRU-TD3算法的控制器设计
    ### GRU网络集成
        门控循环单元（GRU）神经网络是Cho等为解决传统循环神经网络（RNN）在处理长序列数据时梯度消失和梯度爆炸问题而提出的一种改进型RNN结构。GRU通过引入更新门和重置门，有效地控制信息的流动和记忆，从而能够捕捉序列数据中的长期依赖关系。与LSTM相比，GRU结构更为简洁，参数更少，计算效率更高，适合实时控制任务中的时序数据处理。
        GRU单元结构如图所示。图中，xt是t时刻的输入向量，ht是t时刻的隐藏状态向量，zt是更新门向量，rt是重置门向量，~ht是候选隐藏状态向量。GRU通过以下公式进行计算：
        1. 更新门：$ z_t = \sigma(W_z \cdot [x_t, h_{t-1}] + b_z) $
        2. 重置门：$ r_t = \sigma(W_r \cdot [x_t, h_{t-1}] + b_r) $
        3. 候选隐藏状态：$ \tilde{h}_t = \tanh(W_h \cdot [x_t, r_t * h_{t-1}] + b_h) $
        4. 最终隐藏状态：$ h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t $
        其中，$W_z$、$W_r$、$W_h$为权重矩阵，$b_z$、$b_r$、$b_h$为偏置向量，$\sigma$为Sigmoid激活函数，*表示按位乘法。
    
        为了处理由时滞和采样不确定性引起的部分可观测问题（POMDP），分别在Actor网络和Critic网络的前端集成了一层GRU网络，提取振动响应的时序特征，输出预测的状态向量序列。GRU网络的输入为当前时刻及过去n个时刻的状态向量序列，输出为未来n个时刻的预测状态向量序列。通过这种方式，网络能够利用历史信息补偿时滞带来的观测延迟，提高对系统动态的理解和预测能力。

    ### 注意力层集成
        但是单纯依靠GRU网络无法区分预测序列中各时间步的重要性，可能导致部分关键信息被忽略。为此，本文在GRU网络后集成了注意力机制层，以增强网络对重要时间步的关注能力。注意力机制通过计算GRU网络预测的状态向量的权重分布，使网络能够动态调整对预测序列中各时间步的关注度，从而提升对时滞和变步长影响的适应能力。
        
        &代补充一些自注意力机制的公式和说明

        针对前文所述的时滞和采样不确定性问题，本文在GRU-TD3算法中引入时间感知机制。具体而言，将采样时间间隔（$\Delta t$）及计算耗时作为状态向量的一部分输入网络，使智能体能够感知时间维度的变化并动态调整控制策略。时间感知状态向量定义为：
            $ S_t = [x_t, \dot{x}_t, \ddot{x}_t, \Delta t_{last}, \tau_{delay}] $
        其中，$\Delta t_{last}$为上一个控制步的实际采样时间间隔，$\tau_{delay}$为当前时刻的总时滞估计值。通过将时间信息融入状态空间，智能体能够更准确地预测系统在非固定步长下的演化规律，从而制定出更有效的控制策略。
        搭建如图所示的Actor和Critic网络结构：
            @ 图3 Actor和Critic网络结构示意图
        Actor网络的输入为时间感知状态向量序列，经过GRU层和注意力层后，输出一个状态向量，随后通过全连接层生成最终的动作输出。Critic网络的输入为时间感知状态向量序列和动作向量，其中状态向量序列经过GRU层和注意力层处理后，与动作向量拼接，经过全连接层生成当前状态-动作对的价值评估Q。

    ### 训练策略

        为了使智能体能够感知采样步长的变化和滞后程度，显式地将时间信息融入状态空间。定义 $t$ 时刻的增强状态向量 $S_t$ 为：
        $$ S_t = [\tilde{x}_2, \dot{\tilde{x}}_2, \ddot{\tilde{x}}_2, \tilde{x}_1, \dot{\tilde{x}}_1, \Delta \tilde{t}_{k}, \tilde{\tau}_{est}] $$
        其中，$\Delta \tilde{t}_{k}$ 为归一化的上一帧采样间隔，$\tilde{\tau}_{est}$ 为归一化的当前时滞估计值。通过引入时间维度，使得智能体能够动态感知时间变化对系统状态的影响，从而调整控制策略以适应非固定步长和时滞环境。

        &奖励函数设计
        为了平衡减振效果与控制能耗，设计如下奖励函数：&待完善权重
            $ R_t = \begin{cases}
            \frac{tolerance - |x_{2,t+1}|}{tolerance} + \mathbb{1}_{|x_{2,t+1}| < |x_{2,t}|}, & |x_{2,t+1}| \leq tolerance \\
            -1 - \mathbb{1}_{|x_{2,t+1}| > |x_{2,t}|} - \log_{10}\left(\frac{|x_{2,t+1}|}{tolerance}\right), & |x_{2,t+1}| > tolerance
            \end{cases} - \frac{|F_t|}{F_{max}} $
        其中，$tolerance$ 为允许的位移阈值，$\mathbb{1}$ 为指示函数，$F_{max}$ 为最大控制力。该奖励函数在振动响应较小时给予正奖励，并根据控制力大小进行惩罚，从而引导智能体学习有效的减振策略。

        改进后的Time-Aware GRU-Attention-TD3算法训练流程如图所示。
        与标准TD3算法不同，为了适应变步长和时滞环境，本文在训练过程中引入了随机时滞和随机步长。
        具体而言，在每个时间步更新前，随机生成一个采样时间间隔 $\Delta t_k$ 和一个时滞值 $\tau_k$，二者均服从预设的正态分布。
        在经验采集阶段，t时刻，Actor网络根据 $\tau_k$ 观测到环境的时滞状态序列$ s_seq$ 并采取动作a，环境根据上一时刻的状态s、动作a和采样间隔$\Delta t_k$推进到下一时刻状态s'，更新时滞状态序列作为下一时刻Actor网络的观测。根据奖励函数计算奖励r，将经验元组$(s_seq, a, r, s'_seq, \Delta t_k, \tau_k)$存入经验回放池D中。
        当经验回放池中样本数量达到设定阈值后，开始从中随机抽取批量经验进行网络参数更新。
        在训练阶段，目标Actor网络根据回放经验的时滞状态序列 $s_seq$ 计算动作a，Critic网络根据状态序列 $s_seq$ 和动作a计算 目标网络中两个Critic网络的最小值$Q_min$。
        在主网络中，根据奖励值r、$Q_min$ 和Critic网络产生的 $Q(s_seq,a|\theta_i)$ 计算Critic网络的损失函数，并通过最小化该损失函数更新Critic网络参数。其中$\theta_i$ 表示第i个Critic网络的参数。
        主网络中Actor网络根据critic网络计算的当前价值 $Q(s_seq,a|\omega)$ 计算Actor网络的损失函数，并通过最大化该损失函数更新Actor网络参数。其中$\omega$ 表示Actor网络的参数。
        最后，通过软更新方法更新目标网络参数。重复上述过程直至训练结束，最终得到训练好的Time-Aware GRU-Attention-TD3控制器。
        由于引入了GRU循环神经网络，训练过程中容易出现梯度消失或梯度爆炸问题。为此，在反向传播过程中引入了梯度裁剪（Gradient Clipping）机制。在优化器更新参数前，将所有网络参数的梯度范数限制在阈值（如10.0）以内：
        $$ \text{if } \|\nabla\|_2 > \delta, \text{ then } \nabla \leftarrow \nabla \cdot \frac{\delta}{\|\nabla\|_2} $$
        @ 图4 Time-Aware GRU-Attention-TD3算法训练流程图

## 仿真实验设计与分析

    ### 实验设置 
        针对上述提出的Time-Aware GRU-Attention-TD3控制算法，采用pytorch深度学习框架在python中搭建仿真环境，并进行数值仿真验证。仿真对象为前文所述的二自由度结构系统，系统参数如表所示：
            @ 表1 二自由度系统参数表
    
    ### 对比试验设计
        为了验证所提算法在处理时滞与采样不确定性问题上的有效性，设计了以下对比试验：
        1. 被动控制：电磁阻尼器断路，仅提供被动阻尼。
        2. PID控制：基于传统PID控制器进行振动控制，&参数通过Ziegler-Nichols方法调节。
        3. 标准TD3控制：不含GRU和Attention机制的标准TD3算法，网络的输入为当前时刻环境的观测状态。
        4. GRU-Attention-TD3控制：集成GRU网络和注意力机制的改进TD3算法，网络的输入为时间感知状态向量序列。

    ### 超参数设计
        神经网络的超参数设置如表所示：
            @ 表2 神经网络超参数设置表
            | 参数名称 | 符号 | 数值 | 说明 |
            | :--- | :---: | :---: | :--- |
            | GRU序列长度 | $L$ | 36 | GRU输入的时间步数 |
            | 预测时间步长度 | $L_{fc}$ | 4 | 全连接层预测步数 |
            | GRU隐藏层维度 | $H_{gru}$ | 64 | GRU单元数量 |
            | GRU层数 | $N_{gru}$ | 1 | GRU堆叠层数 |
            | MLP隐藏层维度 | $H_{mlp}$ | 128 | 全连接层神经元数量 |
            | 动作范围 | $A_{bound}$ | 5.0 | 控制力输出限制 |
        
        TD3算法的超参数如表所示，其中，为了训练后期的稳定性，目标网络策略平滑的噪声随着训练轮次逐渐减小至0。
            @ 表3 TD3算法超参数设置表
            | 参数名称 | 符号 | 数值 | 说明 |
            | :--- | :---: | :---: | :--- |
            | 折扣因子 | $\gamma$ | 0.99 | 长期回报权重 |
            | 软更新系数 | $\tau$ | 0.002 | 目标网络平滑更新 |
            | 策略噪声 | $\sigma_{policy}$ | 0.2 | 目标策略平滑正则化 |
            | 噪声裁剪 | $c$ | 0.5 | 策略噪声边界 |
            | 策略更新频率 | $d$ | 3 | Actor网络延迟更新频率 |
            | 动作探索噪声 | $\sigma_{action}$ | 0.15 | 动作选择时的高斯噪声 |
            | 梯度裁剪阈值 | $\delta$ | 1.0 | 防止梯度爆炸 |
            | Actor学习率 | $lr_a$ | 2e-6 | 策略网络优化步长 |
            | Critic学习率 | $lr_c$ | 1e-5 | 价值网络优化步长 |
            | 预测器学习率 | $lr_p$ | 3e-4 | GRU预测器优化步长 |
            | 批量大小 | $B$ | 256 | 经验回放Batch Size |
            | 经验池容量 | $C$ | 500,000 | 经验回放缓冲区大小 |

    ### 仿真结果与分析
        图5展示了标准TD3算法和Time-Aware GRU-Attention-TD3算法在理想环境下的训练过程。可以看出，GRU-Attention-TD3算法的训练收敛速度明显快于标准TD3算法，且最终获得的平均奖励值更高，表明引入GRU和Attention机制有效提升了策略学习能力。
            @ 图5 理想环境下两种算法的训练过程对比图

        图6-图9展示了在含时滞和变步长的测试环境中，不同控制算法的振动响应对比结果。随机-n时滞表示时滞服从均值为n步，标准差为sigma步(整数)的正态分布。固定时滞即时滞恒定为n步。
            @ 图6 含变步长与固定-1的时滞环境下不同算法的位移响应对比图
            @ 图7 含变步长与固定-2的时滞环境下不同算法的位移响应对比图
            @ 图8 含变步长与固定-3的时滞环境下不同算法的位移响应对比图
            @ 图9 含变步长与随机-3，标准差2的时滞环境下不同算法的位移响应对比图
        从图中可以看出，与被动控制相比，在固定时滞步数较小（1-2步）时，标准TD3算法的控制效果有所下降，但仍能实现一定程度的减振。而随着时滞步数的增加（3步及以上），标准TD3算法的性能显著下降，甚至出现失稳现象。相比之下，GRU-Attention-TD3算法在各类时滞条件下均表现出较强的鲁棒性，能够有效抑制结构振动，且位移峰值和均方根响应均明显优于其他对比算法。在随机时滞环境下，GRU-Attention-TD3算法同样展现出优异的控制性能，进一步验证了其在非理想时间条件下的适应能力。

## 结论
    本文针对强化学习算法在实际工程中的应用，考虑了时滞和采样不确定性问题，提出了一种基于时间感知的GRU-Attention-TD3振动控制算法，并通过设计对比仿真实验验证了该方法的有效性。具体结论如下：
    1.通过在传统TD3算法的Actor和Critic网络中集成GRU网络和注意力机制，有效提升了对时序特征的提取能力，增强了算法对时滞和变步长影响的适应性。
    2.引入时间感知机制，将采样时间间隔和时滞估计作为状态向量的一部分输入网络，使智能体能够动态感知时间维度的变化，从而调整控制策略以适应非固定步长和时滞环境。
    3.数值仿真结果表明，所提算法在含时滞和变步长的复杂环境下，显著优于传统PID控制和标准TD3算法，展现出更强的鲁棒性和控制效果。
    未来工作将进一步探索该算法在实际嵌入式系统中的部署，以及在更复杂结构系统中的应用潜力。

}